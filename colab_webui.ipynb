{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xiaoseymy/NLP_ability/blob/master/colab_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÁéØÂ¢ÉÈÖçÁΩÆ environment"
      ],
      "metadata": {
        "id": "_o6a8GS2lWQM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9b7iFV3dm1f",
        "outputId": "bb15a8ab-5a60-4593-905a-6888e9567932",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q condacolab\n",
        "# Setting up condacolab and installing packages\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\")\n",
        "%cd -q /content\n",
        "!git clone https://github.com/RVC-Boss/GPT-SoVITS\n",
        "!conda install -y -q -c pytorch -c nvidia cudatoolkit\n",
        "%cd -q /content/GPT-SoVITS\n",
        "!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\n",
        "!/usr/local/bin/pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è¨ Downloading https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:24\n",
            "üîÅ Restarting kernel...\n",
            "Cloning into 'GPT-SoVITS'...\n",
            "remote: Enumerating objects: 3976, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 3976 (delta 0), reused 0 (delta 0), pack-reused 3971 (from 2)\u001b[K\n",
            "Receiving objects: 100% (3976/3976), 11.27 MiB | 15.83 MiB/s, done.\n",
            "Resolving deltas: 100% (2352/2352), done.\n",
            "Channels:\n",
            " - pytorch\n",
            " - nvidia\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    archspec-0.2.3             |     pyhd3eb1b0_0          47 KB\n",
            "    ca-certificates-2024.11.26 |       h06a4308_0         131 KB\n",
            "    certifi-2024.12.14         |   py39h06a4308_0         160 KB\n",
            "    conda-24.11.2              |   py39h06a4308_0         932 KB\n",
            "    cudatoolkit-11.7.0         |      hd8887f6_10       831.6 MB  nvidia\n",
            "    frozendict-2.4.2           |   py39h5eee18b_0          55 KB\n",
            "    openssl-3.0.15             |       h5eee18b_0         5.2 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       838.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudatoolkit        nvidia/linux-64::cudatoolkit-11.7.0-hd8887f6_10 \n",
            "  frozendict         pkgs/main/linux-64::frozendict-2.4.2-py39h5eee18b_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  archspec                               0.2.1-pyhd3eb1b0_0 --> 0.2.3-pyhd3eb1b0_0 \n",
            "  ca-certificates                     2023.12.12-h06a4308_0 --> 2024.11.26-h06a4308_0 \n",
            "  certifi                         2023.11.17-py39h06a4308_0 --> 2024.12.14-py39h06a4308_0 \n",
            "  conda                              23.11.0-py39h06a4308_0 --> 24.11.2-py39h06a4308_0 \n",
            "  openssl                                 3.0.12-h7f8727e_0 --> 3.0.15-h5eee18b_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "done\n",
            "/usr/local/lib/python3.9/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
            "\n",
            "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
            "\n",
            "  conda config --add channels defaults\n",
            "\n",
            "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
            "\n",
            "  deprecated.topic(\n",
            "/usr/local/lib/python3.9/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
            "\n",
            "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
            "\n",
            "  conda config --add channels defaults\n",
            "\n",
            "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
            "\n",
            "  deprecated.topic(\n",
            "Channels:\n",
            " - conda-forge\n",
            " - pytorch\n",
            " - nvidia\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cmake\n",
            "    - ffmpeg\n",
            "    - gcc\n",
            "    - gxx\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n",
            "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
            "    aom-3.9.1                  |       hac33072_0         2.6 MB  conda-forge\n",
            "    binutils_impl_linux-64-2.43|       h4bf12b8_2         5.4 MB  conda-forge\n",
            "    ca-certificates-2024.12.14 |       hbcca054_0         153 KB  conda-forge\n",
            "    cairo-1.18.0               |       h3faef2a_0         959 KB  conda-forge\n",
            "    certifi-2024.12.14         |     pyhd8ed1ab_0         158 KB  conda-forge\n",
            "    cmake-3.29.4               |       h91dbaaa_0        18.1 MB  conda-forge\n",
            "    dav1d-1.2.1                |       hd590300_0         742 KB  conda-forge\n",
            "    expat-2.6.4                |       h5888daf_0         135 KB  conda-forge\n",
            "    ffmpeg-7.0.1               | gpl_hb399a10_100         9.6 MB  conda-forge\n",
            "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
            "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
            "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
            "    font-ttf-ubuntu-0.83       |       h77eed37_3         1.5 MB  conda-forge\n",
            "    fontconfig-2.14.2          |       h14ed4e7_0         266 KB  conda-forge\n",
            "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
            "    fonts-conda-forge-1        |                0           4 KB  conda-forge\n",
            "    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge\n",
            "    fribidi-1.0.10             |       h36c2ea0_0         112 KB  conda-forge\n",
            "    gcc-14.2.0                 |       h96c4ede_1          54 KB  conda-forge\n",
            "    gcc_impl_linux-64-14.2.0   |       h6b349bd_1        69.1 MB  conda-forge\n",
            "    gettext-0.22.5             |       he02047a_3         468 KB  conda-forge\n",
            "    gettext-tools-0.22.5       |       he02047a_3         2.6 MB  conda-forge\n",
            "    gmp-6.3.0                  |       hac33072_2         449 KB  conda-forge\n",
            "    gnutls-3.7.9               |       hb077bed_0         1.9 MB  conda-forge\n",
            "    graphite2-1.3.13           |    h59595ed_1003          95 KB  conda-forge\n",
            "    gxx-14.2.0                 |       h96c4ede_1          54 KB  conda-forge\n",
            "    gxx_impl_linux-64-14.2.0   |       h2c03514_1        13.7 MB  conda-forge\n",
            "    harfbuzz-8.5.0             |       hfac3d4d_0         1.5 MB  conda-forge\n",
            "    icu-73.2                   |       h59595ed_0        11.5 MB  conda-forge\n",
            "    kernel-headers_linux-64-3.10.0|      he073ed8_18         921 KB  conda-forge\n",
            "    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n",
            "    ld_impl_linux-64-2.43      |       h712a8e2_2         654 KB  conda-forge\n",
            "    libabseil-20240116.2       | cxx17_he02047a_1         1.2 MB  conda-forge\n",
            "    libarchive-3.7.4           |       hfca40fe_0         851 KB  conda-forge\n",
            "    libasprintf-0.22.5         |       he8f35ee_3          42 KB  conda-forge\n",
            "    libasprintf-devel-0.22.5   |       he8f35ee_3          33 KB  conda-forge\n",
            "    libass-0.17.1              |       h8fe9dca_1         124 KB  conda-forge\n",
            "    libcurl-8.11.1             |       hc9e6f67_0         454 KB\n",
            "    libdrm-2.4.124             |       hb9d3cd8_0         237 KB  conda-forge\n",
            "    libexpat-2.6.4             |       h5888daf_0          72 KB  conda-forge\n",
            "    libgcc-14.2.0              |       h77fa898_1         829 KB  conda-forge\n",
            "    libgcc-devel_linux-64-14.2.0|     h41c2201_101         2.6 MB  conda-forge\n",
            "    libgcc-ng-14.2.0           |       h69a702a_1          53 KB  conda-forge\n",
            "    libgettextpo-0.22.5        |       he02047a_3         167 KB  conda-forge\n",
            "    libgettextpo-devel-0.22.5  |       he02047a_3          36 KB  conda-forge\n",
            "    libglib-2.80.2             |       hf974151_0         3.7 MB  conda-forge\n",
            "    libgomp-14.2.0             |       h77fa898_1         450 KB  conda-forge\n",
            "    libhwloc-2.11.2            |default_he43201b_1000         2.3 MB  conda-forge\n",
            "    libiconv-1.17              |       hd590300_2         689 KB  conda-forge\n",
            "    libidn2-2.3.7              |       hd590300_0         124 KB  conda-forge\n",
            "    libmamba-1.5.11            |       hfe524e5_0         1.8 MB\n",
            "    libmambapy-1.5.11          |   py39haf1ee3a_0         341 KB\n",
            "    libopenvino-2024.1.0       |       h2da1b83_7         4.9 MB  conda-forge\n",
            "    libopenvino-auto-batch-plugin-2024.1.0|       hb045406_7         107 KB  conda-forge\n",
            "    libopenvino-auto-plugin-2024.1.0|       hb045406_7         224 KB  conda-forge\n",
            "    libopenvino-hetero-plugin-2024.1.0|       h5c03a75_7         187 KB  conda-forge\n",
            "    libopenvino-intel-cpu-plugin-2024.1.0|       h2da1b83_7        10.4 MB  conda-forge\n",
            "    libopenvino-intel-gpu-plugin-2024.1.0|       h2da1b83_7         8.1 MB  conda-forge\n",
            "    libopenvino-intel-npu-plugin-2024.1.0|       he02047a_7         318 KB  conda-forge\n",
            "    libopenvino-ir-frontend-2024.1.0|       h5c03a75_7         196 KB  conda-forge\n",
            "    libopenvino-onnx-frontend-2024.1.0|       h07e8aee_7         1.5 MB  conda-forge\n",
            "    libopenvino-paddle-frontend-2024.1.0|       h07e8aee_7         683 KB  conda-forge\n",
            "    libopenvino-pytorch-frontend-2024.1.0|       he02047a_7         1.1 MB  conda-forge\n",
            "    libopenvino-tensorflow-frontend-2024.1.0|       h39126c6_7         1.3 MB  conda-forge\n",
            "    libopenvino-tensorflow-lite-frontend-2024.1.0|       he02047a_7         476 KB  conda-forge\n",
            "    libopus-1.3.1              |       h7f98852_1         255 KB  conda-forge\n",
            "    libpciaccess-0.18          |       hd590300_0          28 KB  conda-forge\n",
            "    libpng-1.6.43              |       h2797004_0         281 KB  conda-forge\n",
            "    libprotobuf-4.25.3         |       h08a7969_0         2.7 MB  conda-forge\n",
            "    libsanitizer-14.2.0        |       h2a3dede_1         4.3 MB  conda-forge\n",
            "    libsolv-0.7.29             |       ha6fb4c9_0         460 KB  conda-forge\n",
            "    libssh2-1.11.1             |       h251f7ec_0         308 KB\n",
            "    libstdcxx-14.2.0           |       hc0a3c3a_1         3.7 MB  conda-forge\n",
            "    libstdcxx-devel_linux-64-14.2.0|     h41c2201_101        12.9 MB  conda-forge\n",
            "    libstdcxx-ng-14.2.0        |       h4852527_1          53 KB  conda-forge\n",
            "    libtasn1-4.19.0            |       h166bdaf_0         114 KB  conda-forge\n",
            "    libunistring-0.9.10        |       h7f98852_0         1.4 MB  conda-forge\n",
            "    libuuid-2.38.1             |       h0b41bf4_0          33 KB  conda-forge\n",
            "    libuv-1.49.2               |       hb9d3cd8_0         864 KB  conda-forge\n",
            "    libva-2.21.0               |       h4ab18f5_2         185 KB  conda-forge\n",
            "    libvpx-1.14.1              |       hac33072_0         999 KB  conda-forge\n",
            "    libxcb-1.15                |       h0b41bf4_0         375 KB  conda-forge\n",
            "    libxml2-2.12.7             |       hc051c1a_1         688 KB  conda-forge\n",
            "    libzlib-1.2.13             |       h4ab18f5_6          60 KB  conda-forge\n",
            "    lzo-2.10                   |    hd590300_1001         167 KB  conda-forge\n",
            "    ncurses-6.5                |       he02047a_1         868 KB  conda-forge\n",
            "    nettle-3.9.1               |       h7ab15ed_0         988 KB  conda-forge\n",
            "    ocl-icd-2.3.2              |       hb9d3cd8_2          93 KB  conda-forge\n",
            "    opencl-headers-2024.10.24  |       h5888daf_0          53 KB  conda-forge\n",
            "    openh264-2.4.1             |       h59595ed_0         718 KB  conda-forge\n",
            "    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n",
            "    p11-kit-0.24.1             |       hc5aa10d_0         4.5 MB  conda-forge\n",
            "    pcre2-10.43                |       hcad00b1_0         929 KB  conda-forge\n",
            "    pixman-0.44.2              |       h29eaf8c_0         372 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    hb9d3cd8_1002           8 KB  conda-forge\n",
            "    pugixml-1.14               |       h59595ed_0         112 KB  conda-forge\n",
            "    rhash-1.4.5                |       hb9d3cd8_0         183 KB  conda-forge\n",
            "    snappy-1.2.1               |       h8bd8927_1          42 KB  conda-forge\n",
            "    svt-av1-2.1.0              |       hac33072_0         2.5 MB  conda-forge\n",
            "    sysroot_linux-64-2.17      |      h4a8ded7_18        14.8 MB  conda-forge\n",
            "    tbb-2022.0.0               |       hceb3a55_0         174 KB  conda-forge\n",
            "    x264-1!164.3095            |       h166bdaf_2         877 KB  conda-forge\n",
            "    x265-3.5                   |       h924138e_3         3.2 MB  conda-forge\n",
            "    xorg-fixesproto-5.0        |    hb9d3cd8_1003          11 KB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    hb9d3cd8_1003          30 KB  conda-forge\n",
            "    xorg-libice-1.1.2          |       hb9d3cd8_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.5           |       he73a12e_0          27 KB  conda-forge\n",
            "    xorg-libx11-1.8.9          |       h8ee46fc_0         809 KB  conda-forge\n",
            "    xorg-libxau-1.0.12         |       hb9d3cd8_0          14 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.5        |       hb9d3cd8_0          19 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h0b41bf4_2          49 KB  conda-forge\n",
            "    xorg-libxfixes-5.0.3       |    h7f98852_1004          18 KB  conda-forge\n",
            "    xorg-libxrender-0.9.11     |       hd590300_0          37 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    hb9d3cd8_1003          12 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    hb9d3cd8_1004          30 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    hb9d3cd8_1008          72 KB  conda-forge\n",
            "    zlib-1.2.13                |       h4ab18f5_6          91 KB  conda-forge\n",
            "    zstd-1.5.6                 |       ha6fb4c9_0         542 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       255.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aom                conda-forge/linux-64::aom-3.9.1-hac33072_0 \n",
            "  binutils_impl_lin~ conda-forge/linux-64::binutils_impl_linux-64-2.43-h4bf12b8_2 \n",
            "  cairo              conda-forge/linux-64::cairo-1.18.0-h3faef2a_0 \n",
            "  cmake              conda-forge/linux-64::cmake-3.29.4-h91dbaaa_0 \n",
            "  dav1d              conda-forge/linux-64::dav1d-1.2.1-hd590300_0 \n",
            "  expat              conda-forge/linux-64::expat-2.6.4-h5888daf_0 \n",
            "  ffmpeg             conda-forge/linux-64::ffmpeg-7.0.1-gpl_hb399a10_100 \n",
            "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
            "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
            "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
            "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_3 \n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n",
            "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
            "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n",
            "  freetype           conda-forge/linux-64::freetype-2.12.1-h267a509_2 \n",
            "  fribidi            conda-forge/linux-64::fribidi-1.0.10-h36c2ea0_0 \n",
            "  gcc                conda-forge/linux-64::gcc-14.2.0-h96c4ede_1 \n",
            "  gcc_impl_linux-64  conda-forge/linux-64::gcc_impl_linux-64-14.2.0-h6b349bd_1 \n",
            "  gettext            conda-forge/linux-64::gettext-0.22.5-he02047a_3 \n",
            "  gettext-tools      conda-forge/linux-64::gettext-tools-0.22.5-he02047a_3 \n",
            "  gmp                conda-forge/linux-64::gmp-6.3.0-hac33072_2 \n",
            "  gnutls             conda-forge/linux-64::gnutls-3.7.9-hb077bed_0 \n",
            "  graphite2          conda-forge/linux-64::graphite2-1.3.13-h59595ed_1003 \n",
            "  gxx                conda-forge/linux-64::gxx-14.2.0-h96c4ede_1 \n",
            "  gxx_impl_linux-64  conda-forge/linux-64::gxx_impl_linux-64-14.2.0-h2c03514_1 \n",
            "  harfbuzz           conda-forge/linux-64::harfbuzz-8.5.0-hfac3d4d_0 \n",
            "  kernel-headers_li~ conda-forge/noarch::kernel-headers_linux-64-3.10.0-he073ed8_18 \n",
            "  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 \n",
            "  libabseil          conda-forge/linux-64::libabseil-20240116.2-cxx17_he02047a_1 \n",
            "  libasprintf        conda-forge/linux-64::libasprintf-0.22.5-he8f35ee_3 \n",
            "  libasprintf-devel  conda-forge/linux-64::libasprintf-devel-0.22.5-he8f35ee_3 \n",
            "  libass             conda-forge/linux-64::libass-0.17.1-h8fe9dca_1 \n",
            "  libdrm             conda-forge/linux-64::libdrm-2.4.124-hb9d3cd8_0 \n",
            "  libexpat           conda-forge/linux-64::libexpat-2.6.4-h5888daf_0 \n",
            "  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n",
            "  libgcc-devel_linu~ conda-forge/noarch::libgcc-devel_linux-64-14.2.0-h41c2201_101 \n",
            "  libgettextpo       conda-forge/linux-64::libgettextpo-0.22.5-he02047a_3 \n",
            "  libgettextpo-devel conda-forge/linux-64::libgettextpo-devel-0.22.5-he02047a_3 \n",
            "  libglib            conda-forge/linux-64::libglib-2.80.2-hf974151_0 \n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.11.2-default_he43201b_1000 \n",
            "  libiconv           conda-forge/linux-64::libiconv-1.17-hd590300_2 \n",
            "  libidn2            conda-forge/linux-64::libidn2-2.3.7-hd590300_0 \n",
            "  libopenvino        conda-forge/linux-64::libopenvino-2024.1.0-h2da1b83_7 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-batch-plugin-2024.1.0-hb045406_7 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-plugin-2024.1.0-hb045406_7 \n",
            "  libopenvino-heter~ conda-forge/linux-64::libopenvino-hetero-plugin-2024.1.0-h5c03a75_7 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-cpu-plugin-2024.1.0-h2da1b83_7 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-gpu-plugin-2024.1.0-h2da1b83_7 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-npu-plugin-2024.1.0-he02047a_7 \n",
            "  libopenvino-ir-fr~ conda-forge/linux-64::libopenvino-ir-frontend-2024.1.0-h5c03a75_7 \n",
            "  libopenvino-onnx-~ conda-forge/linux-64::libopenvino-onnx-frontend-2024.1.0-h07e8aee_7 \n",
            "  libopenvino-paddl~ conda-forge/linux-64::libopenvino-paddle-frontend-2024.1.0-h07e8aee_7 \n",
            "  libopenvino-pytor~ conda-forge/linux-64::libopenvino-pytorch-frontend-2024.1.0-he02047a_7 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-frontend-2024.1.0-h39126c6_7 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-lite-frontend-2024.1.0-he02047a_7 \n",
            "  libopus            conda-forge/linux-64::libopus-1.3.1-h7f98852_1 \n",
            "  libpciaccess       conda-forge/linux-64::libpciaccess-0.18-hd590300_0 \n",
            "  libpng             conda-forge/linux-64::libpng-1.6.43-h2797004_0 \n",
            "  libprotobuf        conda-forge/linux-64::libprotobuf-4.25.3-h08a7969_0 \n",
            "  libsanitizer       conda-forge/linux-64::libsanitizer-14.2.0-h2a3dede_1 \n",
            "  libstdcxx          conda-forge/linux-64::libstdcxx-14.2.0-hc0a3c3a_1 \n",
            "  libstdcxx-devel_l~ conda-forge/noarch::libstdcxx-devel_linux-64-14.2.0-h41c2201_101 \n",
            "  libtasn1           conda-forge/linux-64::libtasn1-4.19.0-h166bdaf_0 \n",
            "  libunistring       conda-forge/linux-64::libunistring-0.9.10-h7f98852_0 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libuv              conda-forge/linux-64::libuv-1.49.2-hb9d3cd8_0 \n",
            "  libva              conda-forge/linux-64::libva-2.21.0-h4ab18f5_2 \n",
            "  libvpx             conda-forge/linux-64::libvpx-1.14.1-hac33072_0 \n",
            "  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.13-h4ab18f5_6 \n",
            "  lzo                conda-forge/linux-64::lzo-2.10-hd590300_1001 \n",
            "  nettle             conda-forge/linux-64::nettle-3.9.1-h7ab15ed_0 \n",
            "  ocl-icd            conda-forge/linux-64::ocl-icd-2.3.2-hb9d3cd8_2 \n",
            "  opencl-headers     conda-forge/linux-64::opencl-headers-2024.10.24-h5888daf_0 \n",
            "  openh264           conda-forge/linux-64::openh264-2.4.1-h59595ed_0 \n",
            "  p11-kit            conda-forge/linux-64::p11-kit-0.24.1-hc5aa10d_0 \n",
            "  pixman             conda-forge/linux-64::pixman-0.44.2-h29eaf8c_0 \n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-hb9d3cd8_1002 \n",
            "  pugixml            conda-forge/linux-64::pugixml-1.14-h59595ed_0 \n",
            "  rhash              conda-forge/linux-64::rhash-1.4.5-hb9d3cd8_0 \n",
            "  snappy             conda-forge/linux-64::snappy-1.2.1-h8bd8927_1 \n",
            "  svt-av1            conda-forge/linux-64::svt-av1-2.1.0-hac33072_0 \n",
            "  sysroot_linux-64   conda-forge/noarch::sysroot_linux-64-2.17-h4a8ded7_18 \n",
            "  tbb                conda-forge/linux-64::tbb-2022.0.0-hceb3a55_0 \n",
            "  x264               conda-forge/linux-64::x264-1!164.3095-h166bdaf_2 \n",
            "  x265               conda-forge/linux-64::x265-3.5-h924138e_3 \n",
            "  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-hb9d3cd8_1003 \n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-hb9d3cd8_1003 \n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.2-hb9d3cd8_0 \n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.5-he73a12e_0 \n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.9-h8ee46fc_0 \n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.12-hb9d3cd8_0 \n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.5-hb9d3cd8_0 \n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n",
            "  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004 \n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0 \n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-hb9d3cd8_1003 \n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-hb9d3cd8_1004 \n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-hb9d3cd8_1008 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2024.11.26~ --> conda-forge::ca-certificates-2024.12.14-hbcca054_0 \n",
            "  icu                        pkgs/main::icu-73.1-h6a678d5_0 --> conda-forge::icu-73.2-h59595ed_0 \n",
            "  ld_impl_linux-64   pkgs/main::ld_impl_linux-64-2.38-h118~ --> conda-forge::ld_impl_linux-64-2.43-h712a8e2_2 \n",
            "  libarchive         pkgs/main::libarchive-3.6.2-h6ac8c49_2 --> conda-forge::libarchive-3.7.4-hfca40fe_0 \n",
            "  libcurl                                  8.4.0-h251f7ec_1 --> 8.11.1-hc9e6f67_0 \n",
            "  libgcc-ng          pkgs/main::libgcc-ng-11.2.0-h1234567_1 --> conda-forge::libgcc-ng-14.2.0-h69a702a_1 \n",
            "  libgomp              pkgs/main::libgomp-11.2.0-h1234567_1 --> conda-forge::libgomp-14.2.0-h77fa898_1 \n",
            "  libmamba                                 1.5.3-haf1ee3a_0 --> 1.5.11-hfe524e5_0 \n",
            "  libmambapy                           1.5.3-py39h2dafd23_0 --> 1.5.11-py39haf1ee3a_0 \n",
            "  libsolv              pkgs/main::libsolv-0.7.24-he621ea3_0 --> conda-forge::libsolv-0.7.29-ha6fb4c9_0 \n",
            "  libssh2                                 1.10.0-hdbd6064_2 --> 1.11.1-h251f7ec_0 \n",
            "  libstdcxx-ng       pkgs/main::libstdcxx-ng-11.2.0-h12345~ --> conda-forge::libstdcxx-ng-14.2.0-h4852527_1 \n",
            "  libxml2              pkgs/main::libxml2-2.10.4-hf1b16e4_1 --> conda-forge::libxml2-2.12.7-hc051c1a_1 \n",
            "  ncurses                 pkgs/main::ncurses-6.4-h6a678d5_0 --> conda-forge::ncurses-6.5-he02047a_1 \n",
            "  openssl              pkgs/main::openssl-3.0.15-h5eee18b_0 --> conda-forge::openssl-3.4.0-hb9d3cd8_0 \n",
            "  pcre2                   pkgs/main::pcre2-10.42-hebb0a14_0 --> conda-forge::pcre2-10.43-hcad00b1_0 \n",
            "  zlib                    pkgs/main::zlib-1.2.13-h5eee18b_0 --> conda-forge::zlib-1.2.13-h4ab18f5_6 \n",
            "  zstd                     pkgs/main::zstd-1.5.5-hc292b87_0 --> conda-forge::zstd-1.5.6-ha6fb4c9_0 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  _libgcc_mutex           pkgs/main::_libgcc_mutex-0.1-main --> conda-forge::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex          pkgs/main::_openmp_mutex-5.1-1_gnu --> conda-forge::_openmp_mutex-4.5-2_gnu \n",
            "  certifi            pkgs/main/linux-64::certifi-2024.12.1~ --> conda-forge/noarch::certifi-2024.12.14-pyhd8ed1ab_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Ignoring onnxruntime: markers 'sys_platform == \"darwin\"' don't match your environment\n",
            "Ignoring opencc: markers 'sys_platform != \"linux\"' don't match your environment\n",
            "Collecting numpy==1.23.4 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting scipy (from -r requirements.txt (line 2))\n",
            "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard (from -r requirements.txt (line 3))\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 4))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting numba==0.56.4 (from -r requirements.txt (line 5))\n",
            "  Downloading numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting pytorch-lightning (from -r requirements.txt (line 6))\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting gradio<=4.24.0,>=4.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-4.24.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting ffmpeg-python (from -r requirements.txt (line 8))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting onnxruntime-gpu (from -r requirements.txt (line 10))\n",
            "  Downloading onnxruntime_gpu-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (4.65.0)\n",
            "Collecting funasr==1.0.27 (from -r requirements.txt (line 12))\n",
            "  Downloading funasr-1.0.27-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting cn2an (from -r requirements.txt (line 13))\n",
            "  Downloading cn2an-0.5.23-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pypinyin (from -r requirements.txt (line 14))\n",
            "  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyopenjtalk>=0.3.4 (from -r requirements.txt (line 15))\n",
            "  Downloading pyopenjtalk-0.3.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting g2p_en (from -r requirements.txt (line 16))\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting torchaudio (from -r requirements.txt (line 17))\n",
            "  Downloading torchaudio-2.5.1-cp39-cp39-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting modelscope==1.10.0 (from -r requirements.txt (line 18))\n",
            "  Downloading modelscope-1.10.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 19))\n",
            "  Downloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting transformers (from -r requirements.txt (line 20))\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet (from -r requirements.txt (line 21))\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting PyYAML (from -r requirements.txt (line 22))\n",
            "  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting psutil (from -r requirements.txt (line 23))\n",
            "  Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting jieba_fast (from -r requirements.txt (line 24))\n",
            "  Downloading jieba_fast-0.53.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba (from -r requirements.txt (line 25))\n",
            "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting LangSegment>=0.2.0 (from -r requirements.txt (line 26))\n",
            "  Downloading LangSegment-0.3.5-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting Faster_Whisper (from -r requirements.txt (line 27))\n",
            "  Downloading faster_whisper-1.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting wordsegment (from -r requirements.txt (line 28))\n",
            "  Downloading wordsegment-1.3.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting rotary_embedding_torch (from -r requirements.txt (line 29))\n",
            "  Downloading rotary_embedding_torch-0.8.6-py3-none-any.whl.metadata (675 bytes)\n",
            "Collecting pyjyutping (from -r requirements.txt (line 30))\n",
            "  Downloading pyjyutping-1.0.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting g2pk2 (from -r requirements.txt (line 31))\n",
            "  Downloading g2pk2-0.0.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting ko_pron (from -r requirements.txt (line 32))\n",
            "  Downloading ko_pron-1.3-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opencc==1.1.1 (from -r requirements.txt (line 34))\n",
            "  Downloading OpenCC-1.1.1-py2.py3-none-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting python_mecab_ko (from -r requirements.txt (line 35))\n",
            "  Downloading python_mecab_ko-1.3.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting fastapi<0.112.2 (from -r requirements.txt (line 36))\n",
            "  Downloading fastapi-0.112.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting audioread>=2.1.9 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting scikit-learn>=0.19.1 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting joblib>=0.14 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting decorator>=4.0.10 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting soundfile>=0.10.2 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
            "Collecting pooch>=1.0 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (23.1)\n",
            "Collecting llvmlite<0.40,>=0.39.0dev0 (from numba==0.56.4->-r requirements.txt (line 5))\n",
            "  Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from numba==0.56.4->-r requirements.txt (line 5)) (68.2.2)\n",
            "Collecting jamo (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting kaldiio>=2.17.0 (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading kaldiio-2.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting torch-complex (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading torch_complex-0.4.4-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting pytorch-wpe (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading pytorch_wpe-0.0.1-py3-none-any.whl.metadata (242 bytes)\n",
            "Collecting editdistance>=0.5.2 (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading editdistance-0.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting oss2 (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading oss2-2.19.1.tar.gz (298 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting jaconv (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading jaconv-0.4.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core>=1.3.2 (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting tensorboardX (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting openai-whisper (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting attrs (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting datasets>=2.14.5 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting einops (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting filelock>=3.3.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gast>=0.2.2 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pandas (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow>=6.2.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pillow-11.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting pyarrow!=9.0.0,>=6.0.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pyarrow-18.1.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting python-dateutil>=2.1 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.31.0)\n",
            "Collecting simplejson>=3.3.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading simplejson-3.19.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting sortedcontainers>=1.5.9 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (1.26.18)\n",
            "Collecting yapf (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading grpcio-1.68.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting six>1.9 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting torch>=2.1.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting typing-extensions>=4.4.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting altair<6.0,>=4.2.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ffmpy (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==0.14.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.14.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub>=0.19.3 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting importlib-resources<7.0,>=1.3 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting jinja2<4.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting markupsafe~=2.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting matplotlib~=3.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting orjson~=3.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.10.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow>=6.2.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pillow-10.4.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pydantic>=2.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting pydub (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.14.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting future (from ffmpeg-python->-r requirements.txt (line 8))\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting coloredlogs (from onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting sympy (from onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting proces>=0.1.7 (from cn2an->-r requirements.txt (line 13))\n",
            "  Downloading proces-0.1.7-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting nltk>=3.2.4 (from g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting inflect>=0.3.1 (from g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading inflect-7.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting distance>=0.1.3 (from g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy (from onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 20))\n",
            "  Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers->-r requirements.txt (line 20))\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers->-r requirements.txt (line 20))\n",
            "  Downloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting py3langid>=0.2.2 (from LangSegment>=0.2.0->-r requirements.txt (line 26))\n",
            "  Downloading py3langid-0.3.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from Faster_Whisper->-r requirements.txt (line 27))\n",
            "  Downloading ctranslate2-4.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting onnxruntime<2,>=1.14 (from Faster_Whisper->-r requirements.txt (line 27))\n",
            "  Downloading onnxruntime-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting av>=11 (from Faster_Whisper->-r requirements.txt (line 27))\n",
            "  Downloading av-14.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting python-mecab-ko-dic (from python_mecab_ko->-r requirements.txt (line 35))\n",
            "  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<0.112.2->-r requirements.txt (line 36))\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting jsonschema>=3.0 (from altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting narwhals>=1.14.2 (from altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading narwhals-1.19.0-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests>=2.25 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tqdm (from -r requirements.txt (line 11))\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading aiohttp-3.11.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting anyio (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2024.12.14)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.4)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zipp>=3.1.0 (from importlib-resources<7.0,>=1.3->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting more-itertools>=8.5.0 (from inflect>=0.3.1->g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting typeguard>=4.0.1 (from inflect>=0.3.1->g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading fonttools-4.55.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting click (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/site-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 4)) (3.10.0)\n",
            "INFO: pip is looking at multiple versions of py3langid to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting py3langid>=0.2.2 (from LangSegment>=0.2.0->-r requirements.txt (line 26))\n",
            "  Downloading py3langid-0.2.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic>=2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pydantic_core-2.27.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.25->modelscope==1.10.0->-r requirements.txt (line 18)) (2.0.4)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (1.16.0)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "\u001b[33mWARNING: typer 0.15.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting tiktoken (from openai-whisper->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading tiktoken-0.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting crcmod>=1.7 (from oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome>=3.4.7 (from oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pynndescent>=0.5 (from umap-learn->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting tomli>=2.0.1 (from yapf->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading propcache-0.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.9/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.27->-r requirements.txt (line 12)) (41.0.7)\n",
            "Collecting exceptiongroup>=1.0.2 (from anyio->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (2.21)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading rpds_py-0.22.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funasr-1.0.27-py3-none-any.whl (693 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m693.7/693.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.10.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading OpenCC-1.1.1-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.24.0-py3-none-any.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-0.14.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading onnxruntime_gpu-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (226.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m226.2/226.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cn2an-0.5.23-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp39-cp39-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl (906.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading LangSegment-0.3.5-py3-none-any.whl (28 kB)\n",
            "Downloading faster_whisper-1.1.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rotary_embedding_torch-0.8.6-py3-none-any.whl (5.6 kB)\n",
            "Downloading pyjyutping-1.0.0-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2pk2-0.0.3-py3-none-any.whl (25 kB)\n",
            "Downloading ko_pron-1.3-py3-none-any.whl (12 kB)\n",
            "Downloading python_mecab_ko-1.3.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m578.6/578.6 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Downloading av-14.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading editdistance-0.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m401.6/401.6 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading grpcio-1.68.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m450.5/450.5 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
            "Downloading inflect-7.4.0-py3-none-any.whl (34 kB)\n",
            "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.6/134.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Downloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading proces-0.1.7-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py3langid-0.2.2-py3-none-any.whl (750 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m750.6/750.6 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-18.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m431.8/431.8 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m780.9/780.9 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.8.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m436.1/436.1 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading simplejson-3.19.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_wpe-0.0.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_complex-0.4.4-py3-none-any.whl (9.1 kB)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.11-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fonttools-4.55.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-1.19.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m256.4/256.4 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
            "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.9/242.9 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.1/124.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
            "Downloading rpds_py-0.22.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (382 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m382.3/382.3 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m321.5/321.5 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: pyopenjtalk, jieba_fast, jieba, distance, antlr4-python3-runtime, jaconv, openai-whisper, oss2, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for pyopenjtalk (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyopenjtalk: filename=pyopenjtalk-0.3.4-cp39-cp39-linux_x86_64.whl size=1209991 sha256=6b29064915e04c167fbcc8a37693dd482bc7ec51842c58fd6d038cc5c2ccc52d\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/a4/ce/2fd3035dc55d8dc9f20cffae905f43cc79517aa7560bb856e4\n",
            "  Building wheel for jieba_fast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba_fast: filename=jieba_fast-0.53-cp39-cp39-linux_x86_64.whl size=7628765 sha256=537e6ef5e890486d5be386d6b294ed069ff4ffd4579098f8f4aa908b9b04c114\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/04/c8/5c563e7f58588aadfa5af1353a086ef467eb1dadd2fcfac622\n",
            "  Building wheel for jieba (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=dc70c082575d72159446a8060085a3e719d96fde4b4efe2bdb77fbd1c84d1901\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=58efd16b895607fba498e2e31d16070618ced8530131a424134bc332a66bada8\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/b3/aa/04241cced6d1722b132273b1d6aafba317887ec004f48b853a\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=80d3c7735cabefced9e0127486c70728a823f23e9855f9722290e6314f80aeaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.4.0-py3-none-any.whl size=18227 sha256=5b798441317d19f20d212efdacb081c12177548d7dba49738152a1b2a987a753\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/6b/fa/9574efaca6aced07c97ab08d7e40a5cdf8f31ecbe73d55e077\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=3e09516b5b7c1d216d56d0f478acad63c795ce8531dbed0ea4b478a49982ea30\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/29/f3/3dd4d7f88df5d701acd3206732dcb6265379c5ece94b472c17\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.19.1-py3-none-any.whl size=123938 sha256=1391e1cc2060a73ea3ac17d0d23db3a5da587b3de1d1472b35bdb8e006e9f9c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/f9/6e/25b9a00f60e4cc7db56fb53f60546568ac3f697e32e3bff38b\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535317 sha256=1f6c5b744c13d1025f52248077417b84484595c4dc5ca0e136ac8b2fa5d1c957\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/85/b9/f7c05b089e7fad969001438f8a0175c7233a8635b49d5af57e\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp39-cp39-linux_x86_64.whl size=23178 sha256=d0d74fd1cd0827ff2ed506de0439ab074b088311f6721b0d8f832f862729825a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
            "Successfully built pyopenjtalk jieba_fast jieba distance antlr4-python3-runtime jaconv openai-whisper oss2 aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: wordsegment, sortedcontainers, sentencepiece, pytz, python-mecab-ko-dic, pyjyutping, pydub, opencc, mpmath, ko_pron, jieba_fast, jieba, jamo, jaconv, flatbuffers, distance, crcmod, antlr4-python3-runtime, addict, zipp, xxhash, websockets, tzdata, typing-extensions, tqdm, tomlkit, tomli, threadpoolctl, tensorboard-data-server, sympy, sniffio, six, simplejson, shellingham, semantic-version, safetensors, ruff, rpds-py, requests, regex, PyYAML, python-multipart, python_mecab_ko, pypinyin, pyparsing, pygments, pycryptodome, pyarrow, psutil, protobuf, propcache, proces, Pillow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, narwhals, more-itertools, mdurl, markupsafe, llvmlite, kiwisolver, joblib, jmespath, humanfriendly, h11, grpcio, gast, future, fsspec, frozenlist, fonttools, filelock, ffmpy, exceptiongroup, einops, editdistance, dill, decorator, cycler, click, chardet, av, audioread, attrs, async-timeout, annotated-types, aiohappyeyeballs, aiofiles, absl-py, yapf, werkzeug, uvicorn, triton, torch-complex, tiktoken, tensorboardX, soundfile, scipy, referencing, pytorch-wpe, python-dateutil, pyopenjtalk, pydantic-core, py3langid, pooch, omegaconf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, nltk, multiprocess, multidict, markdown-it-py, lightning-utilities, kaldiio, jinja2, importlib-resources, importlib-metadata, huggingface-hub, httpcore, ffmpeg-python, ctranslate2, contourpy, coloredlogs, cn2an, anyio, aiosignal, yarl, typeguard, tokenizers, starlette, scikit-learn, rich, resampy, pydantic, pandas, onnxruntime-gpu, onnxruntime, nvidia-cusolver-cu12, matplotlib, markdown, LangSegment, jsonschema-specifications, hydra-core, httpx, g2pk2, aliyun-python-sdk-core, typer, transformers, torch, tensorboard, pynndescent, librosa, jsonschema, inflect, gradio-client, Faster_Whisper, fastapi, aliyun-python-sdk-kms, aiohttp, umap-learn, torchmetrics, torchaudio, rotary_embedding_torch, oss2, openai-whisper, g2p_en, altair, pytorch-lightning, gradio, funasr, datasets, modelscope\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "Successfully installed Faster_Whisper-1.1.0 LangSegment-0.3.5 Pillow-10.4.0 PyYAML-6.0.2 absl-py-2.1.0 addict-2.4.0 aiofiles-23.2.1 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 altair-5.5.0 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 anyio-4.7.0 async-timeout-5.0.1 attrs-24.3.0 audioread-3.0.1 av-14.0.1 chardet-5.2.0 click-8.1.8 cn2an-0.5.23 coloredlogs-15.0.1 contourpy-1.3.0 crcmod-1.7 ctranslate2-4.5.0 cycler-0.12.1 datasets-3.2.0 decorator-5.1.1 dill-0.3.8 distance-0.1.3 editdistance-0.8.1 einops-0.8.0 exceptiongroup-1.2.2 fastapi-0.112.1 ffmpeg-python-0.2.0 ffmpy-0.5.0 filelock-3.16.1 flatbuffers-24.3.25 fonttools-4.55.3 frozenlist-1.5.0 fsspec-2024.9.0 funasr-1.0.27 future-1.0.0 g2p_en-2.1.0 g2pk2-0.0.3 gast-0.6.0 gradio-4.24.0 gradio-client-0.14.0 grpcio-1.68.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.27.0 humanfriendly-10.0 hydra-core-1.3.2 importlib-metadata-8.5.0 importlib-resources-6.4.5 inflect-7.4.0 jaconv-0.4.0 jamo-0.4.1 jieba-0.42.1 jieba_fast-0.53 jinja2-3.1.5 jmespath-0.10.0 joblib-1.4.2 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 kaldiio-2.18.0 kiwisolver-1.4.7 ko_pron-1.3 librosa-0.9.2 lightning-utilities-0.11.9 llvmlite-0.39.1 markdown-3.7 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.9.4 mdurl-0.1.2 modelscope-1.10.0 more-itertools-10.5.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 narwhals-1.19.0 networkx-3.2.1 nltk-3.9.1 numba-0.56.4 numpy-1.23.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 omegaconf-2.3.0 onnxruntime-1.19.2 onnxruntime-gpu-1.19.2 openai-whisper-20240930 opencc-1.1.1 orjson-3.10.12 oss2-2.19.1 pandas-2.2.3 pooch-1.8.2 proces-0.1.7 propcache-0.2.1 protobuf-5.29.2 psutil-6.1.1 py3langid-0.2.2 pyarrow-18.1.0 pycryptodome-3.21.0 pydantic-2.10.4 pydantic-core-2.27.2 pydub-0.25.1 pygments-2.18.0 pyjyutping-1.0.0 pynndescent-0.5.13 pyopenjtalk-0.3.4 pyparsing-3.2.0 pypinyin-0.53.0 python-dateutil-2.9.0.post0 python-mecab-ko-dic-2.1.1.post2 python-multipart-0.0.20 python_mecab_ko-1.3.7 pytorch-lightning-2.5.0.post0 pytorch-wpe-0.0.1 pytz-2024.2 referencing-0.35.1 regex-2024.11.6 requests-2.32.3 resampy-0.4.3 rich-13.9.4 rotary_embedding_torch-0.8.6 rpds-py-0.22.3 ruff-0.8.4 safetensors-0.4.5 scikit-learn-1.6.0 scipy-1.13.1 semantic-version-2.10.0 sentencepiece-0.2.0 shellingham-1.5.4 simplejson-3.19.3 six-1.17.0 sniffio-1.3.1 sortedcontainers-2.4.0 soundfile-0.12.1 starlette-0.38.6 sympy-1.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorboardX-2.6.2.2 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.21.0 tomli-2.2.1 tomlkit-0.12.0 torch-2.5.1 torch-complex-0.4.4 torchaudio-2.5.1 torchmetrics-1.6.0 tqdm-4.67.1 transformers-4.47.1 triton-3.1.0 typeguard-4.4.1 typer-0.15.1 typing-extensions-4.12.2 tzdata-2024.2 umap-learn-0.5.7 uvicorn-0.34.0 websockets-11.0.3 werkzeug-3.1.3 wordsegment-1.3.1 xxhash-3.5.0 yapf-0.43.0 yarl-1.18.3 zipp-3.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download pretrained models ‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã\n",
        "!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/uvr5\n",
        "%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS\n",
        "%cd /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n",
        "# @title UVR5 pretrains ÂÆâË£Öuvr5Ê®°Âûã\n",
        "%cd /content/GPT-SoVITS/tools/uvr5\n",
        "%rm -r uvr5_weights\n",
        "!git clone https://huggingface.co/Delik/uvr5_weights\n",
        "!git config core.sparseCheckout true\n",
        "!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/"
      ],
      "metadata": {
        "id": "0NgxXg5sjv7z",
        "outputId": "fed3b58e-aa79-4082-f171-4d9172492542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
            "fatal: destination path 'GPT-SoVITS' already exists and is not an empty directory.\n",
            "/content/GPT-SoVITS/tools/damo_asr/models\n",
            "fatal: destination path 'speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch' already exists and is not an empty directory.\n",
            "fatal: destination path 'speech_fsmn_vad_zh-cn-16k-common-pytorch' already exists and is not an empty directory.\n",
            "fatal: destination path 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch' already exists and is not an empty directory.\n",
            "/content/GPT-SoVITS/tools/uvr5\n",
            "Cloning into 'uvr5_weights'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (15/15), 3.25 KiB | 833.00 KiB/s, done.\n",
            "Filtering content: 100% (9/9), 594.44 MiB | 96.65 MiB/s, done.\n",
            "mv: cannot stat '/content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title launch WebUI ÂêØÂä®WebUI\n",
        "!/usr/local/bin/pip install ipykernel\n",
        "!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\n",
        "%cd /content/GPT-SoVITS/\n",
        "!/usr/local/bin/python  webui.py"
      ],
      "metadata": {
        "id": "4oRGUzkrk8C7",
        "outputId": "52d5631d-6499-4ba9-f8cd-d0ee41234b60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel)\n",
            "  Downloading debugpy-1.8.11-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting ipython>=7.23.1 (from ipykernel)\n",
            "  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
            "  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
            "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting nest-asyncio (from ipykernel)\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from ipykernel) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from ipykernel) (6.1.1)\n",
            "Collecting pyzmq>=24 (from ipykernel)\n",
            "  Downloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting tornado>=6.1 (from ipykernel)\n",
            "  Downloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting traitlets>=5.4.0 (from ipykernel)\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
            "Collecting stack-data (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
            "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.21.0)\n",
            "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
            "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading debugpy-1.8.11-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m808.2/808.2 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
            "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Downloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m862.1/862.1 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m437.2/437.2 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
            "Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: wcwidth, pure-eval, ptyprocess, traitlets, tornado, pyzmq, prompt-toolkit, pexpect, parso, nest-asyncio, executing, debugpy, asttokens, stack-data, matplotlib-inline, jupyter-core, jedi, comm, jupyter-client, ipython, ipykernel\n",
            "Successfully installed asttokens-3.0.0 comm-0.2.2 debugpy-1.8.11 executing-2.1.0 ipykernel-6.29.5 ipython-8.18.1 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.7.2 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 parso-0.8.4 pexpect-4.9.0 prompt-toolkit-3.0.48 ptyprocess-0.7.0 pure-eval-0.2.3 pyzmq-26.2.0 stack-data-0.6.3 tornado-6.4.2 traitlets-5.14.3 wcwidth-0.2.13\n",
            "/content/GPT-SoVITS\n",
            "Downloading g2pw model...\n",
            "Extracting g2pw model...\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://3d058a5c1b20f215d4.gradio.live\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/inference_webui_fast.py \"Auto\"\n",
            "---------------------------------------------TTS Config---------------------------------------------\n",
            "device              : cuda\n",
            "is_half             : True\n",
            "version             : v2\n",
            "t2s_weights_path    : GPT_weights_v2/qisili-e10.ckpt\n",
            "vits_weights_path   : SoVITS_weights_v2/qisili_e20_s480.pth\n",
            "bert_base_path      : GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\n",
            "cnhuhbert_base_path : GPT_SoVITS/pretrained_models/chinese-hubert-base\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Loading Text2Semantic weights from GPT_weights_v2/qisili-e10.ckpt\n",
            "Loading VITS weights from SoVITS_weights_v2/qisili_e20_s480.pth\n",
            "Loading BERT weights from GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\n",
            "Loading CNHuBERT weights from GPT_SoVITS/pretrained_models/chinese-hubert-base\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://709bcc371e0a344605.gradio.live\n",
            "Set seed to 903215922\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "ÂΩìÂâç‰ΩøÁî®g2pwËøõË°åÊãºÈü≥Êé®ÁêÜ\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba_fast:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "DEBUG:jieba_fast:Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 0.712 seconds.\n",
            "DEBUG:jieba_fast:Loading model cost 0.712 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "DEBUG:jieba_fast:Prefix dict has been built succesfully.\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ã‰∏ÄÂºÄÂßãÔºåÂ∞èÁéãÂ≠êÂëäËØâËØªËÄÖ‰ªñÂ∞èÊó∂ÂÄô‰∏∫‰∫ÜÁîª‰∏ÄÂè™ÁæäÔºå‰∏éÊàê‰∫∫‰ª¨‰∫âÊâß‰∏ç‰ºë„ÄÇ‰ªñÂõ†Ê≠§ÊÑüÂà∞Â≠§Áã¨ÔºåÊúÄÁªàÁ¶ªÂºÄÂ∞èË°åÊòüÔºåÂú®ÂÆáÂÆô‰∏≠ÂØªÊâæÊúãÂèã„ÄÇ\n",
            "‰ªñÂÖàÂêéÂà∞ËÆø‰∫ÜÂÖ≠‰∏™Ë°åÊòüÔºåÁªìËØÜ‰∫ÜÂ•áÁâπÁöÑÂ±ÖÊ∞ëÔºåÂåÖÊã¨ÈÖíÈ¨º„ÄÅÂêπÈ£éÁÅØ‰∫∫„ÄÅÂêûË±°ËüíÁ≠â„ÄÇÁÑ∂ËÄåÔºå‰ªñÂπ∂Êú™‰ªé‰ªñ‰ª¨ÈÇ£ÈáåÊâæÂà∞ÁúüÊ≠£ÁöÑÂèãÊÉÖÂíåÁêÜËß£„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ã‰∏ÄÂºÄÂßãÔºåÂ∞èÁéãÂ≠êÂëäËØâËØªËÄÖ‰ªñÂ∞èÊó∂ÂÄô‰∏∫‰∫ÜÁîª‰∏ÄÂè™ÁæäÔºå‰∏éÊàê‰∫∫‰ª¨‰∫âÊâß‰∏ç‰ºë„ÄÇ‰ªñÂõ†Ê≠§ÊÑüÂà∞Â≠§Áã¨ÔºåÊúÄÁªàÁ¶ªÂºÄÂ∞èË°åÊòüÔºåÂú®ÂÆáÂÆô‰∏≠ÂØªÊâæÊúãÂèã„ÄÇ', '‰ªñÂÖàÂêéÂà∞ËÆø‰∫ÜÂÖ≠‰∏™Ë°åÊòüÔºåÁªìËØÜ‰∫ÜÂ•áÁâπÁöÑÂ±ÖÊ∞ëÔºåÂåÖÊã¨ÈÖíÈ¨º„ÄÅÂêπÈ£éÁÅØ‰∫∫„ÄÅÂêûË±°ËüíÁ≠â„ÄÇÁÑ∂ËÄåÔºå‰ªñÂπ∂Êú™‰ªé‰ªñ‰ª¨ÈÇ£ÈáåÊâæÂà∞ÁúüÊ≠£ÁöÑÂèãÊÉÖÂíåÁêÜËß£„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 2/2 [00:01<00:00,  1.31it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['‰ªñÂÖàÂêéÂà∞ËÆø‰∫ÜÂÖ≠‰∏™Ë°åÊòü,ÁªìËØÜ‰∫ÜÂ•áÁâπÁöÑÂ±ÖÊ∞ë,ÂåÖÊã¨ÈÖíÈ¨º,ÂêπÈ£éÁÅØ‰∫∫,ÂêûË±°ËüíÁ≠â.ÁÑ∂ËÄå,‰ªñÂπ∂Êú™‰ªé‰ªñ‰ª¨ÈÇ£ÈáåÊâæÂà∞ÁúüÊ≠£ÁöÑÂèãÊÉÖÂíåÁêÜËß£.', 'ÊïÖ‰∫ã‰∏ÄÂºÄÂßã,Â∞èÁéãÂ≠êÂëäËØâËØªËÄÖ‰ªñÂ∞èÊó∂ÂÄô‰∏∫‰∫ÜÁîª‰∏ÄÂè™Áæä,‰∏éÊàê‰∫∫‰ª¨‰∫âÊâß‰∏ç‰ºë.‰ªñÂõ†Ê≠§ÊÑüÂà∞Â≠§Áã¨,ÊúÄÁªàÁ¶ªÂºÄÂ∞èË°åÊòü,Âú®ÂÆáÂÆô‰∏≠ÂØªÊâæÊúãÂèã.']\n",
            " 23% 349/1500 [00:06<00:17, 66.87it/s]T2S Decoding EOS [139 -> 493]\n",
            " 24% 353/1500 [00:06<00:22, 50.82it/s]\n",
            "6.844\t1.534\t7.081\t1.222\n",
            "Set seed to 2797249034\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Â∞èÁéãÂ≠êÊù•Âà∞‰∏ÄÈ¢óÂêç‰∏∫Âú∞ÁêÉÁöÑË°åÊòü‰∏äÔºåÈÅáËßÅ‰∫Ü‰∏ÄÂè™ÁãêÁã∏„ÄÇÁãêÁã∏ÂëäËØâÂ∞èÁéãÂ≠êÔºåÂè™ÊúâÈÄöËøá‰∫íÁõ∏È©ØÊúçÔºåÊâçËÉΩÂª∫Á´ãÁúüÊ≠£ÁöÑÂèãË∞ä„ÄÇ\n",
            "Â∞èÁéãÂ≠êÁî®ÂøÉÈ©ØÊúç‰∫ÜÁãêÁã∏ÔºåÂπ∂‰∏éÁãêÁã∏Âª∫Á´ã‰∫ÜÊ∑±ÂéöÁöÑÊÉÖÊÑüÂÖ≥Á≥ª„ÄÇÂú®Á¶ªÂà´‰πãÈôÖÔºåÁãêÁã∏ÂëäËØâÂ∞èÁéãÂ≠êÔºå‰ªñÂØπÁãêÁã∏Êù•ËØ¥ÊòØÁã¨‰∏ÄÊó†‰∫åÁöÑÔºåÂ∏åÊúõÂ∞èÁéãÂ≠êÊØèÂ§©ÈÉΩÊù•ÁúãÊúõÂÆÉÔºå‰Ωø‰ªñ‰ª¨ÁöÑÂèãË∞äÊõ¥Âä†Áâ¢Âõ∫„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Â∞èÁéãÂ≠êÊù•Âà∞‰∏ÄÈ¢óÂêç‰∏∫Âú∞ÁêÉÁöÑË°åÊòü‰∏äÔºåÈÅáËßÅ‰∫Ü‰∏ÄÂè™ÁãêÁã∏„ÄÇÁãêÁã∏ÂëäËØâÂ∞èÁéãÂ≠êÔºåÂè™ÊúâÈÄöËøá‰∫íÁõ∏È©ØÊúçÔºåÊâçËÉΩÂª∫Á´ãÁúüÊ≠£ÁöÑÂèãË∞ä„ÄÇ', 'Â∞èÁéãÂ≠êÁî®ÂøÉÈ©ØÊúç‰∫ÜÁãêÁã∏ÔºåÂπ∂‰∏éÁãêÁã∏Âª∫Á´ã‰∫ÜÊ∑±ÂéöÁöÑÊÉÖÊÑüÂÖ≥Á≥ª„ÄÇÂú®Á¶ªÂà´‰πãÈôÖÔºåÁãêÁã∏ÂëäËØâÂ∞èÁéãÂ≠êÔºå‰ªñÂØπÁãêÁã∏Êù•ËØ¥ÊòØÁã¨‰∏ÄÊó†‰∫åÁöÑÔºåÂ∏åÊúõÂ∞èÁéãÂ≠êÊØèÂ§©ÈÉΩÊù•ÁúãÊúõÂÆÉÔºå‰Ωø‰ªñ‰ª¨ÁöÑÂèãË∞äÊõ¥Âä†Áâ¢Âõ∫„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 2/2 [00:01<00:00,  1.26it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['Â∞èÁéãÂ≠êÊù•Âà∞‰∏ÄÈ¢óÂêç‰∏∫Âú∞ÁêÉÁöÑË°åÊòü‰∏ä,ÈÅáËßÅ‰∫Ü‰∏ÄÂè™ÁãêÁã∏.ÁãêÁã∏ÂëäËØâÂ∞èÁéãÂ≠ê,Âè™ÊúâÈÄöËøá‰∫íÁõ∏È©ØÊúç,ÊâçËÉΩÂª∫Á´ãÁúüÊ≠£ÁöÑÂèãË∞ä.', 'Â∞èÁéãÂ≠êÁî®ÂøÉÈ©ØÊúç‰∫ÜÁãêÁã∏,Âπ∂‰∏éÁãêÁã∏Âª∫Á´ã‰∫ÜÊ∑±ÂéöÁöÑÊÉÖÊÑüÂÖ≥Á≥ª.Âú®Á¶ªÂà´‰πãÈôÖ,ÁãêÁã∏ÂëäËØâÂ∞èÁéãÂ≠ê,‰ªñÂØπÁãêÁã∏Êù•ËØ¥ÊòØÁã¨‰∏ÄÊó†‰∫åÁöÑ,Â∏åÊúõÂ∞èÁéãÂ≠êÊØèÂ§©ÈÉΩÊù•ÁúãÊúõÂÆÉ,‰Ωø‰ªñ‰ª¨ÁöÑÂèãË∞äÊõ¥Âä†Áâ¢Âõ∫.']\n",
            " 29% 429/1500 [00:07<00:14, 73.61it/s]T2S Decoding EOS [139 -> 573]\n",
            " 29% 433/1500 [00:07<00:18, 58.14it/s]\n",
            "0.000\t1.586\t7.451\t0.208\n",
            "Set seed to 3998702113\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Â∞èÁéãÂ≠êÊù•Âà∞‰∏ÄÈ¢óÁã¨ÁâπÁöÑË°åÊòüÔºå‰∏äÈù¢Â±Ö‰ΩèÁùÄ‰∏Ä‰ΩçËá™Áß∞‰∏∫‚ÄúÂõΩÁéã‚ÄùÁöÑÁî∑Â≠ê„ÄÇÂõΩÁéãËá™Â∞Å‰∏∫‚ÄúÁªüÊ≤ªËÄÖ‚ÄùÔºåËÆ§‰∏∫Ëá™Â∑±ÂèØ‰ª•ÊîØÈÖç‰∏ÄÂàá„ÄÇÁÑ∂ËÄåÔºåÂ∞èÁéãÂ≠êÈÄöËøáËÅ™ÊòéÁöÑÊèêÈóÆÊè≠Á§∫‰∫ÜÂõΩÁéãÁöÑËôöÂ¶ÑÂíåËçíË∞¨„ÄÇÂ∞èÁéãÂ≠êÈÄêÊ∏êÊòéÁôΩ‰∫ÜÊùÉÂäõ‰∏éÁªüÊ≤ªÁöÑËôöÂπªÔºåÂπ∂Á¶ªÂºÄ‰∫ÜËøôÈ¢óË°åÊòü„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Â∞èÁéãÂ≠êÊù•Âà∞‰∏ÄÈ¢óÁã¨ÁâπÁöÑË°åÊòüÔºå‰∏äÈù¢Â±Ö‰ΩèÁùÄ‰∏Ä‰ΩçËá™Áß∞‰∏∫‚ÄúÂõΩÁéã‚ÄùÁöÑÁî∑Â≠ê„ÄÇÂõΩÁéãËá™Â∞Å‰∏∫‚ÄúÁªüÊ≤ªËÄÖ‚ÄùÔºåËÆ§‰∏∫Ëá™Â∑±ÂèØ‰ª•ÊîØÈÖç‰∏ÄÂàá„ÄÇ', 'ÁÑ∂ËÄåÔºåÂ∞èÁéãÂ≠êÈÄöËøáËÅ™ÊòéÁöÑÊèêÈóÆÊè≠Á§∫‰∫ÜÂõΩÁéãÁöÑËôöÂ¶ÑÂíåËçíË∞¨„ÄÇÂ∞èÁéãÂ≠êÈÄêÊ∏êÊòéÁôΩ‰∫ÜÊùÉÂäõ‰∏éÁªüÊ≤ªÁöÑËôöÂπªÔºåÂπ∂Á¶ªÂºÄ‰∫ÜËøôÈ¢óË°åÊòü„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 2/2 [00:02<00:00,  1.34s/it]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['Â∞èÁéãÂ≠êÊù•Âà∞‰∏ÄÈ¢óÁã¨ÁâπÁöÑË°åÊòü,‰∏äÈù¢Â±Ö‰ΩèÁùÄ‰∏Ä‰ΩçËá™Áß∞‰∏∫ÂõΩÁéãÁöÑÁî∑Â≠ê.ÂõΩÁéãËá™Â∞Å‰∏∫ÁªüÊ≤ªËÄÖ,ËÆ§‰∏∫Ëá™Â∑±ÂèØ‰ª•ÊîØÈÖç‰∏ÄÂàá.', 'ÁÑ∂ËÄå,Â∞èÁéãÂ≠êÈÄöËøáËÅ™ÊòéÁöÑÊèêÈóÆÊè≠Á§∫‰∫ÜÂõΩÁéãÁöÑËôöÂ¶ÑÂíåËçíË∞¨.Â∞èÁéãÂ≠êÈÄêÊ∏êÊòéÁôΩ‰∫ÜÊùÉÂäõ‰∏éÁªüÊ≤ªÁöÑËôöÂπª,Âπ∂Á¶ªÂºÄ‰∫ÜËøôÈ¢óË°åÊòü.']\n",
            " 20% 293/1500 [00:05<00:16, 71.67it/s]T2S Decoding EOS [139 -> 440]\n",
            " 20% 300/1500 [00:05<00:20, 57.61it/s]\n",
            "0.000\t2.688\t5.215\t0.462\n",
            "Set seed to 1415003300\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå\n",
            "Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå\n",
            "‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ\n",
            "Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ\n",
            "Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå\n",
            "ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ\n",
            "\n",
            "Âú®‰ªñÁöÑÊóÖÈÄî‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆøÈóÆ‰∫ÜÂ§ö‰∏™Â∞èË°åÊòüÔºåÊØè‰∏™ÊòüÁêÉ‰∏äÈÉΩÊúâ‰∏çÂêåÁöÑ‰∫∫Áâ©ÔºåËøô‰∫õ‰∫∫Áâ©‰ª£Ë°®‰∫ÜÊàê‰∫∫‰∏ñÁïå‰∏≠ÁöÑÂêÑÁßçÁâπË¥®„ÄÇ\n",
            "Â∞èÁéãÂ≠êÈ¶ñÂÖàÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçËá™‰ª•‰∏∫ÊòØÁöÑÂõΩÁéãÔºå‰ªñÂ£∞Áß∞Ëá™Â∑±ÁªüÊ≤ªÁùÄÊï¥‰∏™ÂÆáÂÆô„ÄÇÂõΩÁéãËØïÂõæÂëΩ‰ª§‰∏ÄÂàáÔºå‰ΩÜÂÆûÈôÖ‰∏ä‰ªñÊâÄÁªüÊ≤ªÁöÑÂè™ÊòØ‰∏Ä‰∏™Á©∫Ëç°Ëç°ÁöÑÂ∞èÊòüÁêÉ„ÄÇ\n",
            "ÂõΩÁéãÁöÑÂÇ≤ÊÖ¢ËÆ©Â∞èÁéãÂ≠êÊÑüÂà∞Âõ∞ÊÉëÔºåÂõ†‰∏∫‰ªñÂπ∂Ê≤°ÊúâÁúüÊ≠£ÁöÑÊùÉÂäõÔºåÂè™ÊòØÂú®Ëá™ÊàëÂÆâÊÖ∞„ÄÇÊé•‰∏ãÊù•ÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçËôöËç£ÁöÑ‰∫∫Ôºå\n",
            "Ëøô‰∏™‰∫∫Âè™ÂÖ≥ÂøÉÂà´‰∫∫ÁöÑËµûÁæé„ÄÇ‰ªñÂ∏åÊúõÊØè‰∏™‰∫∫ÈÉΩËÉΩËµûÁæé‰ªñÔºåÂπ∂‰∏îÂè™Âú®‰πéËá™Â∑±ÁöÑÂΩ¢Ë±°„ÄÇÂ∞èÁéãÂ≠êÊÑèËØÜÂà∞ÔºåËøôÁßçËôöËç£ÊòØÂ§ö‰πàËÇ§ÊµÖÔºå\n",
            "Êó†Ê≥ïÂ∏¶Êù•ÁúüÊ≠£ÁöÑÂø´‰πê„ÄÇÂú®Á¨¨‰∏â‰∏™ÊòüÁêÉÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçÊ≤âËø∑‰∫éÈÖíÁ≤æÁöÑÈÖíÈ¨º„ÄÇÈÖíÈ¨ºÂñùÈÖíÊòØ‰∏∫‰∫ÜÂøòËÆ∞Ëá™Â∑±ÁöÑÊÇ≤‰º§Ôºå‰ΩÜËøôÂèàËÆ©‰ªñÈô∑ÂÖ•‰∫ÜÊõ¥Ê∑±ÁöÑÁªùÊúõ„ÄÇ\n",
            "Â∞èÁéãÂ≠êÂØπËøô‰∏™‰∫∫ÊÑüÂà∞ÂêåÊÉÖÔºå‰ΩÜ‰πüÊòéÁôΩËøôÁßçÈÄÉÈÅøÂπ∂‰∏çÊòØËß£ÂÜ≥ÈóÆÈ¢òÁöÑÊñπÊ≥ï„ÄÇÈöèÂêéÔºåÂ∞èÁéãÂ≠êËßÅÂà∞‰∫Ü‰∏Ä‰∏™Âøô‰∫éËÆ°ÁÆóË¥¢ÂØåÁöÑÂïÜ‰∫∫„ÄÇ\n",
            "‰ªñÊï¥Â§©Âøô‰∫éËµöÈí±ÔºåÂç¥‰ªéÊú™‰∫´ÂèóËøáÁîüÊ¥ª‰∏≠ÁöÑÁæéÂ•Ω‰∫ãÁâ©„ÄÇÂïÜ‰∫∫ÂëäËØâÂ∞èÁéãÂ≠êÔºå‰ªñÊã•ÊúâËÆ∏Â§öÊòüÊòüÔºåÂõ†‰∏∫‰ªñÊã•ÊúâÂÆÉ‰ª¨ÁöÑÊâÄÊúâÊùÉ„ÄÇ\n",
            "Â∞èÁéãÂ≠êÂØπÊ≠§ÊÑüÂà∞Âõ∞ÊÉëÔºåÂõ†‰∏∫‰ªñËÆ§‰∏∫ÁúüÊ≠£ÈáçË¶ÅÁöÑÊòØ‰∫´ÂèóÁîüÊ¥ªÔºåËÄå‰∏çÊòØÂçïÁ∫ØËøΩÊ±ÇÁâ©Ë¥®Ë¥¢ÂØå„ÄÇÂ∞èÁéãÂ≠êÁöÑ‰∏ã‰∏Ä‰∏™ÈÅáËßÅÊòØ‰∏Ä‰∏™ÁÇπÁÅØ‰∫∫Ôºå\n",
            "‰ªñÊØèÂ§©ÈÉΩË¶ÅÁÇπ‰∫ÆÂíåÁÜÑÁÅ≠ÁÅØÂÖâÔºåÂ∞ΩÁÆ°‰ªñÁöÑÂ∑•‰ΩúÊØ´Êó†ÊÑè‰πâ„ÄÇËøô‰ΩçÁÇπÁÅØ‰∫∫ÈùûÂ∏∏Âø†ËØöÔºå‰ΩÜ‰ªñÁöÑÂ∑•‰ΩúÂç¥Ê≤°ÊúâÊó∂Èó¥Âéª‰∫´ÂèóÁîüÊ¥ª„ÄÇÂ∞èÁéãÂ≠ê‰∏∫‰ªñÁöÑÂø†ËØöÊÑüÂà∞Ê¨£ÊÖ∞Ôºå\n",
            "‰ΩÜ‰πüÊÑèËØÜÂà∞Ëøô‰ªΩÂø†ËØöÂπ∂‰∏çËÉΩÂ∏¶Êù•Âø´‰πê„ÄÇÊúÄÂêéÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰∏™Âú∞ÁêÜÂ≠¶ÂÆ∂Ôºå‰ªñÂè™ÂÖ≥Ê≥®‰π¶Êú¨Áü•ËØÜÔºåËÄå‰∏ç‰∫≤Ë∫´‰ΩìÈ™å‰∏ñÁïå„ÄÇ\n",
            "‰ªñÂëäËØâÂ∞èÁéãÂ≠êÔºåËá™Â∑±‰ªéÊú™ËßÅËøáËá™Â∑±ÊâÄËÆ∞ÂΩïÁöÑÂ±±ËÑâ„ÄÅÊ≤≥ÊµÅÊàñÂüéÂ∏Ç„ÄÇËøôËÆ©Â∞èÁéãÂ≠êÊÑèËØÜÂà∞ÔºåÁü•ËØÜ‰∏éÁªèÈ™å‰πãÈó¥ÊúâÁùÄÂ∑®Â§ßÁöÑÂ∑ÆË∑ù„ÄÇ\n",
            "\n",
            "ÈÄöËøáËøô‰∫õÈÅ≠ÈÅáÔºåÂ∞èÁéãÂ≠êÂØπÊàêÂπ¥‰∫∫ÁöÑ‰∏ñÁïåÊÑüÂà∞Âõ∞ÊÉëÂíåÂ§±Êúõ„ÄÇ‰ªñÂèëÁé∞ÊàêÂπ¥‰∫∫ÂæÄÂæÄÊ≤âËø∑‰∫éÁêêÁ¢éÂíåÊó†ÊÑè‰πâÁöÑ‰∫ãÊÉÖÔºåËÄåÂøΩËßÜ‰∫ÜÁîüÊ¥ª‰∏≠ÁúüÊ≠£ÈáçË¶ÅÁöÑ‰∏úË•øÔºå\n",
            "ÊØîÂ¶ÇÁà±„ÄÅÂèãÊÉÖÂíåË¥£‰ªª„ÄÇ\n",
            "ÊúÄÁªàÔºåÂ∞èÁéãÂ≠êÊù•Âà∞‰∫ÜÂú∞ÁêÉ„ÄÇÂú®ËøôÈáåÔºå‰ªñÈÅáËßÅ‰∫Ü‰∏Ä‰ΩçÈ£ûË°åÂëòÔºåÂπ∂‰∏é‰ªñÂª∫Á´ã‰∫ÜÊ∑±ÂéöÁöÑÂèãË∞ä„ÄÇÂú®Ê≤ôÊº†‰∏≠Ôºå\n",
            "‰∏§‰∫∫‰∏ÄËµ∑Â∫¶Ëøá‰∫ÜËÆ∏Â§öÊó∂ÂÖâÔºåÈ£ûË°åÂëòË¢´Â∞èÁéãÂ≠êÁöÑÁ∫ØÁúüÂíåÊô∫ÊÖßÊâÄÂê∏Âºï„ÄÇÂú®Âú∞ÁêÉ‰∏äÔºåÂ∞èÁéãÂ≠êËøòÈÅáÂà∞‰∫ÜÁãêÁã∏„ÄÇÁãêÁã∏Êïô‰ºö‰∫Ü‰ªñÂÖ≥‰∫éÁà±ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ\n",
            "ÁãêÁã∏ÂëäËØâÂ∞èÁéãÂ≠êÔºö‚ÄúÁúüÊ≠£ÈáçË¶ÅÁöÑ‰∏úË•øÔºåÁî®ÁúºÁùõÊòØÁúã‰∏çËßÅÁöÑÔºåÂè™ÊúâÁî®ÂøÉÊâçËÉΩÁúãËßÅ„ÄÇ‚ÄùËøôÂè•ËØùÊ∑±Ê∑±Ëß¶Âä®‰∫ÜÂ∞èÁéãÂ≠êÁöÑÂøÉÁÅµÔºå\n",
            "ËÆ©‰ªñÊÑèËØÜÂà∞Ëá™Â∑±ÂØπÁé´Áë∞Ëä±ÁöÑÁã¨ÁâπÊÉÖÊÑüÔºå‰ΩøÂæóËøôÊúµÁé´Áë∞Âú®‰ªñÂøÉ‰∏≠ÂèòÂæóÊó†ÂèØÊõø‰ª£„ÄÇ\n",
            "ÈÄöËøá‰∏éÁãêÁã∏ÁöÑÂØπËØùÔºåÂ∞èÁéãÂ≠êÂºÄÂßãÂèçÊÄùËá™Â∑±‰∏éÁé´Áë∞‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ\n",
            "‰ªñÊÑèËØÜÂà∞ÔºåÂ∞ΩÁÆ°Áé´Áë∞ÊúâÊó∂ÊòæÂæóÂÇ≤ÊÖ¢Ôºå‰ΩÜÊ≠£ÊòØËøôÁßçÁã¨ÁâπÊÄß‰ΩøÂæóÂ•πÂú®‰ªñÂøÉ‰∏≠ÊúâÁùÄÁâπÊÆäÁöÑ‰ΩçÁΩÆ„ÄÇ‰ªñÂÜ≥ÂÆöÂõûÂà∞Ëá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÁÖßÈ°æÈÇ£ÊúµÁé´Áë∞Ôºå\n",
            "Âπ∂ÊâøÊãÖËµ∑Ëá™Â∑±ÁöÑË¥£‰ªª„ÄÇ\n",
            "ÊïÖ‰∫ãÊé•ËøëÂ∞æÂ£∞Êó∂ÔºåÂ∞èÁéãÂ≠ê‰∏éÈ£ûË°åÂëòÈÅìÂà´ÔºåÂπ∂ÈÄöËøá‰∏ÄÊù°ËõáÁöÑÂ∏ÆÂä©ÂõûÂà∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇÂú®Á¶ªÂºÄ‰πãÂâçÔºåÂ∞èÁéãÂ≠êÂëäËØâÈ£ûË°åÂëòÔºå‰∏çË¶ÅÊÇ≤‰º§ÔºåÂõ†‰∏∫‰ªñ‰ª¨‰πãÈó¥ÁöÑÂèãË∞äÂ∞ÜÊ∞∏ËøúÂ≠òÂú®‰∫éÂøÉ‰∏≠„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå', 'Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå', '‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ', 'Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ', 'Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå', 'ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ', 'Âú®‰ªñÁöÑÊóÖÈÄî‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆøÈóÆ‰∫ÜÂ§ö‰∏™Â∞èË°åÊòüÔºåÊØè‰∏™ÊòüÁêÉ‰∏äÈÉΩÊúâ‰∏çÂêåÁöÑ‰∫∫Áâ©ÔºåËøô‰∫õ‰∫∫Áâ©‰ª£Ë°®‰∫ÜÊàê‰∫∫‰∏ñÁïå‰∏≠ÁöÑÂêÑÁßçÁâπË¥®„ÄÇ', 'Â∞èÁéãÂ≠êÈ¶ñÂÖàÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçËá™‰ª•‰∏∫ÊòØÁöÑÂõΩÁéãÔºå‰ªñÂ£∞Áß∞Ëá™Â∑±ÁªüÊ≤ªÁùÄÊï¥‰∏™ÂÆáÂÆô„ÄÇÂõΩÁéãËØïÂõæÂëΩ‰ª§‰∏ÄÂàáÔºå‰ΩÜÂÆûÈôÖ‰∏ä‰ªñÊâÄÁªüÊ≤ªÁöÑÂè™ÊòØ‰∏Ä‰∏™Á©∫Ëç°Ëç°ÁöÑÂ∞èÊòüÁêÉ„ÄÇ', 'ÂõΩÁéãÁöÑÂÇ≤ÊÖ¢ËÆ©Â∞èÁéãÂ≠êÊÑüÂà∞Âõ∞ÊÉëÔºåÂõ†‰∏∫‰ªñÂπ∂Ê≤°ÊúâÁúüÊ≠£ÁöÑÊùÉÂäõÔºåÂè™ÊòØÂú®Ëá™ÊàëÂÆâÊÖ∞„ÄÇÊé•‰∏ãÊù•ÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçËôöËç£ÁöÑ‰∫∫Ôºå', 'Ëøô‰∏™‰∫∫Âè™ÂÖ≥ÂøÉÂà´‰∫∫ÁöÑËµûÁæé„ÄÇ‰ªñÂ∏åÊúõÊØè‰∏™‰∫∫ÈÉΩËÉΩËµûÁæé‰ªñÔºåÂπ∂‰∏îÂè™Âú®‰πéËá™Â∑±ÁöÑÂΩ¢Ë±°„ÄÇÂ∞èÁéãÂ≠êÊÑèËØÜÂà∞ÔºåËøôÁßçËôöËç£ÊòØÂ§ö‰πàËÇ§ÊµÖÔºå', 'Êó†Ê≥ïÂ∏¶Êù•ÁúüÊ≠£ÁöÑÂø´‰πê„ÄÇÂú®Á¨¨‰∏â‰∏™ÊòüÁêÉÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçÊ≤âËø∑‰∫éÈÖíÁ≤æÁöÑÈÖíÈ¨º„ÄÇÈÖíÈ¨ºÂñùÈÖíÊòØ‰∏∫‰∫ÜÂøòËÆ∞Ëá™Â∑±ÁöÑÊÇ≤‰º§Ôºå‰ΩÜËøôÂèàËÆ©‰ªñÈô∑ÂÖ•‰∫ÜÊõ¥Ê∑±ÁöÑÁªùÊúõ„ÄÇ', 'Â∞èÁéãÂ≠êÂØπËøô‰∏™‰∫∫ÊÑüÂà∞ÂêåÊÉÖÔºå‰ΩÜ‰πüÊòéÁôΩËøôÁßçÈÄÉÈÅøÂπ∂‰∏çÊòØËß£ÂÜ≥ÈóÆÈ¢òÁöÑÊñπÊ≥ï„ÄÇÈöèÂêéÔºåÂ∞èÁéãÂ≠êËßÅÂà∞‰∫Ü‰∏Ä‰∏™Âøô‰∫éËÆ°ÁÆóË¥¢ÂØåÁöÑÂïÜ‰∫∫„ÄÇ', '‰ªñÊï¥Â§©Âøô‰∫éËµöÈí±ÔºåÂç¥‰ªéÊú™‰∫´ÂèóËøáÁîüÊ¥ª‰∏≠ÁöÑÁæéÂ•Ω‰∫ãÁâ©„ÄÇÂïÜ‰∫∫ÂëäËØâÂ∞èÁéãÂ≠êÔºå‰ªñÊã•ÊúâËÆ∏Â§öÊòüÊòüÔºåÂõ†‰∏∫‰ªñÊã•ÊúâÂÆÉ‰ª¨ÁöÑÊâÄÊúâÊùÉ„ÄÇ', 'Â∞èÁéãÂ≠êÂØπÊ≠§ÊÑüÂà∞Âõ∞ÊÉëÔºåÂõ†‰∏∫‰ªñËÆ§‰∏∫ÁúüÊ≠£ÈáçË¶ÅÁöÑÊòØ‰∫´ÂèóÁîüÊ¥ªÔºåËÄå‰∏çÊòØÂçïÁ∫ØËøΩÊ±ÇÁâ©Ë¥®Ë¥¢ÂØå„ÄÇÂ∞èÁéãÂ≠êÁöÑ‰∏ã‰∏Ä‰∏™ÈÅáËßÅÊòØ‰∏Ä‰∏™ÁÇπÁÅØ‰∫∫Ôºå', '‰ªñÊØèÂ§©ÈÉΩË¶ÅÁÇπ‰∫ÆÂíåÁÜÑÁÅ≠ÁÅØÂÖâÔºåÂ∞ΩÁÆ°‰ªñÁöÑÂ∑•‰ΩúÊØ´Êó†ÊÑè‰πâ„ÄÇËøô‰ΩçÁÇπÁÅØ‰∫∫ÈùûÂ∏∏Âø†ËØöÔºå‰ΩÜ‰ªñÁöÑÂ∑•‰ΩúÂç¥Ê≤°ÊúâÊó∂Èó¥Âéª‰∫´ÂèóÁîüÊ¥ª„ÄÇ', 'Â∞èÁéãÂ≠ê‰∏∫‰ªñÁöÑÂø†ËØöÊÑüÂà∞Ê¨£ÊÖ∞Ôºå', '‰ΩÜ‰πüÊÑèËØÜÂà∞Ëøô‰ªΩÂø†ËØöÂπ∂‰∏çËÉΩÂ∏¶Êù•Âø´‰πê„ÄÇÊúÄÂêéÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰∏™Âú∞ÁêÜÂ≠¶ÂÆ∂Ôºå‰ªñÂè™ÂÖ≥Ê≥®‰π¶Êú¨Áü•ËØÜÔºå', 'ËÄå‰∏ç‰∫≤Ë∫´‰ΩìÈ™å‰∏ñÁïå„ÄÇ', '‰ªñÂëäËØâÂ∞èÁéãÂ≠êÔºåËá™Â∑±‰ªéÊú™ËßÅËøáËá™Â∑±ÊâÄËÆ∞ÂΩïÁöÑÂ±±ËÑâ„ÄÅÊ≤≥ÊµÅÊàñÂüéÂ∏Ç„ÄÇËøôËÆ©Â∞èÁéãÂ≠êÊÑèËØÜÂà∞ÔºåÁü•ËØÜ‰∏éÁªèÈ™å‰πãÈó¥ÊúâÁùÄÂ∑®Â§ßÁöÑÂ∑ÆË∑ù„ÄÇ', 'ÈÄöËøáËøô‰∫õÈÅ≠ÈÅáÔºåÂ∞èÁéãÂ≠êÂØπÊàêÂπ¥‰∫∫ÁöÑ‰∏ñÁïåÊÑüÂà∞Âõ∞ÊÉëÂíåÂ§±Êúõ„ÄÇ‰ªñÂèëÁé∞ÊàêÂπ¥‰∫∫ÂæÄÂæÄÊ≤âËø∑‰∫éÁêêÁ¢éÂíåÊó†ÊÑè‰πâÁöÑ‰∫ãÊÉÖÔºåËÄåÂøΩËßÜ‰∫ÜÁîüÊ¥ª‰∏≠ÁúüÊ≠£ÈáçË¶ÅÁöÑ‰∏úË•øÔºå', 'ÊØîÂ¶ÇÁà±„ÄÅÂèãÊÉÖÂíåË¥£‰ªª„ÄÇ', 'ÊúÄÁªàÔºåÂ∞èÁéãÂ≠êÊù•Âà∞‰∫ÜÂú∞ÁêÉ„ÄÇÂú®ËøôÈáåÔºå‰ªñÈÅáËßÅ‰∫Ü‰∏Ä‰ΩçÈ£ûË°åÂëòÔºåÂπ∂‰∏é‰ªñÂª∫Á´ã‰∫ÜÊ∑±ÂéöÁöÑÂèãË∞ä„ÄÇÂú®Ê≤ôÊº†‰∏≠Ôºå', '‰∏§‰∫∫‰∏ÄËµ∑Â∫¶Ëøá‰∫ÜËÆ∏Â§öÊó∂ÂÖâÔºåÈ£ûË°åÂëòË¢´Â∞èÁéãÂ≠êÁöÑÁ∫ØÁúüÂíåÊô∫ÊÖßÊâÄÂê∏Âºï„ÄÇÂú®Âú∞ÁêÉ‰∏äÔºåÂ∞èÁéãÂ≠êËøòÈÅáÂà∞‰∫ÜÁãêÁã∏„ÄÇÁãêÁã∏Êïô‰ºö‰∫Ü‰ªñÂÖ≥‰∫éÁà±ÂíåË¥£‰ªªÁöÑÈáçË¶ÅÊÄß„ÄÇ', 'ÁãêÁã∏ÂëäËØâÂ∞èÁéãÂ≠êÔºö‚ÄúÁúüÊ≠£ÈáçË¶ÅÁöÑ‰∏úË•øÔºåÁî®ÁúºÁùõÊòØÁúã‰∏çËßÅÁöÑÔºåÂè™ÊúâÁî®ÂøÉÊâçËÉΩÁúãËßÅ„ÄÇ‚ÄùËøôÂè•ËØùÊ∑±Ê∑±Ëß¶Âä®‰∫ÜÂ∞èÁéãÂ≠êÁöÑÂøÉÁÅµÔºå', 'ËÆ©‰ªñÊÑèËØÜÂà∞Ëá™Â∑±ÂØπÁé´Áë∞Ëä±ÁöÑÁã¨ÁâπÊÉÖÊÑüÔºå‰ΩøÂæóËøôÊúµÁé´Áë∞Âú®‰ªñÂøÉ‰∏≠ÂèòÂæóÊó†ÂèØÊõø‰ª£„ÄÇ', 'ÈÄöËøá‰∏éÁãêÁã∏ÁöÑÂØπËØùÔºåÂ∞èÁéãÂ≠êÂºÄÂßãÂèçÊÄùËá™Â∑±‰∏éÁé´Áë∞‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ', '‰ªñÊÑèËØÜÂà∞ÔºåÂ∞ΩÁÆ°Áé´Áë∞ÊúâÊó∂ÊòæÂæóÂÇ≤ÊÖ¢Ôºå‰ΩÜÊ≠£ÊòØËøôÁßçÁã¨ÁâπÊÄß‰ΩøÂæóÂ•πÂú®‰ªñÂøÉ‰∏≠ÊúâÁùÄÁâπÊÆäÁöÑ‰ΩçÁΩÆ„ÄÇ‰ªñÂÜ≥ÂÆöÂõûÂà∞Ëá™Â∑±ÁöÑÊòüÁêÉÔºå', 'ÂéªÁÖßÈ°æÈÇ£ÊúµÁé´Áë∞Ôºå', 'Âπ∂ÊâøÊãÖËµ∑Ëá™Â∑±ÁöÑË¥£‰ªª„ÄÇ', 'ÊïÖ‰∫ãÊé•ËøëÂ∞æÂ£∞Êó∂ÔºåÂ∞èÁéãÂ≠ê‰∏éÈ£ûË°åÂëòÈÅìÂà´ÔºåÂπ∂ÈÄöËøá‰∏ÄÊù°ËõáÁöÑÂ∏ÆÂä©ÂõûÂà∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇÂú®Á¶ªÂºÄ‰πãÂâçÔºåÂ∞èÁéãÂ≠êÂëäËØâÈ£ûË°åÂëòÔºå‰∏çË¶ÅÊÇ≤‰º§ÔºåÂõ†‰∏∫‰ªñ‰ª¨‰πãÈó¥ÁöÑÂèãË∞äÂ∞ÜÊ∞∏ËøúÂ≠òÂú®‰∫éÂøÉ‰∏≠„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  3% 1/30 [00:01<00:32,  1.13s/it][nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "  3% 1/30 [00:06<03:04,  6.35s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 2617128860\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå\n",
            "Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå\n",
            "‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ\n",
            "Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ\n",
            "Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå\n",
            "ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ\n",
            "\n",
            "Âú®‰ªñÁöÑÊóÖÈÄî‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆøÈóÆ‰∫ÜÂ§ö‰∏™Â∞èË°åÊòüÔºåÊØè‰∏™ÊòüÁêÉ‰∏äÈÉΩÊúâ‰∏çÂêåÁöÑ‰∫∫Áâ©ÔºåËøô‰∫õ‰∫∫Áâ©‰ª£Ë°®‰∫ÜÊàê‰∫∫‰∏ñÁïå‰∏≠ÁöÑÂêÑÁßçÁâπË¥®„ÄÇ\n",
            "Â∞èÁéãÂ≠êÈ¶ñÂÖàÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçËá™‰ª•‰∏∫ÊòØÁöÑÂõΩÁéãÔºå‰ªñÂ£∞Áß∞Ëá™Â∑±ÁªüÊ≤ªÁùÄÊï¥‰∏™ÂÆáÂÆô„ÄÇÂõΩÁéãËØïÂõæÂëΩ‰ª§‰∏ÄÂàáÔºå‰ΩÜÂÆûÈôÖ‰∏ä‰ªñÊâÄÁªüÊ≤ªÁöÑÂè™ÊòØ‰∏Ä‰∏™Á©∫Ëç°Ëç°ÁöÑÂ∞èÊòüÁêÉ„ÄÇ\n",
            "ÂõΩÁéãÁöÑÂÇ≤ÊÖ¢ËÆ©Â∞èÁéãÂ≠êÊÑüÂà∞Âõ∞ÊÉëÔºåÂõ†‰∏∫‰ªñÂπ∂Ê≤°ÊúâÁúüÊ≠£ÁöÑÊùÉÂäõÔºåÂè™ÊòØÂú®Ëá™ÊàëÂÆâÊÖ∞„ÄÇÊé•‰∏ãÊù•ÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçËôöËç£ÁöÑ‰∫∫Ôºå\n",
            "Ëøô‰∏™‰∫∫Âè™ÂÖ≥ÂøÉÂà´‰∫∫ÁöÑËµûÁæé„ÄÇ‰ªñÂ∏åÊúõÊØè‰∏™‰∫∫ÈÉΩËÉΩËµûÁæé‰ªñÔºåÂπ∂‰∏îÂè™Âú®‰πéËá™Â∑±ÁöÑÂΩ¢Ë±°„ÄÇÂ∞èÁéãÂ≠êÊÑèËØÜÂà∞ÔºåËøôÁßçËôöËç£ÊòØÂ§ö‰πàËÇ§ÊµÖÔºå\n",
            "Êó†Ê≥ïÂ∏¶Êù•ÁúüÊ≠£ÁöÑÂø´‰πê„ÄÇÂú®Á¨¨‰∏â‰∏™ÊòüÁêÉÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçÊ≤âËø∑‰∫éÈÖíÁ≤æÁöÑÈÖíÈ¨º„ÄÇÈÖíÈ¨ºÂñùÈÖíÊòØ‰∏∫‰∫ÜÂøòËÆ∞Ëá™Â∑±ÁöÑÊÇ≤‰º§Ôºå‰ΩÜËøôÂèàËÆ©‰ªñÈô∑ÂÖ•‰∫ÜÊõ¥Ê∑±ÁöÑÁªùÊúõ„ÄÇ\n",
            "Â∞èÁéãÂ≠êÂØπËøô‰∏™‰∫∫ÊÑüÂà∞ÂêåÊÉÖÔºå‰ΩÜ‰πüÊòéÁôΩËøôÁßçÈÄÉÈÅøÂπ∂‰∏çÊòØËß£ÂÜ≥ÈóÆÈ¢òÁöÑÊñπÊ≥ï„ÄÇÈöèÂêéÔºåÂ∞èÁéãÂ≠êËßÅÂà∞‰∫Ü‰∏Ä‰∏™Âøô‰∫éËÆ°ÁÆóË¥¢ÂØåÁöÑÂïÜ‰∫∫„ÄÇ\n",
            "‰ªñÊï¥Â§©Âøô‰∫éËµöÈí±ÔºåÂç¥‰ªéÊú™‰∫´ÂèóËøáÁîüÊ¥ª‰∏≠ÁöÑÁæéÂ•Ω‰∫ãÁâ©„ÄÇÂïÜ‰∫∫ÂëäËØâÂ∞èÁéãÂ≠êÔºå‰ªñÊã•ÊúâËÆ∏Â§öÊòüÊòüÔºåÂõ†‰∏∫‰ªñÊã•ÊúâÂÆÉ‰ª¨ÁöÑÊâÄÊúâÊùÉ„ÄÇ\n",
            "Â∞èÁéãÂ≠êÂØπÊ≠§ÊÑüÂà∞Âõ∞ÊÉëÔºåÂõ†‰∏∫‰ªñËÆ§‰∏∫ÁúüÊ≠£ÈáçË¶ÅÁöÑÊòØ‰∫´ÂèóÁîüÊ¥ªÔºåËÄå‰∏çÊòØÂçïÁ∫ØËøΩÊ±ÇÁâ©Ë¥®Ë¥¢ÂØå„ÄÇÂ∞èÁéãÂ≠êÁöÑ‰∏ã‰∏Ä‰∏™ÈÅáËßÅÊòØ‰∏Ä‰∏™ÁÇπÁÅØ‰∫∫Ôºå\n",
            "‰ªñÊØèÂ§©ÈÉΩË¶ÅÁÇπ‰∫ÆÂíåÁÜÑÁÅ≠ÁÅØÂÖâÔºåÂ∞ΩÁÆ°‰ªñÁöÑÂ∑•‰ΩúÊØ´Êó†ÊÑè‰πâ„ÄÇËøô‰ΩçÁÇπÁÅØ‰∫∫ÈùûÂ∏∏Âø†ËØöÔºå‰ΩÜ‰ªñÁöÑÂ∑•‰ΩúÂç¥Ê≤°ÊúâÊó∂Èó¥Âéª‰∫´ÂèóÁîüÊ¥ª„ÄÇÂ∞èÁéãÂ≠ê‰∏∫‰ªñÁöÑÂø†ËØöÊÑüÂà∞Ê¨£ÊÖ∞Ôºå\n",
            "‰ΩÜ‰πüÊÑèËØÜÂà∞Ëøô‰ªΩÂø†ËØöÂπ∂‰∏çËÉΩÂ∏¶Êù•Âø´‰πê„ÄÇÊúÄÂêéÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰∏™Âú∞ÁêÜÂ≠¶ÂÆ∂Ôºå‰ªñÂè™ÂÖ≥Ê≥®‰π¶Êú¨Áü•ËØÜÔºåËÄå‰∏ç‰∫≤Ë∫´‰ΩìÈ™å‰∏ñÁïå„ÄÇ\n",
            "‰ªñÂëäËØâÂ∞èÁéãÂ≠êÔºåËá™Â∑±‰ªéÊú™ËßÅËøáËá™Â∑±ÊâÄËÆ∞ÂΩïÁöÑÂ±±ËÑâ„ÄÅÊ≤≥ÊµÅÊàñÂüéÂ∏Ç„ÄÇËøôËÆ©Â∞èÁéãÂ≠êÊÑèËØÜÂà∞ÔºåÁü•ËØÜ‰∏éÁªèÈ™å‰πãÈó¥ÊúâÁùÄÂ∑®Â§ßÁöÑÂ∑ÆË∑ù„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå', 'Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå', '‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ', 'Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ', 'Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå', 'ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ', 'Âú®‰ªñÁöÑÊóÖÈÄî‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆøÈóÆ‰∫ÜÂ§ö‰∏™Â∞èË°åÊòüÔºåÊØè‰∏™ÊòüÁêÉ‰∏äÈÉΩÊúâ‰∏çÂêåÁöÑ‰∫∫Áâ©ÔºåËøô‰∫õ‰∫∫Áâ©‰ª£Ë°®‰∫ÜÊàê‰∫∫‰∏ñÁïå‰∏≠ÁöÑÂêÑÁßçÁâπË¥®„ÄÇ', 'Â∞èÁéãÂ≠êÈ¶ñÂÖàÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçËá™‰ª•‰∏∫ÊòØÁöÑÂõΩÁéãÔºå‰ªñÂ£∞Áß∞Ëá™Â∑±ÁªüÊ≤ªÁùÄÊï¥‰∏™ÂÆáÂÆô„ÄÇÂõΩÁéãËØïÂõæÂëΩ‰ª§‰∏ÄÂàáÔºå‰ΩÜÂÆûÈôÖ‰∏ä‰ªñÊâÄÁªüÊ≤ªÁöÑÂè™ÊòØ‰∏Ä‰∏™Á©∫Ëç°Ëç°ÁöÑÂ∞èÊòüÁêÉ„ÄÇ', 'ÂõΩÁéãÁöÑÂÇ≤ÊÖ¢ËÆ©Â∞èÁéãÂ≠êÊÑüÂà∞Âõ∞ÊÉëÔºåÂõ†‰∏∫‰ªñÂπ∂Ê≤°ÊúâÁúüÊ≠£ÁöÑÊùÉÂäõÔºåÂè™ÊòØÂú®Ëá™ÊàëÂÆâÊÖ∞„ÄÇÊé•‰∏ãÊù•ÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçËôöËç£ÁöÑ‰∫∫Ôºå', 'Ëøô‰∏™‰∫∫Âè™ÂÖ≥ÂøÉÂà´‰∫∫ÁöÑËµûÁæé„ÄÇ‰ªñÂ∏åÊúõÊØè‰∏™‰∫∫ÈÉΩËÉΩËµûÁæé‰ªñÔºåÂπ∂‰∏îÂè™Âú®‰πéËá™Â∑±ÁöÑÂΩ¢Ë±°„ÄÇÂ∞èÁéãÂ≠êÊÑèËØÜÂà∞ÔºåËøôÁßçËôöËç£ÊòØÂ§ö‰πàËÇ§ÊµÖÔºå', 'Êó†Ê≥ïÂ∏¶Êù•ÁúüÊ≠£ÁöÑÂø´‰πê„ÄÇÂú®Á¨¨‰∏â‰∏™ÊòüÁêÉÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰ΩçÊ≤âËø∑‰∫éÈÖíÁ≤æÁöÑÈÖíÈ¨º„ÄÇÈÖíÈ¨ºÂñùÈÖíÊòØ‰∏∫‰∫ÜÂøòËÆ∞Ëá™Â∑±ÁöÑÊÇ≤‰º§Ôºå‰ΩÜËøôÂèàËÆ©‰ªñÈô∑ÂÖ•‰∫ÜÊõ¥Ê∑±ÁöÑÁªùÊúõ„ÄÇ', 'Â∞èÁéãÂ≠êÂØπËøô‰∏™‰∫∫ÊÑüÂà∞ÂêåÊÉÖÔºå‰ΩÜ‰πüÊòéÁôΩËøôÁßçÈÄÉÈÅøÂπ∂‰∏çÊòØËß£ÂÜ≥ÈóÆÈ¢òÁöÑÊñπÊ≥ï„ÄÇÈöèÂêéÔºåÂ∞èÁéãÂ≠êËßÅÂà∞‰∫Ü‰∏Ä‰∏™Âøô‰∫éËÆ°ÁÆóË¥¢ÂØåÁöÑÂïÜ‰∫∫„ÄÇ', '‰ªñÊï¥Â§©Âøô‰∫éËµöÈí±ÔºåÂç¥‰ªéÊú™‰∫´ÂèóËøáÁîüÊ¥ª‰∏≠ÁöÑÁæéÂ•Ω‰∫ãÁâ©„ÄÇÂïÜ‰∫∫ÂëäËØâÂ∞èÁéãÂ≠êÔºå‰ªñÊã•ÊúâËÆ∏Â§öÊòüÊòüÔºåÂõ†‰∏∫‰ªñÊã•ÊúâÂÆÉ‰ª¨ÁöÑÊâÄÊúâÊùÉ„ÄÇ', 'Â∞èÁéãÂ≠êÂØπÊ≠§ÊÑüÂà∞Âõ∞ÊÉëÔºåÂõ†‰∏∫‰ªñËÆ§‰∏∫ÁúüÊ≠£ÈáçË¶ÅÁöÑÊòØ‰∫´ÂèóÁîüÊ¥ªÔºåËÄå‰∏çÊòØÂçïÁ∫ØËøΩÊ±ÇÁâ©Ë¥®Ë¥¢ÂØå„ÄÇÂ∞èÁéãÂ≠êÁöÑ‰∏ã‰∏Ä‰∏™ÈÅáËßÅÊòØ‰∏Ä‰∏™ÁÇπÁÅØ‰∫∫Ôºå', '‰ªñÊØèÂ§©ÈÉΩË¶ÅÁÇπ‰∫ÆÂíåÁÜÑÁÅ≠ÁÅØÂÖâÔºåÂ∞ΩÁÆ°‰ªñÁöÑÂ∑•‰ΩúÊØ´Êó†ÊÑè‰πâ„ÄÇËøô‰ΩçÁÇπÁÅØ‰∫∫ÈùûÂ∏∏Âø†ËØöÔºå‰ΩÜ‰ªñÁöÑÂ∑•‰ΩúÂç¥Ê≤°ÊúâÊó∂Èó¥Âéª‰∫´ÂèóÁîüÊ¥ª„ÄÇ', 'Â∞èÁéãÂ≠ê‰∏∫‰ªñÁöÑÂø†ËØöÊÑüÂà∞Ê¨£ÊÖ∞Ôºå', '‰ΩÜ‰πüÊÑèËØÜÂà∞Ëøô‰ªΩÂø†ËØöÂπ∂‰∏çËÉΩÂ∏¶Êù•Âø´‰πê„ÄÇÊúÄÂêéÔºåÂ∞èÁéãÂ≠êÈÅáÂà∞‰∫Ü‰∏Ä‰∏™Âú∞ÁêÜÂ≠¶ÂÆ∂Ôºå‰ªñÂè™ÂÖ≥Ê≥®‰π¶Êú¨Áü•ËØÜÔºå', 'ËÄå‰∏ç‰∫≤Ë∫´‰ΩìÈ™å‰∏ñÁïå„ÄÇ', '‰ªñÂëäËØâÂ∞èÁéãÂ≠êÔºåËá™Â∑±‰ªéÊú™ËßÅËøáËá™Â∑±ÊâÄËÆ∞ÂΩïÁöÑÂ±±ËÑâ„ÄÅÊ≤≥ÊµÅÊàñÂüéÂ∏Ç„ÄÇËøôËÆ©Â∞èÁéãÂ≠êÊÑèËØÜÂà∞ÔºåÁü•ËØÜ‰∏éÁªèÈ™å‰πãÈó¥ÊúâÁùÄÂ∑®Â§ßÁöÑÂ∑ÆË∑ù„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  5% 1/19 [00:00<00:14,  1.26it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 3568864763\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå\n",
            "Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå\n",
            "‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ\n",
            "Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ\n",
            "Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå\n",
            "ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå', 'Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå', '‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ', 'Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ', 'Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå', 'ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            " 17% 1/6 [00:00<00:04,  1.20it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 2140844079\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå\n",
            "Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå\n",
            "‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ\n",
            "Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ\n",
            "Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå\n",
            "ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå', 'Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå', '‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ', 'Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºå', 'Âè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ', 'Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇ', 'Áé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå', 'ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            " 12% 1/8 [00:00<00:05,  1.22it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 3383740086\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå\n",
            "Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå\n",
            "‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ\n",
            "Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ\n",
            "Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå\n",
            "ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå', 'Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå', '‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ', 'Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ', 'Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå', 'ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            " 17% 1/6 [00:00<00:04,  1.22it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 2686204254\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©ÔºåÂ∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå\n",
            "‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ\n",
            "Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ\n",
            "Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå\n",
            "ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå', 'Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå', '‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ', 'Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ', 'Áé´Áë∞Ëä±ÊòØ‰∏ÄÊúµÁæé‰∏ΩËÄåÂèàÂ®áÂ´©ÁöÑËä±ÊúµÔºåËôΩÁÑ∂Â∞èÁéãÂ≠êÊ∑±Áà±ÁùÄÂ•πÔºå‰ΩÜÂ•πÁöÑËôöËç£ÂíåÁüõÁõæËÆ©‰ªñÊÑüÂà∞Âõ∞ÊÉë„ÄÇÁé´Áë∞Â∏∏Â∏∏Ë¶ÅÊ±ÇÂ∞èÁéãÂ≠êÁÖßÈ°æÂ•πÔºå', 'ÁªôÂ•πÈÅÆÈ£éÊå°Èõ®Ôºå‰ΩÜÂêåÊó∂ÂèàË°®Áé∞ÂæóÂÇ≤ÊÖ¢Êó†Á§º„ÄÇÂ∞èÁéãÂ≠êÂØπËøôÊÆµÂÖ≥Á≥ªÊÑüÂà∞ÁüõÁõæÔºåÂõ†Ê≠§ÂÜ≥ÂÆöÁ¶ªÂºÄËá™Â∑±ÁöÑÊòüÁêÉÔºåÂéªÊé¢Á¥¢Êõ¥ÂπøÈòîÁöÑÂÆáÂÆô„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            " 17% 1/6 [00:01<00:05,  1.01s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 1212756944\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©ÔºåÂ∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå\n",
            "‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ\n",
            "Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©Ôºå', 'Â∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇÂ∞èÁéãÂ≠êÁöÑÂ§ñË°®ÂíåË®ÄË°åÈÉΩÊòæÂæóÂ§©ÁúüÊó†ÈÇ™Ôºå', '‰ªñÁöÑÂà∞Êù•ËÆ©È£ûË°åÂëòÊÑüÂà∞ÊÉäËÆ∂ÂíåÂ•ΩÂ•á„ÄÇ', 'Âú®‰∏éÈ£ûË°åÂëòÁöÑ‰∫§Ë∞à‰∏≠ÔºåÂ∞èÁéãÂ≠êËÆ≤Ëø∞‰∫ÜËá™Â∑±ÁöÑÊòüÁêÉ„ÄÇ‰ªñÁöÑÊòüÁêÉÈùûÂ∏∏Â∞èÔºåÂè™Êúâ‰ªñÂíå‰ªñÁöÑÁé´Áë∞Ëä±„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            " 25% 1/4 [00:00<00:02,  1.23it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 3314173012\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©ÔºåÂ∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©ÔºåÂ∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 1828387257\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©ÔºåÂ∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['ÊïÖ‰∫ãÁöÑÂèôËø∞ËÄÖÊòØ‰∏Ä‰ΩçÈ£ûË°åÂëòÔºå‰ªñÂú®ÊííÂìàÊãâÊ≤ôÊº†‰∏≠Âõ†È£ûÊú∫ÊïÖÈöúËÄåËø´Èôç„ÄÇÂú®Â≠§Áã¨ÂíåÁªùÊúõ‰∏≠Ôºå‰ªñÈÅáÂà∞‰∫Ü‰∏Ä‰∏™ÈáëÂèëÁöÑÂ∞èÁî∑Â≠©ÔºåÂ∞èÁéãÂ≠ê„ÄÇÂ∞èÁéãÂ≠êÊù•Ëá™‰∏Ä‰∏™Âêç‰∏∫B-612ÁöÑÂ∞èÊòüÁêÉÔºåÊòüÁêÉ‰∏äÂè™Êúâ‰∏âÂ∫ßÂ∞èÁÅ´Â±±Âíå‰∏ÄÊúµ‰ªñÊ∑±Áà±ÁöÑÁé´Áë∞Ëä±„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/inference_webui_fast.py \"Auto\"\n",
            "---------------------------------------------TTS Config---------------------------------------------\n",
            "device              : cuda\n",
            "is_half             : True\n",
            "version             : v2\n",
            "t2s_weights_path    : GPT_weights_v2/qisili-e10.ckpt\n",
            "vits_weights_path   : SoVITS_weights_v2/qisili_e20_s480.pth\n",
            "bert_base_path      : GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\n",
            "cnhuhbert_base_path : GPT_SoVITS/pretrained_models/chinese-hubert-base\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Loading Text2Semantic weights from GPT_weights_v2/qisili-e10.ckpt\n",
            "Loading VITS weights from SoVITS_weights_v2/qisili_e20_s480.pth\n",
            "Loading BERT weights from GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\n",
            "Loading CNHuBERT weights from GPT_SoVITS/pretrained_models/chinese-hubert-base\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://cc34f096349bc4ae6c.gradio.live\n",
            "Set seed to 998165638\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "ÂΩìÂâç‰ΩøÁî®g2pwËøõË°åÊãºÈü≥Êé®ÁêÜ\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba_fast:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "DEBUG:jieba_fast:Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 0.971 seconds.\n",
            "DEBUG:jieba_fast:Loading model cost 0.971 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "DEBUG:jieba_fast:Prefix dict has been built succesfully.\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇÂ•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå\n",
            "‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ\n",
            "Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºåÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºå\n",
            "Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "ÁÑ∂ËÄåÔºåÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºåÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:\n",
            "SorryÔºå‰Ω†ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ\n",
            "‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºåÂ•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•ΩÔºå\n",
            "‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå\n",
            "Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ\n",
            "‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇÂ•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå', '‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ', 'Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºå', 'ÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†Ôºå', 'ÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå', '‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ', 'ÁÑ∂ËÄåÔºåÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºå', 'ÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:', 'SorryÔºå‰Ω†ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ', '‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºå', 'Â•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•ΩÔºå', '‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇ', 'Â•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå', 'Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ', '‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            " 61% 11/18 [00:12<00:07,  1.11s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 3370589638\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇÂ•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå\n",
            "‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ\n",
            "Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºåÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºå\n",
            "Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇÂ•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå', '‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ', 'Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºå', 'ÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†Ôºå', 'ÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 8/8 [00:03<00:00,  2.41it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['ÂøÉ‰ºö‰∏çÂÆâ.', '‰ªéÈÇ£‰ª•Âêé,', 'ÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫,', 'Êúâ‰∏ÄÂ§©Â§úÈáå,Áî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠ê,', '‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄô,Â∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°,Â§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè.', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü.Á¨¨‰∫åÂ§©,Áî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫,Â•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†,', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÂáèÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫.Âõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞,Â•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜí,‰∫∫‰æøÊó•Êó•Ê∂àÁò¶.', 'Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫,ÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂Ââç,Ëøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ.Â•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèã,']\n",
            " 21% 308/1500 [00:08<00:21, 55.24it/s]T2S Decoding EOS [139 -> 449]\n",
            " 21% 309/1500 [00:08<00:31, 37.71it/s]\n",
            "0.000\t3.324\t8.254\t0.916\n",
            "Set seed to 962689694\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇÂ•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå\n",
            "‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ\n",
            "Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºåÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºå\n",
            "Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇÂ•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå', '‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ', 'Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºå', 'ÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†Ôºå', 'ÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 8/8 [00:06<00:00,  1.25it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['ÂøÉ‰ºö‰∏çÂÆâ.', '‰ªéÈÇ£‰ª•Âêé,', 'ÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫,', 'Êúâ‰∏ÄÂ§©Â§úÈáå,Áî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠ê,', '‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄô,Â∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°,Â§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè.', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü.Á¨¨‰∫åÂ§©,Áî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫,Â•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†,', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÂáèÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫.Âõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞,Â•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜí,‰∫∫‰æøÊó•Êó•Ê∂àÁò¶.', 'Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫,ÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂Ââç,Ëøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ.Â•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèã,']\n",
            " 23% 345/1500 [00:06<00:19, 59.08it/s]T2S Decoding EOS [139 -> 490]\n",
            " 23% 350/1500 [00:06<00:22, 51.92it/s]\n",
            "0.000\t6.388\t6.747\t0.292\n",
            "Set seed to 2475823956\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇ\n",
            "Â•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ\n",
            "Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºåÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºå\n",
            "Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇ', 'Â•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå', '‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ', 'Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºåÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 7/7 [00:03<00:00,  2.10it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['‰ªéÈÇ£‰ª•Âêé,', 'Â•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèã,', 'Êúâ‰∏ÄÂ§©Â§úÈáå,Áî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠ê,ÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫,', '‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄô,Â∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°,Â§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè.', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü.Á¨¨‰∫åÂ§©,Áî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫,Â•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†,ÂøÉ‰ºö‰∏çÂÆâ.', 'Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫,ÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂Ââç,Ëøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ.', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÂáèÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫.Âõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞,Â•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜí,‰∫∫‰æøÊó•Êó•Ê∂àÁò¶.']\n",
            " 21% 319/1500 [00:07<00:24, 47.52it/s]T2S Decoding EOS [139 -> 459]\n",
            " 21% 319/1500 [00:07<00:28, 41.67it/s]\n",
            "0.000\t3.333\t7.662\t0.584\n",
            "Set seed to 2732494705\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇ\n",
            "Â•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ\n",
            "Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºåÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫ÔºåÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂ÂâçÔºåËøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ„ÄÇ', 'Â•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèãÔºå', '‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄôÔºåÂ∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°ÔºåÂ§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè„ÄÇ', 'Êúâ‰∏ÄÂ§©Â§úÈáåÔºåÁî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠êÔºåÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫Ôºå']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 4/4 [00:01<00:00,  2.07it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['Â•≥Â≠©Êúâ‰∏™ÂæàË¶ÅÂ•ΩÁöÑÁî∑ÊúãÂèã,', 'Êúâ‰∏ÄÂ§©Â§úÈáå,Áî∑Â≠©ÂæàÊÉ≥ÂøµÂ•≥Â≠©Â≠ê,ÁîµËØùÊâìËøáÂéªÂç¥ÈÅ≠ÈÅáÂÖ≥Êú∫,', '‰∏§‰∏™‰∫∫‰∏çËßÅÈù¢ÁöÑÊó∂ÂÄô,Â∞±ÊâìÊâìÁîµËØùÊàñÂèëÂèëÁü≠‰ø°,Â§ßÂÆ∂ÈÉΩÂñúÊ¨¢ËøôÊ†∑ÁöÑËÅîÁªúÊñπÂºè.', 'Â•≥Â≠©ÊØèÂ§©‰∏¥Áù°Ââç‰ºöÂÖàÂÖ≥ÊéâÊâãÊú∫,ÁÑ∂ÂêéÊääÂÆÉÊîæÂú®ÂÜôÂ≠óÂè∞‰∏äËá™Â∑±ÁöÑÁõ∏Êû∂Ââç,Ëøô‰∏™‰π†ÊÉØ‰ªé‰π∞‰∫ÜÊâãÊú∫ÁöÑÊó∂ÂÄôÂ∞±ËøôÊ†∑‰øùÊåÅÁùÄ.']\n",
            " 18% 268/1500 [00:05<00:21, 58.05it/s]T2S Decoding EOS [139 -> 412]\n",
            " 18% 272/1500 [00:05<00:23, 53.02it/s]\n",
            "0.000\t1.935\t5.135\t0.203\n",
            "Set seed to 1904441767\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºå\n",
            "Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  2.38it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['‰ªéÈÇ£‰ª•Âêé,', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü.Á¨¨‰∫åÂ§©,Áî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫,Â•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†,ÂøÉ‰ºö‰∏çÂÆâ.', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÂáèÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫.Âõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞,Â•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜí,‰∫∫‰æøÊó•Êó•Ê∂àÁò¶.']\n",
            " 21% 313/1500 [00:07<00:21, 55.02it/s]T2S Decoding EOS [139 -> 456]\n",
            " 21% 316/1500 [00:07<00:26, 44.49it/s]\n",
            "0.000\t1.260\t7.106\t0.180\n",
            "Set seed to 3296924602\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºåÂ•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  2.34it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['‰ªéÈÇ£‰ª•Âêé,', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü.Á¨¨‰∫åÂ§©,Áî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫,Â•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†,ÂøÉ‰ºö‰∏çÂÆâ.', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÂáèÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫.Âõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞,Â•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜí,‰∫∫‰æøÊó•Êó•Ê∂àÁò¶.']\n",
            " 18% 266/1500 [00:04<00:20, 61.25it/s]T2S Decoding EOS [139 -> 412]\n",
            " 18% 272/1500 [00:04<00:21, 56.63it/s]\n",
            "0.000\t1.284\t4.807\t0.192\n",
            "Set seed to 668976167\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºåÂ•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ-Êï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:02<00:00,  1.14it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['‰ªéÈÇ£‰ª•Âêé,', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü.Á¨¨‰∫åÂ§©,Áî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫,Â•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†,ÂøÉ‰ºö‰∏çÂÆâ.', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÂáèÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫.Âõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞,Â•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜí,‰∫∫‰æøÊó•Êó•Ê∂àÁò¶.']\n",
            " 23% 341/1500 [00:06<00:19, 58.91it/s]T2S Decoding EOS [139 -> 484]\n",
            " 23% 344/1500 [00:06<00:21, 54.04it/s]\n",
            "0.000\t2.642\t6.375\t0.174\n",
            "Set seed to 4228759937\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºåÂ•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÔºå‰∏ÄÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÔºå‰∏ÄÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  2.51it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['‰ªéÈÇ£‰ª•Âêé,', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü.Á¨¨‰∫åÂ§©,Áî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫,Â•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†,ÂøÉ‰ºö‰∏çÂÆâ.', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ,‰∏ÄÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫.Âõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞,Â•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜí,‰∫∫‰æøÊó•Êó•Ê∂àÁò¶.']\n",
            " 22% 330/1500 [00:05<00:19, 61.36it/s]T2S Decoding EOS [139 -> 471]\n",
            " 22% 331/1500 [00:05<00:20, 57.70it/s]\n",
            "0.000\t1.195\t5.740\t0.177\n",
            "Set seed to 1012144860\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºåÂ•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÔºå‰∏ÄÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÔºå‰∏ÄÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:02<00:00,  1.09it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['‰ªéÈÇ£‰ª•Âêé,', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü.Á¨¨‰∫åÂ§©,Áî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫,Â•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†,ÂøÉ‰ºö‰∏çÂÆâ.', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ,‰∏ÄÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫.Âõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞,Â•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜí,‰∫∫‰æøÊó•Êó•Ê∂àÁò¶.']\n",
            " 25% 369/1500 [00:06<00:18, 62.54it/s]T2S Decoding EOS [139 -> 512]\n",
            " 25% 372/1500 [00:06<00:20, 54.54it/s]\n",
            "0.000\t2.758\t6.825\t0.167\n",
            "Set seed to 531055905\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ\n",
            "\n",
            "‰ªéÈÇ£‰ª•ÂêéÔºåÂ•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÔºå‰∏ÄÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü„ÄÇÁ¨¨‰∫åÂ§©ÔºåÁî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥:‚Äú‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫ÔºåÂ•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†ÔºåÂøÉ‰ºö‰∏çÂÆâ„ÄÇ', '‰ªéÈÇ£‰ª•ÂêéÔºå', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØÔºå‰∏ÄÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫„ÄÇÂõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞ÔºåÂ•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜíÔºå‰∫∫‰æøÊó•Êó•Ê∂àÁò¶„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:02<00:00,  1.17it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['‰ªéÈÇ£‰ª•Âêé,', 'Âõ†‰∏∫Â•≥Â≠©Â≠êÂ∑≤ÁªèÁù°‰∏ã‰∫Ü.Á¨¨‰∫åÂ§©,Áî∑Â≠©ÂØπÂ•≥Â≠©ËØ¥‰ª•ÂêéÊôö‰∏äÂà´ÂÖ≥Êú∫,Â•Ω‰πà?ÊàëÊÉ≥‰Ω†ÁöÑÊó∂ÂÄôÊâæ‰∏çÂà∞‰Ω†,ÂøÉ‰ºö‰∏çÂÆâ.', 'Â•≥Â≠©ÂºÄÂßã‰∫ÜÂè¶‰∏ÄÁßç‰π†ÊÉØ,‰∏ÄÊï¥Â§úÈÉΩ‰∏çÂÖ≥Êú∫.Âõ†‰∏∫ÂÆ≥ÊÄï‰ªñÊâìÊù•Ëá™Â∑±‰ºöÂõ†Áù°ÂæóÊ≤âËÄåÂê¨‰∏çÂà∞,Â•≥Â≠©Â§úÂ§úÈÉΩÂæàË≠¶ÈÜí,‰∫∫‰æøÊó•Êó•Ê∂àÁò¶.']\n",
            " 25% 381/1500 [00:06<00:17, 63.67it/s]T2S Decoding EOS [139 -> 523]\n",
            " 26% 383/1500 [00:06<00:19, 56.66it/s]\n",
            "0.000\t2.563\t6.767\t0.174\n",
            "Set seed to 1029944760\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "„ÄÇÁÑ∂ËÄåÔºåÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºåÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:\n",
            "SorryÔºå‰Ω†ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ\n",
            "‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºåÂ•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•ΩÔºå\n",
            "‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºåÂè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['„ÄÇÁÑ∂ËÄåÔºåÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºåÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:', 'SorryÔºå‰Ω†ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ', '‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºåÂ•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•ΩÔºå', '‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºåÂè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            " 25% 1/4 [00:00<00:02,  1.35it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 3041484852\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "„ÄÇÁÑ∂ËÄåÔºåÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºåÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:\n",
            "SorryÔºå‰Ω†ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ\n",
            "‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºåÂ•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•Ω\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['„ÄÇÁÑ∂ËÄåÔºåÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºåÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:', 'SorryÔºå‰Ω†ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ', '‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºåÂ•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•Ω„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            " 33% 1/3 [00:00<00:01,  1.33it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/queueing.py\", line 522, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1689, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 1267, in call_function\n",
            "    prediction = await utils.async_iteration(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 574, in async_iteration\n",
            "    return await iterator.__anext__()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 567, in __anext__\n",
            "    return await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 1005, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 550, in run_sync_iterator_async\n",
            "    return next(iterator)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/utils.py\", line 733, in gen_wrapper\n",
            "    response = next(iterator)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/inference_webui_fast.py\", line 151, in inference\n",
            "    for item in tts_pipeline.run(inputs):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 36, in generator_context\n",
            "    response = gen.send(None)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TTS.py\", line 774, in run\n",
            "    data = self.text_preprocessor.preprocess(text, text_lang, text_split_method, self.configs.version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 65, in preprocess\n",
            "    phones, bert_features, norm_text = self.segment_and_extract_feature_for_text(text, lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 117, in segment_and_extract_feature_for_text\n",
            "    return self.get_phones_and_bert(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 134, in get_phones_and_bert\n",
            "    return self.get_phones_and_bert(formattext,\"zh\",version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 177, in get_phones_and_bert\n",
            "    phones, word2ph, norm_text = self.clean_text_inf(textlist[i], lang, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/TTS_infer_pack/TextPreprocessor.py\", line 208, in clean_text_inf\n",
            "    phones, word2ph, norm_text = clean_text(text, language, version)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/cleaner.py\", line 46, in clean_text\n",
            "    phones = language_module.g2p(norm_text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 365, in g2p\n",
            "    phone_list = _g2p(text)\n",
            "  File \"/content/GPT-SoVITS/GPT_SoVITS/text/english.py\", line 272, in __call__\n",
            "    tokens = pos_tag(words)  # tuples of (word, tag)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 168, in pos_tag\n",
            "    tagger = _get_tagger(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/__init__.py\", line 110, in _get_tagger\n",
            "    tagger = PerceptronTagger()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 183, in __init__\n",
            "    self.load_from_json(lang)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/tag/perceptron.py\", line 273, in load_from_json\n",
            "    loc = find(f\"taggers/averaged_perceptron_tagger_{lang}/\")\n",
            "  File \"/usr/local/lib/python3.9/site-packages/nltk/data.py\", line 579, in find\n",
            "    raise LookupError(resource_not_found)\n",
            "LookupError: \n",
            "**********************************************************************\n",
            "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
            "  Please use the NLTK Downloader to obtain the resource:\n",
            "\n",
            "  \u001b[31m>>> import nltk\n",
            "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
            "  \u001b[0m\n",
            "  For more information see: https://www.nltk.org/data.html\n",
            "\n",
            "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
            "\n",
            "  Searched in:\n",
            "    - '/root/nltk_data'\n",
            "    - '/usr/local/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "    - '/usr/share/nltk_data'\n",
            "    - '/usr/local/share/nltk_data'\n",
            "    - '/usr/lib/nltk_data'\n",
            "    - '/usr/local/lib/nltk_data'\n",
            "**********************************************************************\n",
            "\n",
            "Set seed to 1765986045\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "„ÄÇÁÑ∂ËÄåÔºåÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºåÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:\n",
            "ÂØπ‰∏çËµ∑ÔºåÊÇ®ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ\n",
            "‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºåÂ•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•Ω\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['„ÄÇÁÑ∂ËÄåÔºåÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºåÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:', 'ÂØπ‰∏çËµ∑ÔºåÊÇ®ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ', '‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºåÂ•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•Ω„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  2.04it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['ÂØπ‰∏çËµ∑,ÊÇ®ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫.', '‰∫éÊòØÂ•≥Â≠©Áü•ÈÅì,Â•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫.Âæà‰πÖ‰ª•Âêé,Â•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ.Âç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•Ω.', '.ÁÑ∂ËÄå,ÊÖ¢ÊÖ¢Âú∞,‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï.Â•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢,‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØù,ÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞']\n",
            " 27% 402/1500 [00:06<00:16, 65.83it/s]T2S Decoding EOS [139 -> 543]\n",
            " 27% 403/1500 [00:06<00:19, 57.69it/s]\n",
            "0.000\t1.474\t6.989\t0.187\n",
            "Set seed to 3071035148\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "„ÄÇÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºåÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:\n",
            "ÂØπ‰∏çËµ∑ÔºåÊÇ®ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ\n",
            "‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºåÂ•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•Ω\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['„ÄÇÊÖ¢ÊÖ¢Âú∞Ôºå‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï„ÄÇÂ•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢Ôºå‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØùÔºåÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞:', 'ÂØπ‰∏çËµ∑ÔºåÊÇ®ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫„ÄÇ', '‰∫éÊòØÂ•≥Â≠©Áü•ÈÅìÔºåÂ•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫„ÄÇÂæà‰πÖ‰ª•ÂêéÔºåÂ•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ„ÄÇÂç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•Ω„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  2.08it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['ÂØπ‰∏çËµ∑,ÊÇ®ÊâÄÊã®ÊâìÁöÑÁîµËØùÂ∑≤ÂÖ≥Êú∫.', '‰∫éÊòØÂ•≥Â≠©Áü•ÈÅì,Â•πÁöÑÁà±ÊÉÖÂ∑≤ÁªèÂÖ≥Êú∫.Âæà‰πÖ‰ª•Âêé,Â•≥Â≠©Êúâ‰∫ÜÂè¶‰∏ÄÂú∫Áà±ÊÉÖ.Âç≥‰Ωø‰∏§‰∏™‰∫∫Âú®‰∏ÄËµ∑ÁöÑÊÑüËßâ‰πüÂæàÂ•Ω.', '.ÊÖ¢ÊÖ¢Âú∞,‰∏§‰∏™‰∫∫‰πãÈó¥ËøòÊòØÊúâ‰∫ÜË£ÇÁóï.Â•≥Â≠©ÂæàÊÉ≥ÊåΩÂõûÂç≥Â∞ÜÂàÜÊâãÁöÑÂ±ÄÈù¢,‰æøÂú®‰∏Ä‰∏™Ê∑±Â§úÈáåÁªôÁî∑Â≠©ÊâìÁîµËØù,ÂõûÁ≠îÂ•πÁöÑÊòØÂæàÂ•ΩÂê¨ÁöÑÂ•≥Â£∞']\n",
            " 22% 325/1500 [00:05<00:19, 60.62it/s]T2S Decoding EOS [139 -> 466]\n",
            " 22% 326/1500 [00:05<00:21, 55.86it/s]\n",
            "0.000\t1.442\t5.840\t0.170\n",
            "Set seed to 3490969530\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå\n",
            "Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ\n",
            "‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå', 'Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ', '‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  1.58it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑.', '‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å.Â•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú.Â•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ‰π†ÊÉØ,', '‰∏ÄÂ§©Â§úÈáå,Â•≥Â≠©Ë∫´ÊüìÊÄ•Áóá,ÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØù,Âç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏ä,ËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫.Â•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑.']\n",
            " 22% 333/1500 [00:05<00:18, 63.37it/s]T2S Decoding EOS [139 -> 474]\n",
            " 22% 334/1500 [00:05<00:20, 56.37it/s]\n",
            "0.000\t1.895\t5.929\t0.192\n",
            "Set seed to 1551703371\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå\n",
            "Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ\n",
            "‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå', 'Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ', '‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  1.62it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑.', '‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å.Â•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú.Â•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ‰π†ÊÉØ,', '‰∏ÄÂ§©Â§úÈáå,Â•≥Â≠©Ë∫´ÊüìÊÄ•Áóá,ÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØù,Âç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏ä,ËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫.Â•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑.']\n",
            " 18% 272/1500 [00:05<00:25, 47.54it/s]T2S Decoding EOS [139 -> 412]\n",
            " 18% 272/1500 [00:05<00:24, 50.12it/s]\n",
            "0.000\t1.852\t5.430\t0.248\n",
            "Set seed to 2568231042\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå\n",
            "Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ\n",
            "‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå', 'Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ', '‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  1.56it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑.', '‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å.Â•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú.Â•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ‰π†ÊÉØ,', '‰∏ÄÂ§©Â§úÈáå,Â•≥Â≠©Ë∫´ÊüìÊÄ•Áóá,ÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØù,Âç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏ä,ËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫.Â•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑.']\n",
            " 22% 331/1500 [00:06<00:26, 44.74it/s]T2S Decoding EOS [139 -> 474]\n",
            " 22% 334/1500 [00:06<00:24, 48.24it/s]\n",
            "0.000\t1.922\t6.928\t0.299\n",
            "Set seed to 1537006418\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå\n",
            "Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ\n",
            "‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå', 'Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ', '‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  1.61it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑.', '‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å.Â•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú.Â•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ‰π†ÊÉØ,', '‰∏ÄÂ§©Â§úÈáå,Â•≥Â≠©Ë∫´ÊüìÊÄ•Áóá,ÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØù,Âç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏ä,ËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫.Â•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑.']\n",
            " 22% 323/1500 [00:06<00:28, 41.20it/s]T2S Decoding EOS [139 -> 466]\n",
            " 22% 326/1500 [00:06<00:23, 49.52it/s]\n",
            "0.000\t1.868\t6.586\t0.290\n",
            "Set seed to 4272417679\n",
            "Âπ∂Ë°åÊé®ÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "ÂàÜÊ°∂Â§ÑÁêÜÊ®°ÂºèÂ∑≤ÂºÄÂêØ\n",
            "Actual Input Reference Text: ‰Ω†ÂíåÊïÖ‰∫ãÈáåÁöÑÁéõ‰∏ΩÂÄíÊòØÂæàÂÉèÔºåÂà´ÊÉ≥Â§™Â§ö„ÄÇ\n",
            "############ ÂàáÂàÜÊñáÊú¨ ############\n",
            "Actual Input Target Text:\n",
            "‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÔºåÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå\n",
            "Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ\n",
            "‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å„ÄÇÂ•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØùÔºåÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú„ÄÇÂ•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ\"‰π†ÊÉØÔºå', 'Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑„ÄÇ', '‰∏ÄÂ§©Â§úÈáåÔºåÂ•≥Â≠©Ë∫´ÊüìÊÄ•ÁóáÔºåÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØùÔºåÂç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏äÔºåËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫„ÄÇÂ•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑„ÄÇ']\n",
            "############ ÊèêÂèñÊñáÊú¨BertÁâπÂæÅ ############\n",
            "100% 3/3 [00:01<00:00,  1.85it/s]\n",
            "############ Êé®ÁêÜ ############\n",
            "Processed text from the frontend (per sentence): ['Âè™ÊòØ‰∏çÂÜçÊúüÂæÖÂÆÉ‰ºöÂìçËµ∑.', '‰ΩÜÂ•≥Â≠©ÊÄé‰πà‰πü‰∏çËÇØÂ´Å.Â•≥Â≠©ÁöÑÂøÉÈáåËøòÊòØ‰ºöÊÉ≥Ëµ∑ÈÇ£‰∏™Áî∑Â≠©ÁöÑËØù,ÂíåÈÇ£‰∏™ÂÖ≥Êú∫ÁöÑÂ§ú.Â•≥Â≠©ËøòÊòØ‰øùÊåÅÁùÄÊï¥Â§ú‰∏çÂÖ≥Êú∫ÁöÑ‰π†ÊÉØ,', '‰∏ÄÂ§©Â§úÈáå,Â•≥Â≠©Ë∫´ÊüìÊÄ•Áóá,ÊÖå‰π±‰πã‰∏≠ÊääÊú¨ÊÉ≥Êã®ÁªôÁà∂ÊØçÁöÑÁîµËØù,Âç¥ÊâìÂà∞‰∫ÜËøô‰∏™Áî∑Â≠©ÁöÑÊâãÊú∫‰∏ä,ËøôÊ¨°Áî∑Â≠©Ê≤°ÂÖ≥Êú∫.Â•≥Â≠©Âπ≥ÂÆâÂú∞ÊÅ¢Â§ç‰∫ÜÂÅ•Â∫∑.']\n",
            " 24% 364/1500 [00:07<00:20, 55.90it/s]T2S Decoding EOS [139 -> 504]\n",
            " 24% 364/1500 [00:07<00:23, 47.78it/s]\n",
            "0.000\t1.627\t7.622\t0.168\n"
          ]
        }
      ]
    }
  ]
}