{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xiaoseymy/NLP_ability/blob/master/colab_webui.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "环境配置 environment"
      ],
      "metadata": {
        "id": "_o6a8GS2lWQM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9b7iFV3dm1f",
        "outputId": "17759b87-c325-4a4c-8563-118b8d476d23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q condacolab\n",
        "# Setting up condacolab and installing packages\n",
        "import condacolab\n",
        "condacolab.install_from_url(\"https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh\")\n",
        "%cd -q /content\n",
        "!git clone https://github.com/RVC-Boss/GPT-SoVITS\n",
        "!conda install -y -q -c pytorch -c nvidia cudatoolkit\n",
        "%cd -q /content/GPT-SoVITS\n",
        "!conda install -y -q -c conda-forge gcc gxx ffmpeg cmake -c pytorch -c nvidia\n",
        "!/usr/local/bin/pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://repo.anaconda.com/miniconda/Miniconda3-py39_23.11.0-2-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:21\n",
            "🔁 Restarting kernel...\n",
            "Cloning into 'GPT-SoVITS'...\n",
            "remote: Enumerating objects: 3970, done.\u001b[K\n",
            "remote: Total 3970 (delta 0), reused 0 (delta 0), pack-reused 3970 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3970/3970), 11.26 MiB | 15.93 MiB/s, done.\n",
            "Resolving deltas: 100% (2351/2351), done.\n",
            "Channels:\n",
            " - pytorch\n",
            " - nvidia\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    archspec-0.2.3             |     pyhd3eb1b0_0          47 KB\n",
            "    ca-certificates-2024.11.26 |       h06a4308_0         131 KB\n",
            "    certifi-2024.8.30          |   py39h06a4308_0         162 KB\n",
            "    conda-24.11.1              |   py39h06a4308_0         933 KB\n",
            "    cudatoolkit-11.7.0         |      hd8887f6_10       831.6 MB  nvidia\n",
            "    frozendict-2.4.2           |   py39h5eee18b_0          55 KB\n",
            "    openssl-3.0.15             |       h5eee18b_0         5.2 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       838.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  cudatoolkit        nvidia/linux-64::cudatoolkit-11.7.0-hd8887f6_10 \n",
            "  frozendict         pkgs/main/linux-64::frozendict-2.4.2-py39h5eee18b_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  archspec                               0.2.1-pyhd3eb1b0_0 --> 0.2.3-pyhd3eb1b0_0 \n",
            "  ca-certificates                     2023.12.12-h06a4308_0 --> 2024.11.26-h06a4308_0 \n",
            "  certifi                         2023.11.17-py39h06a4308_0 --> 2024.8.30-py39h06a4308_0 \n",
            "  conda                              23.11.0-py39h06a4308_0 --> 24.11.1-py39h06a4308_0 \n",
            "  openssl                                 3.0.12-h7f8727e_0 --> 3.0.15-h5eee18b_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "done\n",
            "/usr/local/lib/python3.9/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
            "\n",
            "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
            "\n",
            "  conda config --add channels defaults\n",
            "\n",
            "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
            "\n",
            "  deprecated.topic(\n",
            "/usr/local/lib/python3.9/site-packages/conda/base/context.py:201: FutureWarning: Adding 'defaults' to channel list implicitly is deprecated and will be removed in 25.3. \n",
            "\n",
            "To remove this warning, please choose a default channel explicitly with conda's regular configuration system, e.g. by adding 'defaults' to the list of channels:\n",
            "\n",
            "  conda config --add channels defaults\n",
            "\n",
            "For more information see https://docs.conda.io/projects/conda/en/stable/user-guide/configuration/use-condarc.html\n",
            "\n",
            "  deprecated.topic(\n",
            "Channels:\n",
            " - conda-forge\n",
            " - pytorch\n",
            " - nvidia\n",
            " - defaults\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cmake\n",
            "    - ffmpeg\n",
            "    - gcc\n",
            "    - gxx\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _libgcc_mutex-0.1          |      conda_forge           3 KB  conda-forge\n",
            "    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge\n",
            "    aom-3.9.1                  |       hac33072_0         2.6 MB  conda-forge\n",
            "    binutils_impl_linux-64-2.43|       h4bf12b8_2         5.4 MB  conda-forge\n",
            "    ca-certificates-2024.12.14 |       hbcca054_0         153 KB  conda-forge\n",
            "    cairo-1.18.0               |       h3faef2a_0         959 KB  conda-forge\n",
            "    certifi-2024.8.30          |     pyhd8ed1ab_0         160 KB  conda-forge\n",
            "    cmake-3.29.4               |       h91dbaaa_0        18.1 MB  conda-forge\n",
            "    dav1d-1.2.1                |       hd590300_0         742 KB  conda-forge\n",
            "    expat-2.6.4                |       h5888daf_0         135 KB  conda-forge\n",
            "    ffmpeg-7.0.1               | gpl_hb399a10_100         9.6 MB  conda-forge\n",
            "    font-ttf-dejavu-sans-mono-2.37|       hab24e00_0         388 KB  conda-forge\n",
            "    font-ttf-inconsolata-3.000 |       h77eed37_0          94 KB  conda-forge\n",
            "    font-ttf-source-code-pro-2.038|       h77eed37_0         684 KB  conda-forge\n",
            "    font-ttf-ubuntu-0.83       |       h77eed37_3         1.5 MB  conda-forge\n",
            "    fontconfig-2.14.2          |       h14ed4e7_0         266 KB  conda-forge\n",
            "    fonts-conda-ecosystem-1    |                0           4 KB  conda-forge\n",
            "    fonts-conda-forge-1        |                0           4 KB  conda-forge\n",
            "    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge\n",
            "    fribidi-1.0.10             |       h36c2ea0_0         112 KB  conda-forge\n",
            "    gcc-14.2.0                 |       h96c4ede_1          54 KB  conda-forge\n",
            "    gcc_impl_linux-64-14.2.0   |       h6b349bd_1        69.1 MB  conda-forge\n",
            "    gettext-0.22.5             |       he02047a_3         468 KB  conda-forge\n",
            "    gettext-tools-0.22.5       |       he02047a_3         2.6 MB  conda-forge\n",
            "    gmp-6.3.0                  |       hac33072_2         449 KB  conda-forge\n",
            "    gnutls-3.7.9               |       hb077bed_0         1.9 MB  conda-forge\n",
            "    graphite2-1.3.13           |    h59595ed_1003          95 KB  conda-forge\n",
            "    gxx-14.2.0                 |       h96c4ede_1          54 KB  conda-forge\n",
            "    gxx_impl_linux-64-14.2.0   |       h2c03514_1        13.7 MB  conda-forge\n",
            "    harfbuzz-8.5.0             |       hfac3d4d_0         1.5 MB  conda-forge\n",
            "    icu-73.2                   |       h59595ed_0        11.5 MB  conda-forge\n",
            "    kernel-headers_linux-64-3.10.0|      he073ed8_18         921 KB  conda-forge\n",
            "    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n",
            "    ld_impl_linux-64-2.43      |       h712a8e2_2         654 KB  conda-forge\n",
            "    libabseil-20240116.2       | cxx17_he02047a_1         1.2 MB  conda-forge\n",
            "    libarchive-3.7.4           |       hfca40fe_0         851 KB  conda-forge\n",
            "    libasprintf-0.22.5         |       he8f35ee_3          42 KB  conda-forge\n",
            "    libasprintf-devel-0.22.5   |       he8f35ee_3          33 KB  conda-forge\n",
            "    libass-0.17.1              |       h8fe9dca_1         124 KB  conda-forge\n",
            "    libcurl-8.9.1              |       h251f7ec_0         439 KB\n",
            "    libdrm-2.4.124             |       hb9d3cd8_0         237 KB  conda-forge\n",
            "    libexpat-2.6.4             |       h5888daf_0          72 KB  conda-forge\n",
            "    libgcc-14.2.0              |       h77fa898_1         829 KB  conda-forge\n",
            "    libgcc-devel_linux-64-14.2.0|     h41c2201_101         2.6 MB  conda-forge\n",
            "    libgcc-ng-14.2.0           |       h69a702a_1          53 KB  conda-forge\n",
            "    libgettextpo-0.22.5        |       he02047a_3         167 KB  conda-forge\n",
            "    libgettextpo-devel-0.22.5  |       he02047a_3          36 KB  conda-forge\n",
            "    libglib-2.80.2             |       hf974151_0         3.7 MB  conda-forge\n",
            "    libgomp-14.2.0             |       h77fa898_1         450 KB  conda-forge\n",
            "    libhwloc-2.11.2            |default_he43201b_1000         2.3 MB  conda-forge\n",
            "    libiconv-1.17              |       hd590300_2         689 KB  conda-forge\n",
            "    libidn2-2.3.7              |       hd590300_0         124 KB  conda-forge\n",
            "    libmamba-1.5.11            |       hfe524e5_0         1.8 MB\n",
            "    libmambapy-1.5.11          |   py39haf1ee3a_0         341 KB\n",
            "    libopenvino-2024.1.0       |       h2da1b83_7         4.9 MB  conda-forge\n",
            "    libopenvino-auto-batch-plugin-2024.1.0|       hb045406_7         107 KB  conda-forge\n",
            "    libopenvino-auto-plugin-2024.1.0|       hb045406_7         224 KB  conda-forge\n",
            "    libopenvino-hetero-plugin-2024.1.0|       h5c03a75_7         187 KB  conda-forge\n",
            "    libopenvino-intel-cpu-plugin-2024.1.0|       h2da1b83_7        10.4 MB  conda-forge\n",
            "    libopenvino-intel-gpu-plugin-2024.1.0|       h2da1b83_7         8.1 MB  conda-forge\n",
            "    libopenvino-intel-npu-plugin-2024.1.0|       he02047a_7         318 KB  conda-forge\n",
            "    libopenvino-ir-frontend-2024.1.0|       h5c03a75_7         196 KB  conda-forge\n",
            "    libopenvino-onnx-frontend-2024.1.0|       h07e8aee_7         1.5 MB  conda-forge\n",
            "    libopenvino-paddle-frontend-2024.1.0|       h07e8aee_7         683 KB  conda-forge\n",
            "    libopenvino-pytorch-frontend-2024.1.0|       he02047a_7         1.1 MB  conda-forge\n",
            "    libopenvino-tensorflow-frontend-2024.1.0|       h39126c6_7         1.3 MB  conda-forge\n",
            "    libopenvino-tensorflow-lite-frontend-2024.1.0|       he02047a_7         476 KB  conda-forge\n",
            "    libopus-1.3.1              |       h7f98852_1         255 KB  conda-forge\n",
            "    libpciaccess-0.18          |       hd590300_0          28 KB  conda-forge\n",
            "    libpng-1.6.43              |       h2797004_0         281 KB  conda-forge\n",
            "    libprotobuf-4.25.3         |       h08a7969_0         2.7 MB  conda-forge\n",
            "    libsanitizer-14.2.0        |       h2a3dede_1         4.3 MB  conda-forge\n",
            "    libsolv-0.7.29             |       ha6fb4c9_0         460 KB  conda-forge\n",
            "    libstdcxx-14.2.0           |       hc0a3c3a_1         3.7 MB  conda-forge\n",
            "    libstdcxx-devel_linux-64-14.2.0|     h41c2201_101        12.9 MB  conda-forge\n",
            "    libstdcxx-ng-14.2.0        |       h4852527_1          53 KB  conda-forge\n",
            "    libtasn1-4.19.0            |       h166bdaf_0         114 KB  conda-forge\n",
            "    libunistring-0.9.10        |       h7f98852_0         1.4 MB  conda-forge\n",
            "    libuuid-2.38.1             |       h0b41bf4_0          33 KB  conda-forge\n",
            "    libuv-1.49.2               |       hb9d3cd8_0         864 KB  conda-forge\n",
            "    libva-2.21.0               |       h4ab18f5_2         185 KB  conda-forge\n",
            "    libvpx-1.14.1              |       hac33072_0         999 KB  conda-forge\n",
            "    libxcb-1.15                |       h0b41bf4_0         375 KB  conda-forge\n",
            "    libxml2-2.12.7             |       hc051c1a_1         688 KB  conda-forge\n",
            "    libzlib-1.2.13             |       h4ab18f5_6          60 KB  conda-forge\n",
            "    lzo-2.10                   |    hd590300_1001         167 KB  conda-forge\n",
            "    ncurses-6.5                |       he02047a_1         868 KB  conda-forge\n",
            "    nettle-3.9.1               |       h7ab15ed_0         988 KB  conda-forge\n",
            "    ocl-icd-2.3.2              |       hb9d3cd8_2          93 KB  conda-forge\n",
            "    opencl-headers-2024.10.24  |       h5888daf_0          53 KB  conda-forge\n",
            "    openh264-2.4.1             |       h59595ed_0         718 KB  conda-forge\n",
            "    openssl-3.4.0              |       hb9d3cd8_0         2.8 MB  conda-forge\n",
            "    p11-kit-0.24.1             |       hc5aa10d_0         4.5 MB  conda-forge\n",
            "    pcre2-10.43                |       hcad00b1_0         929 KB  conda-forge\n",
            "    pixman-0.44.2              |       h29eaf8c_0         372 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    hb9d3cd8_1002           8 KB  conda-forge\n",
            "    pugixml-1.14               |       h59595ed_0         112 KB  conda-forge\n",
            "    rhash-1.4.5                |       hb9d3cd8_0         183 KB  conda-forge\n",
            "    snappy-1.2.1               |       h8bd8927_1          42 KB  conda-forge\n",
            "    svt-av1-2.1.0              |       hac33072_0         2.5 MB  conda-forge\n",
            "    sysroot_linux-64-2.17      |      h4a8ded7_18        14.8 MB  conda-forge\n",
            "    tbb-2022.0.0               |       hceb3a55_0         174 KB  conda-forge\n",
            "    x264-1!164.3095            |       h166bdaf_2         877 KB  conda-forge\n",
            "    x265-3.5                   |       h924138e_3         3.2 MB  conda-forge\n",
            "    xorg-fixesproto-5.0        |    hb9d3cd8_1003          11 KB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    hb9d3cd8_1003          30 KB  conda-forge\n",
            "    xorg-libice-1.1.2          |       hb9d3cd8_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.5           |       he73a12e_0          27 KB  conda-forge\n",
            "    xorg-libx11-1.8.9          |       h8ee46fc_0         809 KB  conda-forge\n",
            "    xorg-libxau-1.0.12         |       hb9d3cd8_0          14 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.5        |       hb9d3cd8_0          19 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h0b41bf4_2          49 KB  conda-forge\n",
            "    xorg-libxfixes-5.0.3       |    h7f98852_1004          18 KB  conda-forge\n",
            "    xorg-libxrender-0.9.11     |       hd590300_0          37 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    hb9d3cd8_1003          12 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    hb9d3cd8_1004          30 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    hb9d3cd8_1008          72 KB  conda-forge\n",
            "    zlib-1.2.13                |       h4ab18f5_6          91 KB  conda-forge\n",
            "    zstd-1.5.6                 |       ha6fb4c9_0         542 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       254.7 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aom                conda-forge/linux-64::aom-3.9.1-hac33072_0 \n",
            "  binutils_impl_lin~ conda-forge/linux-64::binutils_impl_linux-64-2.43-h4bf12b8_2 \n",
            "  cairo              conda-forge/linux-64::cairo-1.18.0-h3faef2a_0 \n",
            "  cmake              conda-forge/linux-64::cmake-3.29.4-h91dbaaa_0 \n",
            "  dav1d              conda-forge/linux-64::dav1d-1.2.1-hd590300_0 \n",
            "  expat              conda-forge/linux-64::expat-2.6.4-h5888daf_0 \n",
            "  ffmpeg             conda-forge/linux-64::ffmpeg-7.0.1-gpl_hb399a10_100 \n",
            "  font-ttf-dejavu-s~ conda-forge/noarch::font-ttf-dejavu-sans-mono-2.37-hab24e00_0 \n",
            "  font-ttf-inconsol~ conda-forge/noarch::font-ttf-inconsolata-3.000-h77eed37_0 \n",
            "  font-ttf-source-c~ conda-forge/noarch::font-ttf-source-code-pro-2.038-h77eed37_0 \n",
            "  font-ttf-ubuntu    conda-forge/noarch::font-ttf-ubuntu-0.83-h77eed37_3 \n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.14.2-h14ed4e7_0 \n",
            "  fonts-conda-ecosy~ conda-forge/noarch::fonts-conda-ecosystem-1-0 \n",
            "  fonts-conda-forge  conda-forge/noarch::fonts-conda-forge-1-0 \n",
            "  freetype           conda-forge/linux-64::freetype-2.12.1-h267a509_2 \n",
            "  fribidi            conda-forge/linux-64::fribidi-1.0.10-h36c2ea0_0 \n",
            "  gcc                conda-forge/linux-64::gcc-14.2.0-h96c4ede_1 \n",
            "  gcc_impl_linux-64  conda-forge/linux-64::gcc_impl_linux-64-14.2.0-h6b349bd_1 \n",
            "  gettext            conda-forge/linux-64::gettext-0.22.5-he02047a_3 \n",
            "  gettext-tools      conda-forge/linux-64::gettext-tools-0.22.5-he02047a_3 \n",
            "  gmp                conda-forge/linux-64::gmp-6.3.0-hac33072_2 \n",
            "  gnutls             conda-forge/linux-64::gnutls-3.7.9-hb077bed_0 \n",
            "  graphite2          conda-forge/linux-64::graphite2-1.3.13-h59595ed_1003 \n",
            "  gxx                conda-forge/linux-64::gxx-14.2.0-h96c4ede_1 \n",
            "  gxx_impl_linux-64  conda-forge/linux-64::gxx_impl_linux-64-14.2.0-h2c03514_1 \n",
            "  harfbuzz           conda-forge/linux-64::harfbuzz-8.5.0-hfac3d4d_0 \n",
            "  kernel-headers_li~ conda-forge/noarch::kernel-headers_linux-64-3.10.0-he073ed8_18 \n",
            "  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 \n",
            "  libabseil          conda-forge/linux-64::libabseil-20240116.2-cxx17_he02047a_1 \n",
            "  libasprintf        conda-forge/linux-64::libasprintf-0.22.5-he8f35ee_3 \n",
            "  libasprintf-devel  conda-forge/linux-64::libasprintf-devel-0.22.5-he8f35ee_3 \n",
            "  libass             conda-forge/linux-64::libass-0.17.1-h8fe9dca_1 \n",
            "  libdrm             conda-forge/linux-64::libdrm-2.4.124-hb9d3cd8_0 \n",
            "  libexpat           conda-forge/linux-64::libexpat-2.6.4-h5888daf_0 \n",
            "  libgcc             conda-forge/linux-64::libgcc-14.2.0-h77fa898_1 \n",
            "  libgcc-devel_linu~ conda-forge/noarch::libgcc-devel_linux-64-14.2.0-h41c2201_101 \n",
            "  libgettextpo       conda-forge/linux-64::libgettextpo-0.22.5-he02047a_3 \n",
            "  libgettextpo-devel conda-forge/linux-64::libgettextpo-devel-0.22.5-he02047a_3 \n",
            "  libglib            conda-forge/linux-64::libglib-2.80.2-hf974151_0 \n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.11.2-default_he43201b_1000 \n",
            "  libiconv           conda-forge/linux-64::libiconv-1.17-hd590300_2 \n",
            "  libidn2            conda-forge/linux-64::libidn2-2.3.7-hd590300_0 \n",
            "  libopenvino        conda-forge/linux-64::libopenvino-2024.1.0-h2da1b83_7 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-batch-plugin-2024.1.0-hb045406_7 \n",
            "  libopenvino-auto-~ conda-forge/linux-64::libopenvino-auto-plugin-2024.1.0-hb045406_7 \n",
            "  libopenvino-heter~ conda-forge/linux-64::libopenvino-hetero-plugin-2024.1.0-h5c03a75_7 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-cpu-plugin-2024.1.0-h2da1b83_7 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-gpu-plugin-2024.1.0-h2da1b83_7 \n",
            "  libopenvino-intel~ conda-forge/linux-64::libopenvino-intel-npu-plugin-2024.1.0-he02047a_7 \n",
            "  libopenvino-ir-fr~ conda-forge/linux-64::libopenvino-ir-frontend-2024.1.0-h5c03a75_7 \n",
            "  libopenvino-onnx-~ conda-forge/linux-64::libopenvino-onnx-frontend-2024.1.0-h07e8aee_7 \n",
            "  libopenvino-paddl~ conda-forge/linux-64::libopenvino-paddle-frontend-2024.1.0-h07e8aee_7 \n",
            "  libopenvino-pytor~ conda-forge/linux-64::libopenvino-pytorch-frontend-2024.1.0-he02047a_7 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-frontend-2024.1.0-h39126c6_7 \n",
            "  libopenvino-tenso~ conda-forge/linux-64::libopenvino-tensorflow-lite-frontend-2024.1.0-he02047a_7 \n",
            "  libopus            conda-forge/linux-64::libopus-1.3.1-h7f98852_1 \n",
            "  libpciaccess       conda-forge/linux-64::libpciaccess-0.18-hd590300_0 \n",
            "  libpng             conda-forge/linux-64::libpng-1.6.43-h2797004_0 \n",
            "  libprotobuf        conda-forge/linux-64::libprotobuf-4.25.3-h08a7969_0 \n",
            "  libsanitizer       conda-forge/linux-64::libsanitizer-14.2.0-h2a3dede_1 \n",
            "  libstdcxx          conda-forge/linux-64::libstdcxx-14.2.0-hc0a3c3a_1 \n",
            "  libstdcxx-devel_l~ conda-forge/noarch::libstdcxx-devel_linux-64-14.2.0-h41c2201_101 \n",
            "  libtasn1           conda-forge/linux-64::libtasn1-4.19.0-h166bdaf_0 \n",
            "  libunistring       conda-forge/linux-64::libunistring-0.9.10-h7f98852_0 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libuv              conda-forge/linux-64::libuv-1.49.2-hb9d3cd8_0 \n",
            "  libva              conda-forge/linux-64::libva-2.21.0-h4ab18f5_2 \n",
            "  libvpx             conda-forge/linux-64::libvpx-1.14.1-hac33072_0 \n",
            "  libxcb             conda-forge/linux-64::libxcb-1.15-h0b41bf4_0 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.13-h4ab18f5_6 \n",
            "  lzo                conda-forge/linux-64::lzo-2.10-hd590300_1001 \n",
            "  nettle             conda-forge/linux-64::nettle-3.9.1-h7ab15ed_0 \n",
            "  ocl-icd            conda-forge/linux-64::ocl-icd-2.3.2-hb9d3cd8_2 \n",
            "  opencl-headers     conda-forge/linux-64::opencl-headers-2024.10.24-h5888daf_0 \n",
            "  openh264           conda-forge/linux-64::openh264-2.4.1-h59595ed_0 \n",
            "  p11-kit            conda-forge/linux-64::p11-kit-0.24.1-hc5aa10d_0 \n",
            "  pixman             conda-forge/linux-64::pixman-0.44.2-h29eaf8c_0 \n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-hb9d3cd8_1002 \n",
            "  pugixml            conda-forge/linux-64::pugixml-1.14-h59595ed_0 \n",
            "  rhash              conda-forge/linux-64::rhash-1.4.5-hb9d3cd8_0 \n",
            "  snappy             conda-forge/linux-64::snappy-1.2.1-h8bd8927_1 \n",
            "  svt-av1            conda-forge/linux-64::svt-av1-2.1.0-hac33072_0 \n",
            "  sysroot_linux-64   conda-forge/noarch::sysroot_linux-64-2.17-h4a8ded7_18 \n",
            "  tbb                conda-forge/linux-64::tbb-2022.0.0-hceb3a55_0 \n",
            "  x264               conda-forge/linux-64::x264-1!164.3095-h166bdaf_2 \n",
            "  x265               conda-forge/linux-64::x265-3.5-h924138e_3 \n",
            "  xorg-fixesproto    conda-forge/linux-64::xorg-fixesproto-5.0-hb9d3cd8_1003 \n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-hb9d3cd8_1003 \n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.1.2-hb9d3cd8_0 \n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.5-he73a12e_0 \n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.8.9-h8ee46fc_0 \n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.12-hb9d3cd8_0 \n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.5-hb9d3cd8_0 \n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h0b41bf4_2 \n",
            "  xorg-libxfixes     conda-forge/linux-64::xorg-libxfixes-5.0.3-h7f98852_1004 \n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.11-hd590300_0 \n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-hb9d3cd8_1003 \n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-hb9d3cd8_1004 \n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-hb9d3cd8_1008 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2024.11.26~ --> conda-forge::ca-certificates-2024.12.14-hbcca054_0 \n",
            "  icu                        pkgs/main::icu-73.1-h6a678d5_0 --> conda-forge::icu-73.2-h59595ed_0 \n",
            "  ld_impl_linux-64   pkgs/main::ld_impl_linux-64-2.38-h118~ --> conda-forge::ld_impl_linux-64-2.43-h712a8e2_2 \n",
            "  libarchive         pkgs/main::libarchive-3.6.2-h6ac8c49_2 --> conda-forge::libarchive-3.7.4-hfca40fe_0 \n",
            "  libcurl                                  8.4.0-h251f7ec_1 --> 8.9.1-h251f7ec_0 \n",
            "  libgcc-ng          pkgs/main::libgcc-ng-11.2.0-h1234567_1 --> conda-forge::libgcc-ng-14.2.0-h69a702a_1 \n",
            "  libgomp              pkgs/main::libgomp-11.2.0-h1234567_1 --> conda-forge::libgomp-14.2.0-h77fa898_1 \n",
            "  libmamba                                 1.5.3-haf1ee3a_0 --> 1.5.11-hfe524e5_0 \n",
            "  libmambapy                           1.5.3-py39h2dafd23_0 --> 1.5.11-py39haf1ee3a_0 \n",
            "  libsolv              pkgs/main::libsolv-0.7.24-he621ea3_0 --> conda-forge::libsolv-0.7.29-ha6fb4c9_0 \n",
            "  libstdcxx-ng       pkgs/main::libstdcxx-ng-11.2.0-h12345~ --> conda-forge::libstdcxx-ng-14.2.0-h4852527_1 \n",
            "  libxml2              pkgs/main::libxml2-2.10.4-hf1b16e4_1 --> conda-forge::libxml2-2.12.7-hc051c1a_1 \n",
            "  ncurses                 pkgs/main::ncurses-6.4-h6a678d5_0 --> conda-forge::ncurses-6.5-he02047a_1 \n",
            "  openssl              pkgs/main::openssl-3.0.15-h5eee18b_0 --> conda-forge::openssl-3.4.0-hb9d3cd8_0 \n",
            "  pcre2                   pkgs/main::pcre2-10.42-hebb0a14_0 --> conda-forge::pcre2-10.43-hcad00b1_0 \n",
            "  zlib                    pkgs/main::zlib-1.2.13-h5eee18b_0 --> conda-forge::zlib-1.2.13-h4ab18f5_6 \n",
            "  zstd                     pkgs/main::zstd-1.5.5-hc292b87_0 --> conda-forge::zstd-1.5.6-ha6fb4c9_0 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  _libgcc_mutex           pkgs/main::_libgcc_mutex-0.1-main --> conda-forge::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex          pkgs/main::_openmp_mutex-5.1-1_gnu --> conda-forge::_openmp_mutex-4.5-2_gnu \n",
            "  certifi            pkgs/main/linux-64::certifi-2024.8.30~ --> conda-forge/noarch::certifi-2024.8.30-pyhd8ed1ab_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Ignoring onnxruntime: markers 'sys_platform == \"darwin\"' don't match your environment\n",
            "Ignoring opencc: markers 'sys_platform != \"linux\"' don't match your environment\n",
            "Collecting numpy==1.23.4 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting scipy (from -r requirements.txt (line 2))\n",
            "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard (from -r requirements.txt (line 3))\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting librosa==0.9.2 (from -r requirements.txt (line 4))\n",
            "  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting numba==0.56.4 (from -r requirements.txt (line 5))\n",
            "  Downloading numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting pytorch-lightning (from -r requirements.txt (line 6))\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting gradio<=4.24.0,>=4.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-4.24.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting ffmpeg-python (from -r requirements.txt (line 8))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting onnxruntime-gpu (from -r requirements.txt (line 10))\n",
            "  Downloading onnxruntime_gpu-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (4.65.0)\n",
            "Collecting funasr==1.0.27 (from -r requirements.txt (line 12))\n",
            "  Downloading funasr-1.0.27-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting cn2an (from -r requirements.txt (line 13))\n",
            "  Downloading cn2an-0.5.22-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pypinyin (from -r requirements.txt (line 14))\n",
            "  Downloading pypinyin-0.53.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pyopenjtalk>=0.3.4 (from -r requirements.txt (line 15))\n",
            "  Downloading pyopenjtalk-0.3.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting g2p_en (from -r requirements.txt (line 16))\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting torchaudio (from -r requirements.txt (line 17))\n",
            "  Downloading torchaudio-2.5.1-cp39-cp39-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting modelscope==1.10.0 (from -r requirements.txt (line 18))\n",
            "  Downloading modelscope-1.10.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 19))\n",
            "  Downloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting transformers (from -r requirements.txt (line 20))\n",
            "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet (from -r requirements.txt (line 21))\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting PyYAML (from -r requirements.txt (line 22))\n",
            "  Downloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting psutil (from -r requirements.txt (line 23))\n",
            "  Downloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting jieba_fast (from -r requirements.txt (line 24))\n",
            "  Downloading jieba_fast-0.53.tar.gz (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba (from -r requirements.txt (line 25))\n",
            "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting LangSegment>=0.2.0 (from -r requirements.txt (line 26))\n",
            "  Downloading LangSegment-0.3.5-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting Faster_Whisper (from -r requirements.txt (line 27))\n",
            "  Downloading faster_whisper-1.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting wordsegment (from -r requirements.txt (line 28))\n",
            "  Downloading wordsegment-1.3.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting rotary_embedding_torch (from -r requirements.txt (line 29))\n",
            "  Downloading rotary_embedding_torch-0.8.6-py3-none-any.whl.metadata (675 bytes)\n",
            "Collecting pyjyutping (from -r requirements.txt (line 30))\n",
            "  Downloading pyjyutping-1.0.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting g2pk2 (from -r requirements.txt (line 31))\n",
            "  Downloading g2pk2-0.0.3-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting ko_pron (from -r requirements.txt (line 32))\n",
            "  Downloading ko_pron-1.3-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opencc==1.1.1 (from -r requirements.txt (line 34))\n",
            "  Downloading OpenCC-1.1.1-py2.py3-none-manylinux1_x86_64.whl.metadata (10 kB)\n",
            "Collecting python_mecab_ko (from -r requirements.txt (line 35))\n",
            "  Downloading python_mecab_ko-1.3.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting fastapi<0.112.2 (from -r requirements.txt (line 36))\n",
            "  Downloading fastapi-0.112.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting audioread>=2.1.9 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting scikit-learn>=0.19.1 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting joblib>=0.14 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting decorator>=4.0.10 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting soundfile>=0.10.2 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl.metadata (14 kB)\n",
            "Collecting pooch>=1.0 (from librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from librosa==0.9.2->-r requirements.txt (line 4)) (23.1)\n",
            "Collecting llvmlite<0.40,>=0.39.0dev0 (from numba==0.56.4->-r requirements.txt (line 5))\n",
            "  Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from numba==0.56.4->-r requirements.txt (line 5)) (68.2.2)\n",
            "Collecting jamo (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting kaldiio>=2.17.0 (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading kaldiio-2.18.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting torch-complex (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading torch_complex-0.4.4-py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting pytorch-wpe (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading pytorch_wpe-0.0.1-py3-none-any.whl.metadata (242 bytes)\n",
            "Collecting editdistance>=0.5.2 (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading editdistance-0.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting oss2 (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading oss2-2.19.1.tar.gz (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting umap-learn (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting jaconv (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading jaconv-0.4.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core>=1.3.2 (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting tensorboardX (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting openai-whisper (from funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting attrs (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting datasets>=2.14.5 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting einops (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting filelock>=3.3.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gast>=0.2.2 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting pandas (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow>=6.2.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pillow-11.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting pyarrow!=9.0.0,>=6.0.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pyarrow-18.1.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting python-dateutil>=2.1 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (2.31.0)\n",
            "Collecting simplejson>=3.3.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading simplejson-3.19.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting sortedcontainers>=1.5.9 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.9/site-packages (from modelscope==1.10.0->-r requirements.txt (line 18)) (1.26.18)\n",
            "Collecting yapf (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading grpcio-1.68.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting six>1.9 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting torch>=2.1.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting typing-extensions>=4.4.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting altair<6.0,>=4.2.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ffmpy (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==0.14.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.14.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub>=0.19.3 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting importlib-resources<7.0,>=1.3 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting jinja2<4.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting markupsafe~=2.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting matplotlib~=3.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting orjson~=3.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.10.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow>=6.2.0 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pillow-10.4.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting pydantic>=2.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.14.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting future (from ffmpeg-python->-r requirements.txt (line 8))\n",
            "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting coloredlogs (from onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting flatbuffers (from onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Collecting sympy (from onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting proces>=0.1.3 (from cn2an->-r requirements.txt (line 13))\n",
            "  Downloading proces-0.1.7-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting nltk>=3.2.4 (from g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting inflect>=0.3.1 (from g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading inflect-7.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting distance>=0.1.3 (from g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch>=2.1.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Collecting sympy (from onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers->-r requirements.txt (line 20))\n",
            "  Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers->-r requirements.txt (line 20))\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers->-r requirements.txt (line 20))\n",
            "  Downloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting py3langid>=0.2.2 (from LangSegment>=0.2.0->-r requirements.txt (line 26))\n",
            "  Downloading py3langid-0.3.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from Faster_Whisper->-r requirements.txt (line 27))\n",
            "  Downloading ctranslate2-4.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting onnxruntime<2,>=1.14 (from Faster_Whisper->-r requirements.txt (line 27))\n",
            "  Downloading onnxruntime-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting av>=11 (from Faster_Whisper->-r requirements.txt (line 27))\n",
            "  Downloading av-14.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting python-mecab-ko-dic (from python_mecab_ko->-r requirements.txt (line 35))\n",
            "  Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi<0.112.2->-r requirements.txt (line 36))\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting jsonschema>=3.0 (from altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting narwhals>=1.14.2 (from altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading narwhals-1.18.4-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests>=2.25 (from modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tqdm (from -r requirements.txt (line 11))\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2022.5.0 (from fsspec[http]>=2022.5.0->pytorch-lightning->-r requirements.txt (line 6))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading aiohttp-3.11.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting anyio (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.9/site-packages (from httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7)) (3.4)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting omegaconf<2.4,>=2.2 (from hydra-core>=1.3.2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.3.2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zipp>=3.1.0 (from importlib-resources<7.0,>=1.3->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting more-itertools>=8.5.0 (from inflect>=0.3.1->g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting typeguard>=4.0.1 (from inflect>=0.3.1->g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading typeguard-4.4.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 3))\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading fonttools-4.55.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting click (from nltk>=3.2.4->g2p_en->-r requirements.txt (line 16))\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pytz>=2020.1 (from pandas->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.9/site-packages (from pooch>=1.0->librosa==0.9.2->-r requirements.txt (line 4)) (3.10.0)\n",
            "INFO: pip is looking at multiple versions of py3langid to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting py3langid>=0.2.2 (from LangSegment>=0.2.0->-r requirements.txt (line 26))\n",
            "  Downloading py3langid-0.2.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic>=2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.1 (from pydantic>=2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pydantic_core-2.27.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.25->modelscope==1.10.0->-r requirements.txt (line 18)) (2.0.4)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.19.1->librosa==0.9.2->-r requirements.txt (line 4))\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (1.16.0)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "\u001b[33mWARNING: typer 0.15.1 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu->-r requirements.txt (line 10))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting tiktoken (from openai-whisper->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading tiktoken-0.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting crcmod>=1.7 (from oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome>=3.4.7 (from oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pynndescent>=0.5 (from umap-learn->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting tomli>=2.0.1 (from yapf->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading propcache-0.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.14.5->modelscope==1.10.0->-r requirements.txt (line 18))\n",
            "  Downloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.27->-r requirements.txt (line 12))\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.9/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->funasr==1.0.27->-r requirements.txt (line 12)) (41.0.7)\n",
            "Collecting exceptiongroup>=1.0.2 (from anyio->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting sniffio>=1.1 (from anyio->httpx>=0.24.1->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa==0.9.2->-r requirements.txt (line 4)) (2.21)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading rpds_py-0.22.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.9->typer[all]<1.0,>=0.9; sys_platform != \"emscripten\"->gradio<=4.24.0,>=4.0->-r requirements.txt (line 7))\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading numpy-1.23.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numba-0.56.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funasr-1.0.27-py3-none-any.whl (693 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.7/693.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.10.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading OpenCC-1.1.1-py2.py3-none-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.24.0-py3-none-any.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-0.14.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading onnxruntime_gpu-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (226.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.2/226.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cn2an-0.5.22-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypinyin-0.53.0-py2.py3-none-any.whl (834 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m834.7/834.7 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.5.1-cp39-cp39-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.5.1-cp39-cp39-manylinux1_x86_64.whl (906.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m875.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-6.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading LangSegment-0.3.5-py3-none-any.whl (28 kB)\n",
            "Downloading faster_whisper-1.1.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rotary_embedding_torch-0.8.6-py3-none-any.whl (5.6 kB)\n",
            "Downloading pyjyutping-1.0.0-py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.7/143.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading g2pk2-0.0.3-py3-none-any.whl (25 kB)\n",
            "Downloading ko_pron-1.3-py3-none-any.whl (12 kB)\n",
            "Downloading python_mecab_ko-1.3.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.6/578.6 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Downloading av-14.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading editdistance-0.8.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.6/401.6 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading grpcio-1.68.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.8/447.8 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
            "Downloading inflect-7.4.0-py3-none-any.whl (34 kB)\n",
            "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kaldiio-2.18.0-py3-none-any.whl (28 kB)\n",
            "Downloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading llvmlite-0.39.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading matplotlib-3.9.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.19.2-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading proces-0.1.7-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py3langid-0.2.2-py3-none-any.whl (750 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-18.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (40.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.0/457.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
            "Downloading regex-2024.11.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.9/780.9 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.8.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.1/436.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading simplejson-3.19.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading python_mecab_ko_dic-2.1.1.post2-py3-none-any.whl (34.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_wpe-0.0.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_complex-0.4.4-py3-none-any.whl (9.1 kB)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading anyio-4.7.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fonttools-4.55.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.5/88.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.7-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-1.18.4-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.3/251.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading typeguard-4.4.1-py3-none-any.whl (35 kB)\n",
            "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
            "Downloading frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.9/242.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.1/124.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading referencing-0.35.1-py3-none-any.whl (26 kB)\n",
            "Downloading rpds_py-0.22.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (382 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.3/382.3 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Downloading yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.5/321.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: pyopenjtalk, jieba_fast, jieba, distance, antlr4-python3-runtime, jaconv, openai-whisper, oss2, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for pyopenjtalk (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyopenjtalk: filename=pyopenjtalk-0.3.4-cp39-cp39-linux_x86_64.whl size=1209991 sha256=ed73c0104c29b08e07443ccd61f23272c911091e7f9bb0318d94c0e08e131eeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/a4/ce/2fd3035dc55d8dc9f20cffae905f43cc79517aa7560bb856e4\n",
            "  Building wheel for jieba_fast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba_fast: filename=jieba_fast-0.53-cp39-cp39-linux_x86_64.whl size=7628765 sha256=f4343c92c79c99a14ba0e7c1774a786c52c1dcaa000927302bc50c5728e8c6f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/04/c8/5c563e7f58588aadfa5af1353a086ef467eb1dadd2fcfac622\n",
            "  Building wheel for jieba (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=c842510cb29174190413fad929a433743e36d1ae49a8548d225a119e3b9c9a39\n",
            "  Stored in directory: /root/.cache/pip/wheels/7d/74/cf/08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16258 sha256=72163eada1dda281390e54fbca71106a09c05f499e1f06338c32995118b473bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/b3/aa/04241cced6d1722b132273b1d6aafba317887ec004f48b853a\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=a3c825ace777ac100d78c8311974bebf2c2434e1236cf37ff45cda1b180b478b\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/cf/80/f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
            "  Building wheel for jaconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jaconv: filename=jaconv-0.4.0-py3-none-any.whl size=18227 sha256=df4b8149fce1aad402beca752caab7eec85edb39b5947e61d659f20c53cd4de7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/6b/fa/9574efaca6aced07c97ab08d7e40a5cdf8f31ecbe73d55e077\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=5543056472b0034b9b24526013e4124446eee8269ff1f277dc140a754eb25505\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/29/f3/3dd4d7f88df5d701acd3206732dcb6265379c5ece94b472c17\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.19.1-py3-none-any.whl size=123938 sha256=295bd3703be6ace5a430f0aea6354197e4c13d7c752421a2c136f161e3f45ade\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/f9/6e/25b9a00f60e4cc7db56fb53f60546568ac3f697e32e3bff38b\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535317 sha256=64bd8e01a43661213006e7b119940c18b2c8de1b848bddb8a373917872810169\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/85/b9/f7c05b089e7fad969001438f8a0175c7233a8635b49d5af57e\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp39-cp39-linux_x86_64.whl size=23178 sha256=cb0e24867b9c01ab259357375a285428df948828261788c74ecb9e881fc155ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
            "Successfully built pyopenjtalk jieba_fast jieba distance antlr4-python3-runtime jaconv openai-whisper oss2 aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: wordsegment, sortedcontainers, sentencepiece, pytz, python-mecab-ko-dic, pyjyutping, pydub, opencc, mpmath, ko_pron, jieba_fast, jieba, jamo, jaconv, flatbuffers, distance, crcmod, antlr4-python3-runtime, addict, zipp, xxhash, websockets, tzdata, typing-extensions, tqdm, tomlkit, tomli, threadpoolctl, tensorboard-data-server, sympy, sniffio, six, simplejson, shellingham, semantic-version, safetensors, ruff, rpds-py, requests, regex, PyYAML, python-multipart, python_mecab_ko, pypinyin, pyparsing, pygments, pycryptodome, pyarrow, psutil, protobuf, propcache, proces, Pillow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, narwhals, more-itertools, mdurl, markupsafe, llvmlite, kiwisolver, joblib, jmespath, humanfriendly, h11, grpcio, gast, future, fsspec, frozenlist, fonttools, filelock, ffmpy, exceptiongroup, einops, editdistance, dill, decorator, cycler, click, chardet, av, audioread, attrs, async-timeout, annotated-types, aiohappyeyeballs, aiofiles, absl-py, yapf, werkzeug, uvicorn, triton, torch-complex, tiktoken, tensorboardX, soundfile, scipy, referencing, pytorch-wpe, python-dateutil, pyopenjtalk, pydantic-core, py3langid, pooch, omegaconf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, nltk, multiprocess, multidict, markdown-it-py, lightning-utilities, kaldiio, jinja2, importlib-resources, importlib-metadata, huggingface-hub, httpcore, ffmpeg-python, ctranslate2, contourpy, coloredlogs, cn2an, anyio, aiosignal, yarl, typeguard, tokenizers, starlette, scikit-learn, rich, resampy, pydantic, pandas, onnxruntime-gpu, onnxruntime, nvidia-cusolver-cu12, matplotlib, markdown, LangSegment, jsonschema-specifications, hydra-core, httpx, g2pk2, aliyun-python-sdk-core, typer, transformers, torch, tensorboard, pynndescent, librosa, jsonschema, inflect, gradio-client, Faster_Whisper, fastapi, aliyun-python-sdk-kms, aiohttp, umap-learn, torchmetrics, torchaudio, rotary_embedding_torch, oss2, openai-whisper, g2p_en, altair, pytorch-lightning, gradio, funasr, datasets, modelscope\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "Successfully installed Faster_Whisper-1.1.0 LangSegment-0.3.5 Pillow-10.4.0 PyYAML-6.0.2 absl-py-2.1.0 addict-2.4.0 aiofiles-23.2.1 aiohappyeyeballs-2.4.4 aiohttp-3.11.10 aiosignal-1.3.2 aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 altair-5.5.0 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 anyio-4.7.0 async-timeout-5.0.1 attrs-24.2.0 audioread-3.0.1 av-14.0.1 chardet-5.2.0 click-8.1.7 cn2an-0.5.22 coloredlogs-15.0.1 contourpy-1.3.0 crcmod-1.7 ctranslate2-4.5.0 cycler-0.12.1 datasets-3.2.0 decorator-5.1.1 dill-0.3.8 distance-0.1.3 editdistance-0.8.1 einops-0.8.0 exceptiongroup-1.2.2 fastapi-0.112.1 ffmpeg-python-0.2.0 ffmpy-0.4.0 filelock-3.16.1 flatbuffers-24.3.25 fonttools-4.55.3 frozenlist-1.5.0 fsspec-2024.9.0 funasr-1.0.27 future-1.0.0 g2p_en-2.1.0 g2pk2-0.0.3 gast-0.6.0 gradio-4.24.0 gradio-client-0.14.0 grpcio-1.68.1 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.26.5 humanfriendly-10.0 hydra-core-1.3.2 importlib-metadata-8.5.0 importlib-resources-6.4.5 inflect-7.4.0 jaconv-0.4.0 jamo-0.4.1 jieba-0.42.1 jieba_fast-0.53 jinja2-3.1.4 jmespath-0.10.0 joblib-1.4.2 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 kaldiio-2.18.0 kiwisolver-1.4.7 ko_pron-1.3 librosa-0.9.2 lightning-utilities-0.11.9 llvmlite-0.39.1 markdown-3.7 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.9.4 mdurl-0.1.2 modelscope-1.10.0 more-itertools-10.5.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 narwhals-1.18.4 networkx-3.2.1 nltk-3.9.1 numba-0.56.4 numpy-1.23.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 omegaconf-2.3.0 onnxruntime-1.19.2 onnxruntime-gpu-1.19.2 openai-whisper-20240930 opencc-1.1.1 orjson-3.10.12 oss2-2.19.1 pandas-2.2.3 pooch-1.8.2 proces-0.1.7 propcache-0.2.1 protobuf-5.29.1 psutil-6.1.0 py3langid-0.2.2 pyarrow-18.1.0 pycryptodome-3.21.0 pydantic-2.10.3 pydantic-core-2.27.1 pydub-0.25.1 pygments-2.18.0 pyjyutping-1.0.0 pynndescent-0.5.13 pyopenjtalk-0.3.4 pyparsing-3.2.0 pypinyin-0.53.0 python-dateutil-2.9.0.post0 python-mecab-ko-dic-2.1.1.post2 python-multipart-0.0.19 python_mecab_ko-1.3.7 pytorch-lightning-2.4.0 pytorch-wpe-0.0.1 pytz-2024.2 referencing-0.35.1 regex-2024.11.6 requests-2.32.3 resampy-0.4.3 rich-13.9.4 rotary_embedding_torch-0.8.6 rpds-py-0.22.3 ruff-0.8.3 safetensors-0.4.5 scikit-learn-1.6.0 scipy-1.13.1 semantic-version-2.10.0 sentencepiece-0.2.0 shellingham-1.5.4 simplejson-3.19.3 six-1.17.0 sniffio-1.3.1 sortedcontainers-2.4.0 soundfile-0.12.1 starlette-0.38.6 sympy-1.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorboardX-2.6.2.2 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.21.0 tomli-2.2.1 tomlkit-0.12.0 torch-2.5.1 torch-complex-0.4.4 torchaudio-2.5.1 torchmetrics-1.6.0 tqdm-4.67.1 transformers-4.47.0 triton-3.1.0 typeguard-4.4.1 typer-0.15.1 typing-extensions-4.12.2 tzdata-2024.2 umap-learn-0.5.7 uvicorn-0.34.0 websockets-11.0.3 werkzeug-3.1.3 wordsegment-1.3.1 xxhash-3.5.0 yapf-0.43.0 yarl-1.18.3 zipp-3.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download pretrained models 下载预训练模型\n",
        "!mkdir -p /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!mkdir -p /content/GPT-SoVITS/tools/uvr5\n",
        "%cd /content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
        "!git clone https://huggingface.co/lj1995/GPT-SoVITS\n",
        "%cd /content/GPT-SoVITS/tools/damo_asr/models\n",
        "!git clone https://www.modelscope.cn/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/speech_fsmn_vad_zh-cn-16k-common-pytorch.git\n",
        "!git clone https://www.modelscope.cn/damo/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git\n",
        "# @title UVR5 pretrains 安装uvr5模型\n",
        "%cd /content/GPT-SoVITS/tools/uvr5\n",
        "%rm -r uvr5_weights\n",
        "!git clone https://huggingface.co/Delik/uvr5_weights\n",
        "!git config core.sparseCheckout true\n",
        "!mv /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/GPT-SoVITS/* /content/GPT-SoVITS/GPT_SoVITS/pretrained_models/"
      ],
      "metadata": {
        "id": "0NgxXg5sjv7z",
        "outputId": "2260dd55-0560-46ef-d942-7111c5bcabbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GPT-SoVITS/GPT_SoVITS/pretrained_models\n",
            "Cloning into 'GPT-SoVITS'...\n",
            "remote: Enumerating objects: 29, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 29 (delta 3), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (29/29), 104.78 KiB | 6.16 MiB/s, done.\n",
            "Filtering content: 100% (8/8), 1.44 GiB | 51.34 MiB/s, done.\n",
            "/content/GPT-SoVITS/tools/damo_asr/models\n",
            "Cloning into 'speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch'...\n",
            "remote: Enumerating objects: 466, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 466 (delta 40), reused 60 (delta 32), pack-reused 395\u001b[K\n",
            "Receiving objects: 100% (466/466), 1.12 GiB | 1.50 MiB/s, done.\n",
            "Resolving deltas: 100% (268/268), done.\n",
            "Cloning into 'speech_fsmn_vad_zh-cn-16k-common-pytorch'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 184 (delta 18), reused 23 (delta 10), pack-reused 145\u001b[K\n",
            "Receiving objects: 100% (184/184), 4.66 MiB | 4.32 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "Cloning into 'punc_ct-transformer_zh-cn-common-vocab272727-pytorch'...\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 170 (delta 23), reused 27 (delta 12), pack-reused 123\u001b[K\n",
            "Receiving objects: 100% (170/170), 257.56 MiB | 1.33 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n",
            "/content/GPT-SoVITS/tools/uvr5\n",
            "Cloning into 'uvr5_weights'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 0 (delta 0), pack-reused 15 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (15/15), 3.25 KiB | 1.08 MiB/s, done.\n",
            "Filtering content: 100% (9/9), 594.44 MiB | 85.83 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title launch WebUI 启动WebUI\n",
        "!/usr/local/bin/pip install ipykernel\n",
        "!sed -i '10s/False/True/' /content/GPT-SoVITS/config.py\n",
        "%cd /content/GPT-SoVITS/\n",
        "!/usr/local/bin/python  webui.py"
      ],
      "metadata": {
        "id": "4oRGUzkrk8C7",
        "outputId": "bb9edfb9-ea88-476e-d547-0b8199b7abd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipykernel\n",
            "  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting comm>=0.1.1 (from ipykernel)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting debugpy>=1.6.5 (from ipykernel)\n",
            "  Downloading debugpy-1.8.11-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting ipython>=7.23.1 (from ipykernel)\n",
            "  Downloading ipython-8.18.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel)\n",
            "  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel)\n",
            "  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting matplotlib-inline>=0.1 (from ipykernel)\n",
            "  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting nest-asyncio (from ipykernel)\n",
            "  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from ipykernel) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from ipykernel) (6.1.0)\n",
            "Collecting pyzmq>=24 (from ipykernel)\n",
            "  Downloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting tornado>=6.1 (from ipykernel)\n",
            "  Downloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting traitlets>=5.4.0 (from ipykernel)\n",
            "  Downloading traitlets-5.14.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting prompt-toolkit<3.1.0,>=3.0.41 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading prompt_toolkit-3.0.48-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
            "Collecting stack-data (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading stack_data-0.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
            "Collecting pexpect>4.3 (from ipython>=7.23.1->ipykernel)\n",
            "  Downloading pexpect-4.9.0-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.21.0)\n",
            "Collecting parso<0.9.0,>=0.8.4 (from jedi>=0.16->ipython>=7.23.1->ipykernel)\n",
            "  Downloading parso-0.8.4-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting ptyprocess>=0.5 (from pexpect>4.3->ipython>=7.23.1->ipykernel)\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting wcwidth (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel)\n",
            "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
            "Collecting executing>=1.2.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.1.0 (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading asttokens-3.0.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting pure-eval (from stack-data->ipython>=7.23.1->ipykernel)\n",
            "  Downloading pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading debugpy-1.8.11-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.2/808.2 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)\n",
            "Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)\n",
            "Downloading pyzmq-26.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (862 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.1/862.1 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tornado-6.4.2-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.2/437.2 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traitlets-5.14.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
            "Downloading asttokens-3.0.0-py3-none-any.whl (26 kB)\n",
            "Downloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
            "Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)\n",
            "Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Installing collected packages: wcwidth, pure-eval, ptyprocess, traitlets, tornado, pyzmq, prompt-toolkit, pexpect, parso, nest-asyncio, executing, debugpy, asttokens, stack-data, matplotlib-inline, jupyter-core, jedi, comm, jupyter-client, ipython, ipykernel\n",
            "Successfully installed asttokens-3.0.0 comm-0.2.2 debugpy-1.8.11 executing-2.1.0 ipykernel-6.29.5 ipython-8.18.1 jedi-0.19.2 jupyter-client-8.6.3 jupyter-core-5.7.2 matplotlib-inline-0.1.7 nest-asyncio-1.6.0 parso-0.8.4 pexpect-4.9.0 prompt-toolkit-3.0.48 ptyprocess-0.7.0 pure-eval-0.2.3 pyzmq-26.2.0 stack-data-0.6.3 tornado-6.4.2 traitlets-5.14.3 wcwidth-0.2.13\n",
            "/content/GPT-SoVITS\n",
            "Downloading g2pw model...\n",
            "Extracting g2pw model...\n",
            "Running on local URL:  http://0.0.0.0:9874\n",
            "Running on public URL: https://62c218b5b9d01a8b51.gradio.live\n",
            "\"/usr/local/bin/python\" tools/cmd-denoise.py -i \"/content/output\" -o \"/content/output/denoise_opt\" -p float16\n",
            "2024-12-16 05:02:40,902 - modelscope - INFO - PyTorch version 2.5.1 Found.\n",
            "2024-12-16 05:02:40,903 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2024-12-16 05:02:40,903 - modelscope - INFO - No valid ast index found from /root/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
            "2024-12-16 05:02:40,963 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 992b9f659196eb26fd5148bdc0a014ae and a total number of 946 components indexed\n",
            "2024-12-16 05:02:46,211 - modelscope - WARNING - Model revision not specified, use revision: v1.0.2\n",
            "Downloading: 100% 1.45k/1.45k [00:00<00:00, 138kB/s]\n",
            "Downloading: 100% 903/903 [00:00<00:00, 83.0kB/s]\n",
            "Downloading: 100% 177k/177k [00:00<00:00, 490kB/s]\n",
            "Downloading: 100% 88.2k/88.2k [00:00<00:00, 501kB/s]\n",
            "Downloading: 100% 55.3M/55.3M [00:05<00:00, 9.97MB/s]\n",
            "Downloading: 100% 12.8k/12.8k [00:00<00:00, 1.40MB/s]\n",
            "Downloading: 100% 75.0k/75.0k [00:00<00:00, 436kB/s]\n",
            "Downloading: 100% 152k/152k [00:00<00:00, 401kB/s]\n",
            "2024-12-16 05:03:00,368 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k\n",
            "2024-12-16 05:03:00,368 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k.\n",
            "2024-12-16 05:03:00,369 - modelscope - INFO - initialize model from /root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k\n",
            "/usr/local/lib/python3.9/site-packages/modelscope/models/audio/ans/frcrn.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\n",
            "2024-12-16 05:03:01,021 - modelscope - WARNING - No preprocessor field found in cfg.\n",
            "2024-12-16 05:03:01,022 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
            "2024-12-16 05:03:01,022 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/root/.cache/modelscope/hub/damo/speech_frcrn_ans_cirm_16k'}. trying to build by task and model information.\n",
            "2024-12-16 05:03:01,022 - modelscope - WARNING - No preprocessor key ('speech_frcrn_ans_cirm_16k', 'acoustic-noise-suppression') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
            "  0% 0/11 [00:00<?, ?it/s]inputs:(1, 288000)\n",
            "padding: 24000\n",
            "inputs after padding:(1, 312000)\n",
            "  9% 1/11 [00:04<00:41,  4.14s/it]inputs:(1, 272000)\n",
            "padding: 20000\n",
            "inputs after padding:(1, 292000)\n",
            " 18% 2/11 [00:06<00:25,  2.88s/it]inputs:(1, 272000)\n",
            "padding: 20000\n",
            "inputs after padding:(1, 292000)\n",
            " 27% 3/11 [00:08<00:21,  2.72s/it]inputs:(1, 272000)\n",
            "padding: 20000\n",
            "inputs after padding:(1, 292000)\n",
            " 36% 4/11 [00:10<00:17,  2.51s/it]inputs:(1, 208000)\n",
            "inputs after padding:(1, 208000)\n",
            " 45% 5/11 [00:12<00:12,  2.11s/it]inputs:(1, 272000)\n",
            "padding: 20000\n",
            "inputs after padding:(1, 292000)\n",
            " 55% 6/11 [00:14<00:10,  2.08s/it]inputs:(1, 176000)\n",
            "padding: 20000\n",
            "inputs after padding:(1, 196000)\n",
            " 64% 7/11 [00:15<00:07,  1.80s/it]inputs:(1, 272000)\n",
            "padding: 20000\n",
            "inputs after padding:(1, 292000)\n",
            " 73% 8/11 [00:17<00:05,  1.87s/it]inputs:(1, 288000)\n",
            "padding: 24000\n",
            "inputs after padding:(1, 312000)\n",
            " 82% 9/11 [00:19<00:03,  1.94s/it]inputs:(1, 288000)\n",
            "padding: 24000\n",
            "inputs after padding:(1, 312000)\n",
            " 91% 10/11 [00:22<00:02,  2.17s/it]Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/tools/cmd-denoise.py\", line 17, in execute_denoise\n",
            "    ans(\"%s/%s\"%(input_folder,name),output_path='%s/%s'%(output_folder,name))\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/pipelines/base.py\", line 219, in __call__\n",
            "    output = self._process_single(input, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/pipelines/base.py\", line 247, in _process_single\n",
            "    out = self.preprocess(input, **preprocess_params)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/pipelines/audio/ans_pipeline.py\", line 47, in preprocess\n",
            "    file_bytes = File.read(inputs)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/fileio/file.py\", line 274, in read\n",
            "    return storage.read(uri)\n",
            "  File \"/usr/local/lib/python3.9/site-packages/modelscope/fileio/file.py\", line 53, in read\n",
            "    with open(filepath, 'rb') as f:\n",
            "IsADirectoryError: [Errno 21] Is a directory: '/content/output/denoise_opt'\n",
            "100% 11/11 [00:22<00:00,  2.03s/it]\n",
            "\"/usr/local/bin/python\" tools/asr/funasr_asr.py -i \"/content/output/denoise_opt\" -o \"/content/output/asr_opt\" -s large -l zh -p float32\n",
            "2024-12-16 05:04:05,032 - modelscope - INFO - PyTorch version 2.5.1 Found.\n",
            "2024-12-16 05:04:05,033 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer\n",
            "2024-12-16 05:04:05,069 - modelscope - INFO - Loading done! Current index file version is 1.10.0, with md5 992b9f659196eb26fd5148bdc0a014ae and a total number of 946 components indexed\n",
            "2024-12-16 05:04:06,005 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 10.9k/10.9k [00:00<00:00, 1.03MB/s]\n",
            "Downloading: 100% 173k/173k [00:00<00:00, 504kB/s]\n",
            "Downloading: 100% 2.45k/2.45k [00:00<00:00, 935kB/s]\n",
            "Downloading: 100% 472/472 [00:00<00:00, 175kB/s]\n",
            "Downloading: 100% 840M/840M [01:46<00:00, 8.25MB/s]\n",
            "Downloading: 100% 19.1k/19.1k [00:00<00:00, 225kB/s]\n",
            "Downloading: 100% 7.90M/7.90M [00:00<00:00, 9.68MB/s]\n",
            "Downloading: 100% 48.7k/48.7k [00:00<00:00, 293kB/s]\n",
            "Downloading: 100% 91.5k/91.5k [00:00<00:00, 355kB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/model.pt\n",
            "/usr/local/lib/python3.9/site-packages/funasr/train_utils/load_pretrained_model.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  src_state = torch.load(path, map_location=map_location)\n",
            "2024-12-16 05:06:08,501 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 7.85k/7.85k [00:00<00:00, 847kB/s]\n",
            "Downloading: 100% 1.19k/1.19k [00:00<00:00, 421kB/s]\n",
            "Downloading: 100% 365/365 [00:00<00:00, 137kB/s]\n",
            "Downloading: 100% 1.64M/1.64M [00:00<00:00, 2.07MB/s]\n",
            "Downloading: 100% 8.45k/8.45k [00:00<00:00, 3.08MB/s]\n",
            "Downloading: 100% 27.3k/27.3k [00:00<00:00, 316kB/s]\n",
            "Downloading: 100% 2.16M/2.16M [00:00<00:00, 2.60MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/model.pt\n",
            "2024-12-16 05:06:15,388 - modelscope - INFO - Use user-specified model revision: v2.0.4\n",
            "Downloading: 100% 6.00k/6.00k [00:00<00:00, 1.88MB/s]\n",
            "Downloading: 100% 810/810 [00:00<00:00, 293kB/s]\n",
            "Downloading: 100% 373/373 [00:00<00:00, 121kB/s]\n",
            "Downloading: 100% 278M/278M [03:11<00:00, 1.52MB/s]\n",
            "Downloading: 100% 863/863 [00:00<00:00, 309kB/s]\n",
            "Downloading: 100% 11.2k/11.2k [00:00<00:00, 3.65MB/s]\n",
            "Downloading: 100% 151k/151k [00:00<00:00, 323kB/s]\n",
            "Downloading: 100% 4.01M/4.01M [00:00<00:00, 4.79MB/s]\n",
            "ckpt: /root/.cache/modelscope/hub/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/model.pt\n",
            "FunASR 模型加载完成: ZH\n",
            "  0% 0/10 [00:00<?, ?it/s]\n",
            "split_1_1.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  1.49it/s]\u001b[A\n",
            "{'load_data': '0.126', 'extract_feat': '0.153', 'forward': '0.673', 'batch_size': '1', 'rtf': '0.037'}, : 100% 1/1 [00:00<00:00,  1.49it/s]\u001b[A\n",
            "rtf_avg: 0.037: 100% 1/1 [00:00<00:00,  1.48it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A/usr/local/lib/python3.9/site-packages/funasr/models/paraformer/model.py:249: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(False):\n",
            "/usr/local/lib/python3.9/site-packages/funasr/models/paraformer/cif_predictor.py:212: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(False):\n",
            "\n",
            "\n",
            "100% 1/1 [00:01<00:00,  1.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.033', 'forward': '1.710', 'batch_size': '1', 'rtf': '0.101'}, : 100% 1/1 [00:01<00:00,  1.71s/it]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.101: 100% 1/1 [00:01<00:00,  1.71s/it]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.066', 'batch_size': '1', 'rtf': '-0.066'}, : 100% 1/1 [00:00<00:00, 15.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.066: 100% 1/1 [00:00<00:00, 15.14it/s]\n",
            "\n",
            "100% 1/1 [00:01<00:00,  1.81s/it]\u001b[A\n",
            "rtf_avg: 0.099, time_speech:  18.000, time_escape: 1.785: 100% 1/1 [00:01<00:00,  1.81s/it]\n",
            " 10% 1/10 [00:02<00:22,  2.48s/it]\n",
            "split_1_2.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  5.64it/s]\u001b[A\n",
            "{'load_data': '0.012', 'extract_feat': '0.024', 'forward': '0.177', 'batch_size': '1', 'rtf': '0.010'}, : 100% 1/1 [00:00<00:00,  5.64it/s]\u001b[A\n",
            "rtf_avg: 0.010: 100% 1/1 [00:00<00:00,  5.61it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  6.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.023', 'forward': '0.147', 'batch_size': '2', 'rtf': '0.009'}, :  50% 1/2 [00:00<00:00,  6.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.009:  50% 1/2 [00:00<00:00,  6.62it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.021', 'batch_size': '1', 'rtf': '-0.021'}, : 100% 1/1 [00:00<00:00, 46.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.021: 100% 1/1 [00:00<00:00, 46.10it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  5.23it/s]\u001b[A\n",
            "rtf_avg: 0.011, time_speech:  17.000, time_escape: 0.179: 100% 1/1 [00:00<00:00,  5.23it/s]\n",
            " 20% 2/10 [00:02<00:09,  1.24s/it]\n",
            "split_1_3.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  7.15it/s]\u001b[A\n",
            "{'load_data': '0.011', 'extract_feat': '0.019', 'forward': '0.140', 'batch_size': '1', 'rtf': '0.011'}, : 100% 1/1 [00:00<00:00,  7.15it/s]\u001b[A\n",
            "rtf_avg: 0.011: 100% 1/1 [00:00<00:00,  7.12it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  6.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.015', 'forward': '0.146', 'batch_size': '1', 'rtf': '0.011'}, : 100% 1/1 [00:00<00:00,  6.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.011: 100% 1/1 [00:00<00:00,  6.83it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.015', 'batch_size': '1', 'rtf': '-0.015'}, : 100% 1/1 [00:00<00:00, 66.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.015: 100% 1/1 [00:00<00:00, 65.69it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  5.62it/s]\u001b[A\n",
            "rtf_avg: 0.013, time_speech:  13.000, time_escape: 0.166: 100% 1/1 [00:00<00:00,  5.62it/s]\n",
            " 30% 3/10 [00:03<00:05,  1.22it/s]\n",
            "split_2_1.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  4.80it/s]\u001b[A\n",
            "{'load_data': '0.012', 'extract_feat': '0.025', 'forward': '0.208', 'batch_size': '1', 'rtf': '0.012'}, : 100% 1/1 [00:00<00:00,  4.80it/s]\u001b[A\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00,  4.78it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "100% 1/1 [00:00<00:00,  5.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.018', 'forward': '0.169', 'batch_size': '1', 'rtf': '0.010'}, : 100% 1/1 [00:00<00:00,  5.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.010: 100% 1/1 [00:00<00:00,  5.78it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.025', 'batch_size': '1', 'rtf': '-0.025'}, : 100% 1/1 [00:00<00:00, 39.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.025: 100% 1/1 [00:00<00:00, 39.07it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  4.65it/s]\u001b[A\n",
            "rtf_avg: 0.011, time_speech:  18.000, time_escape: 0.203: 100% 1/1 [00:00<00:00,  4.64it/s]\n",
            " 40% 4/10 [00:03<00:03,  1.51it/s]\n",
            "split_2_2.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  5.49it/s]\u001b[A\n",
            "{'load_data': '0.012', 'extract_feat': '0.029', 'forward': '0.182', 'batch_size': '1', 'rtf': '0.011'}, : 100% 1/1 [00:00<00:00,  5.49it/s]\u001b[A\n",
            "rtf_avg: 0.011: 100% 1/1 [00:00<00:00,  5.46it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  6.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.022', 'forward': '0.143', 'batch_size': '2', 'rtf': '0.009'}, :  50% 1/2 [00:00<00:00,  6.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.009:  50% 1/2 [00:00<00:00,  6.82it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.024', 'batch_size': '1', 'rtf': '-0.024'}, : 100% 1/1 [00:00<00:00, 41.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.024: 100% 1/1 [00:00<00:00, 41.00it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  5.33it/s]\u001b[A\n",
            "rtf_avg: 0.010, time_speech:  17.000, time_escape: 0.176: 100% 1/1 [00:00<00:00,  5.32it/s]\n",
            " 50% 5/10 [00:03<00:02,  1.79it/s]\n",
            "split_2_3.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  5.62it/s]\u001b[A\n",
            "{'load_data': '0.012', 'extract_feat': '0.023', 'forward': '0.178', 'batch_size': '1', 'rtf': '0.010'}, : 100% 1/1 [00:00<00:00,  5.62it/s]\u001b[A\n",
            "rtf_avg: 0.010: 100% 1/1 [00:00<00:00,  5.60it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  5.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.022', 'forward': '0.195', 'batch_size': '2', 'rtf': '0.012'}, :  50% 1/2 [00:00<00:00,  5.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.012:  50% 1/2 [00:00<00:00,  5.06it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.021', 'batch_size': '1', 'rtf': '-0.021'}, : 100% 1/1 [00:00<00:00, 47.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.021: 100% 1/1 [00:00<00:00, 45.79it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  4.23it/s]\u001b[A\n",
            "rtf_avg: 0.013, time_speech:  17.000, time_escape: 0.224: 100% 1/1 [00:00<00:00,  4.23it/s]\n",
            " 60% 6/10 [00:04<00:02,  1.96it/s]\n",
            "split_2_4.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  5.43it/s]\u001b[A\n",
            "{'load_data': '0.015', 'extract_feat': '0.025', 'forward': '0.184', 'batch_size': '1', 'rtf': '0.010'}, : 100% 1/1 [00:00<00:00,  5.43it/s]\u001b[A\n",
            "rtf_avg: 0.010: 100% 1/1 [00:00<00:00,  5.40it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25% 1/4 [00:00<00:00,  5.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.027', 'forward': '0.196', 'batch_size': '4', 'rtf': '0.012'}, :  25% 1/4 [00:00<00:00,  5.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.012:  25% 1/4 [00:00<00:00,  5.07it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.041', 'batch_size': '1', 'rtf': '-0.041'}, : 100% 1/1 [00:00<00:00, 24.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.041: 100% 1/1 [00:00<00:00, 23.96it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  3.90it/s]\u001b[A\n",
            "rtf_avg: 0.014, time_speech:  18.000, time_escape: 0.244: 100% 1/1 [00:00<00:00,  3.89it/s]\n",
            " 70% 7/10 [00:04<00:01,  2.05it/s]\n",
            "split_3_1.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  5.72it/s]\u001b[A\n",
            "{'load_data': '0.013', 'extract_feat': '0.025', 'forward': '0.175', 'batch_size': '1', 'rtf': '0.010'}, : 100% 1/1 [00:00<00:00,  5.72it/s]\u001b[A\n",
            "rtf_avg: 0.010: 100% 1/1 [00:00<00:00,  5.69it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25% 1/4 [00:00<00:00,  6.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.027', 'forward': '0.148', 'batch_size': '4', 'rtf': '0.009'}, :  25% 1/4 [00:00<00:00,  6.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.009:  25% 1/4 [00:00<00:00,  6.68it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.019', 'batch_size': '1', 'rtf': '-0.019'}, : 100% 1/1 [00:00<00:00, 52.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.019: 100% 1/1 [00:00<00:00, 51.46it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  5.35it/s]\u001b[A\n",
            "rtf_avg: 0.010, time_speech:  17.000, time_escape: 0.174: 100% 1/1 [00:00<00:00,  5.34it/s]\n",
            " 80% 8/10 [00:05<00:00,  2.23it/s]\n",
            "split_3_2.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  5.37it/s]\u001b[A\n",
            "{'load_data': '0.013', 'extract_feat': '0.024', 'forward': '0.186', 'batch_size': '1', 'rtf': '0.011'}, : 100% 1/1 [00:00<00:00,  5.37it/s]\u001b[A\n",
            "rtf_avg: 0.011: 100% 1/1 [00:00<00:00,  5.34it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 50% 1/2 [00:00<00:00,  6.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.026', 'forward': '0.159', 'batch_size': '2', 'rtf': '0.010'}, :  50% 1/2 [00:00<00:00,  6.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.010:  50% 1/2 [00:00<00:00,  6.24it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.021', 'batch_size': '1', 'rtf': '-0.021'}, : 100% 1/1 [00:00<00:00, 47.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.021: 100% 1/1 [00:00<00:00, 47.23it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  4.98it/s]\u001b[A\n",
            "rtf_avg: 0.011, time_speech:  17.000, time_escape: 0.188: 100% 1/1 [00:00<00:00,  4.98it/s]\n",
            " 90% 9/10 [00:05<00:00,  2.32it/s]\n",
            "split_3_3.wav\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100% 1/1 [00:00<00:00,  7.71it/s]\u001b[A\n",
            "{'load_data': '0.013', 'extract_feat': '0.021', 'forward': '0.130', 'batch_size': '1', 'rtf': '0.012'}, : 100% 1/1 [00:00<00:00,  7.71it/s]\u001b[A\n",
            "rtf_avg: 0.012: 100% 1/1 [00:00<00:00,  7.67it/s]\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25% 1/4 [00:00<00:00,  8.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': '0.000', 'extract_feat': '0.020', 'forward': '0.120', 'batch_size': '4', 'rtf': '0.012'}, :  25% 1/4 [00:00<00:00,  8.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: 0.012:  25% 1/4 [00:00<00:00,  8.08it/s]\n",
            "\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "{'load_data': 0.0, 'extract_feat': 0.0, 'forward': '0.013', 'batch_size': '1', 'rtf': '-0.013'}, : 100% 1/1 [00:00<00:00, 75.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "rtf_avg: -0.013: 100% 1/1 [00:00<00:00, 74.60it/s]\n",
            "\n",
            "100% 1/1 [00:00<00:00,  6.46it/s]\u001b[A\n",
            "rtf_avg: 0.013, time_speech:  11.000, time_escape: 0.142: 100% 1/1 [00:00<00:00,  6.45it/s]\n",
            "100% 10/10 [00:05<00:00,  1.70it/s]\n",
            "ASR 任务完成->标注文件路径: /content/output/asr_opt/denoise_opt.list\n",
            "\n",
            "\"/usr/local/bin/python\" tools/subfix_webui.py --load_list \"/content/output/asr_opt/denoise_opt.list\" --webui_port 9871 --is_share True\n",
            "Running on local URL:  http://0.0.0.0:9871\n",
            "Running on public URL: https://8308205efa37ae31bc.gradio.live\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/1-get-text.py\n",
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "split_1_2.wav\n",
            "split_1_1.wav\n",
            "当前使用g2pw进行拼音推理\n",
            "当前使用g2pw进行拼音推理\n",
            "Building prefix dict from the default dictionary ...\n",
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Dumping model to file cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 1.413 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "Loading model cost 1.510 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "split_2_1.wav\n",
            "split_1_3.wav\n",
            "split_2_2.wav\n",
            "split_2_3.wav\n",
            "split_2_4.wav\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/2-get-hubert-wav32k.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/prepare_datasets/3-get-semantic.py\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS/TEMP/tmp_s2.json\"\n",
            "phoneme_data_len: 7\n",
            "wav_data_len: 98\n",
            "100% 98/98 [00:00<00:00, 31613.74it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  98\n",
            "<All keys matched successfully>\n",
            "<All keys matched successfully>\n",
            "  0% 0/25 [00:00<?, ?it/s][rank0]:[W1216 05:28:50.863955819 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "100% 25/25 [01:07<00:00,  2.70s/it]\n",
            "100% 25/25 [00:27<00:00,  1.11s/it]\n",
            "100% 25/25 [00:25<00:00,  1.01s/it]\n",
            "100% 25/25 [00:25<00:00,  1.02s/it]\n",
            "100% 25/25 [00:26<00:00,  1.07s/it]\n",
            "100% 25/25 [00:27<00:00,  1.10s/it]\n",
            "100% 25/25 [00:25<00:00,  1.01s/it]\n",
            "100% 25/25 [00:26<00:00,  1.04s/it]\n",
            "100% 25/25 [00:27<00:00,  1.11s/it]\n",
            "100% 25/25 [00:26<00:00,  1.08s/it]\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s2_train.py --config \"/content/GPT-SoVITS/TEMP/tmp_s2.json\"\n",
            "phoneme_data_len: 7\n",
            "wav_data_len: 98\n",
            "100% 98/98 [00:00<00:00, 55299.58it/s]\n",
            "skipped_phone:  0 , skipped_dur:  0\n",
            "total left:  98\n",
            "logs/ouruola/logs_s2/D_233333333333.pth\n",
            "load \n",
            "logs/ouruola/logs_s2/G_233333333333.pth\n",
            "load \n",
            "  0% 0/25 [00:00<?, ?it/s][rank0]:[W1216 05:36:26.915663149 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "100% 25/25 [01:04<00:00,  2.56s/it]\n",
            "100% 25/25 [00:28<00:00,  1.15s/it]\n",
            "100% 25/25 [00:25<00:00,  1.02s/it]\n",
            "100% 25/25 [00:27<00:00,  1.11s/it]\n",
            "100% 25/25 [00:24<00:00,  1.01it/s]\n",
            "100% 25/25 [00:27<00:00,  1.10s/it]\n",
            "100% 25/25 [00:27<00:00,  1.08s/it]\n",
            "100% 25/25 [00:24<00:00,  1.03it/s]\n",
            "100% 25/25 [00:27<00:00,  1.09s/it]\n",
            "100% 25/25 [00:27<00:00,  1.09s/it]\n",
            "100% 25/25 [00:25<00:00,  1.02s/it]\n",
            "100% 25/25 [00:27<00:00,  1.10s/it]\n",
            "100% 25/25 [00:24<00:00,  1.02it/s]\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/s1_train.py --config_file \"/content/GPT-SoVITS/TEMP/tmp_s1.yaml\" \n",
            "Seed set to 1234\n",
            "Using 16bit Automatic Mixed Precision (AMP)\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/models/t2s_lightning_module.py:26: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "<All keys matched successfully>\n",
            "ckpt_path: None\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "semantic_data_len: 7\n",
            "phoneme_data_len: 7\n",
            "       item_name                                     semantic_audio\n",
            "0  split_1_1.wav  520 271 53 105 280 280 280 280 280 280 280 280...\n",
            "1  split_1_3.wav  636 776 954 700 853 853 659 853 606 220 322 78...\n",
            "2  split_2_2.wav  727 427 202 962 563 725 775 479 671 676 590 45...\n",
            "3  split_2_4.wav  683 132 239 417 103 221 212 302 620 10 630 630...\n",
            "4  split_1_2.wav  954 905 12 316 322 727 659 662 613 799 171 943...\n",
            "5  split_2_1.wav  520 53 105 280 280 486 486 486 486 486 486 486...\n",
            "6  split_2_3.wav  700 700 659 606 589 971 971 40 241 610 566 467...\n",
            "dataset.__len__(): 98\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                 | Params | Mode \n",
            "-------------------------------------------------------\n",
            "0 | model | Text2SemanticDecoder | 77.6 M | train\n",
            "-------------------------------------------------------\n",
            "77.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "77.6 M    Total params\n",
            "310.426   Total estimated model params size (MB)\n",
            "257       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "/content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "/usr/local/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "Epoch 0:   0% 0/25 [00:00<?, ?it/s] /content/GPT-SoVITS/GPT_SoVITS/AR/data/dataset.py:230: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  bert_feature = torch.load(path_bert, map_location=\"cpu\")\n",
            "Epoch 9: 100% 25/25 [00:05<00:00,  4.23it/s, v_num=0, total_loss_step=598.0, lr_step=0.002, top_3_acc_step=0.979, total_loss_epoch=1.23e+3, lr_epoch=0.002, top_3_acc_epoch=0.978]`Trainer.fit` stopped: `max_epochs=10` reached.\n",
            "Epoch 9: 100% 25/25 [00:15<00:00,  1.66it/s, v_num=0, total_loss_step=598.0, lr_step=0.002, top_3_acc_step=0.979, total_loss_epoch=1.23e+3, lr_epoch=0.002, top_3_acc_epoch=0.978]\n",
            "\"/usr/local/bin/python\" GPT_SoVITS/inference_webui_fast.py \"Auto\"\n",
            "---------------------------------------------TTS Config---------------------------------------------\n",
            "device              : cuda\n",
            "is_half             : True\n",
            "version             : v2\n",
            "t2s_weights_path    : GPT_weights_v2/ouruola-e10.ckpt\n",
            "vits_weights_path   : SoVITS_weights_v2/ouruola_e20_s500.pth\n",
            "bert_base_path      : GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\n",
            "cnhuhbert_base_path : GPT_SoVITS/pretrained_models/chinese-hubert-base\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Loading Text2Semantic weights from GPT_weights_v2/ouruola-e10.ckpt\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e20_s500.pth\n",
            "Loading BERT weights from GPT_SoVITS/pretrained_models/chinese-roberta-wwm-ext-large\n",
            "Loading CNHuBERT weights from GPT_SoVITS/pretrained_models/chinese-hubert-base\n",
            "Running on local URL:  http://0.0.0.0:9872\n",
            "Running on public URL: https://7c578a46cd006fee32.gradio.live\n",
            "Set seed to 2263292353\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "当前使用g2pw进行拼音推理\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba_fast:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "DEBUG:jieba_fast:Loading model from cache /content/GPT-SoVITS/TEMP/jieba.cache\n",
            "Loading model cost 0.879 seconds.\n",
            "DEBUG:jieba_fast:Loading model cost 0.879 seconds.\n",
            "Prefix dict has been built succesfully.\n",
            "DEBUG:jieba_fast:Prefix dict has been built succesfully.\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "我爱你中国，亲爱的母亲，喻明宇我爱你，超级爱你超级爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['我爱你中国，亲爱的母亲，喻明宇我爱你，超级爱你超级爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  7.89it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['我爱你中国,亲爱的母亲,喻明宇我爱你,超级爱你超级爱你.']\n",
            " 11% 162/1500 [00:02<00:18, 71.70it/s]T2S Decoding EOS [139 -> 305]\n",
            " 11% 165/1500 [00:02<00:21, 60.86it/s]\n",
            "8.916\t0.130\t2.756\t0.523\n",
            "Set seed to 2882901029\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "我爱你中国，亲爱的母亲，喻明宇我爱你，超级爱你超级爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['我爱你中国，亲爱的母亲，喻明宇我爱你，超级爱你超级爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  7.67it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['我爱你中国,亲爱的母亲,喻明宇我爱你,超级爱你超级爱你.']\n",
            " 10% 157/1500 [00:02<00:19, 68.47it/s]T2S Decoding EOS [139 -> 302]\n",
            " 11% 162/1500 [00:02<00:19, 68.82it/s]\n",
            "0.000\t0.131\t2.356\t0.329\n",
            "Set seed to 631503674\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  6.13it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            "  7% 99/1500 [00:01<00:22, 62.10it/s]T2S Decoding EOS [139 -> 243]\n",
            "  7% 103/1500 [00:01<00:21, 64.20it/s]\n",
            "0.000\t0.164\t1.607\t0.797\n",
            "Loading Text2Semantic weights from GPT_weights_v2/ouruola-e5.ckpt\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e15_s375.pth\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e10_s250.pth\n",
            "Set seed to 1165018851\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  2.49it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            " 10% 147/1500 [00:02<00:19, 68.41it/s]T2S Decoding EOS [139 -> 292]\n",
            " 10% 152/1500 [00:02<00:23, 58.23it/s]\n",
            "0.000\t0.403\t2.625\t0.167\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e8_s200.pth\n",
            "Set seed to 3463607333\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  6.48it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            " 16% 246/1500 [00:03<00:17, 70.62it/s]T2S Decoding EOS [139 -> 387]\n",
            " 16% 247/1500 [00:03<00:18, 69.10it/s]\n",
            "0.000\t0.156\t3.577\t0.173\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e4_s100.pth\n",
            "Set seed to 183073491\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  6.31it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            " 11% 162/1500 [00:02<00:19, 69.62it/s]T2S Decoding EOS [139 -> 305]\n",
            " 11% 165/1500 [00:02<00:19, 68.17it/s]\n",
            "0.000\t0.160\t2.423\t0.075\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e15_s375.pth\n",
            "Set seed to 447413333\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  6.45it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            " 22% 330/1500 [00:05<00:16, 70.15it/s]T2S Decoding EOS [139 -> 476]\n",
            " 22% 336/1500 [00:06<00:20, 55.47it/s]\n",
            "0.000\t0.156\t6.059\t0.152\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e10_s250.pth\n",
            "Set seed to 4210463343\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  6.36it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            " 16% 240/1500 [00:04<00:26, 46.82it/s]T2S Decoding EOS [139 -> 382]\n",
            " 16% 242/1500 [00:04<00:21, 57.84it/s]\n",
            "0.000\t0.159\t4.187\t0.269\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e8_s200.pth\n",
            "Set seed to 1839891028\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  6.34it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            " 10% 147/1500 [00:02<00:19, 68.73it/s]T2S Decoding EOS [139 -> 293]\n",
            " 10% 153/1500 [00:02<00:20, 66.90it/s]\n",
            "0.000\t0.159\t2.289\t0.153\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e4_s100.pth\n",
            "Set seed to 3690240452\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  6.20it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            " 16% 239/1500 [00:03<00:19, 64.27it/s]T2S Decoding EOS [139 -> 382]\n",
            " 16% 242/1500 [00:03<00:18, 67.71it/s]\n",
            "0.000\t0.163\t3.576\t0.065\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e20_s500.pth\n",
            "Set seed to 3314790432\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  6.20it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            " 20% 297/1500 [00:05<00:17, 69.82it/s]T2S Decoding EOS [139 -> 439]\n",
            " 20% 299/1500 [00:05<00:22, 54.42it/s]\n",
            "0.000\t0.163\t5.497\t0.150\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e15_s375.pth\n",
            "Set seed to 4039978500\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  5.70it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇爱你.']\n",
            "  7% 103/1500 [00:01<00:19, 70.36it/s]T2S Decoding EOS [139 -> 249]\n",
            "  7% 109/1500 [00:01<00:20, 67.20it/s]\n",
            "0.000\t0.177\t1.624\t0.164\n",
            "Set seed to 678871752\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "神秘北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边，忽然的瞬间，在那遥远的地点，我看见，恋人幸福的光点\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['神秘北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边，忽然的瞬间，在那遥远的地点，我看见，恋人幸福的光点。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  1.97it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['神秘北极圈,阿拉斯加的山巅,谁的脸,出现海角的天边,忽然的瞬间,在那遥远的地点,我看见,恋人幸福的光点.']\n",
            " 29% 435/1500 [00:06<00:19, 54.53it/s]T2S Decoding EOS [139 -> 577]\n",
            " 29% 437/1500 [00:06<00:15, 67.97it/s]\n",
            "0.000\t0.507\t6.431\t0.236\n",
            "Loading Text2Semantic weights from GPT_weights_v2/ouruola-e10.ckpt\n",
            "Loading VITS weights from SoVITS_weights_v2/ouruola_e20_s500.pth\n",
            "Set seed to 1011435659\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "神秘北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边，忽然的瞬间，在那遥远的地点，我看见，恋人幸福的光点\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['神秘北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边，忽然的瞬间，在那遥远的地点，我看见，恋人幸福的光点。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  2.04it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['神秘北极圈,阿拉斯加的山巅,谁的脸,出现海角的天边,忽然的瞬间,在那遥远的地点,我看见,恋人幸福的光点.']\n",
            " 21% 321/1500 [00:05<00:25, 46.19it/s]T2S Decoding EOS [139 -> 462]\n",
            " 21% 322/1500 [00:05<00:21, 55.89it/s]\n",
            "0.000\t0.491\t5.768\t0.160\n",
            "Set seed to 3490133544\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "谢宛瑾我爱你，谢宛瑾我爱你，喻明宇也爱你\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['谢宛瑾我爱你，谢宛瑾我爱你，喻明宇也爱你。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  6.32it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['谢宛瑾我爱你,谢宛瑾我爱你,喻明宇也爱你.']\n",
            "  8% 120/1500 [00:02<00:26, 51.96it/s]T2S Decoding EOS [139 -> 260]\n",
            "  8% 120/1500 [00:02<00:24, 57.46it/s]\n",
            "0.000\t0.159\t2.091\t0.254\n",
            "Set seed to 2621330911\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "对这个世界如果你有太多的抱怨，跌倒了就不敢继续往前走，为什么人要这么的脆弱 堕落，请你打开电视看看，多少人为生命在努力勇敢的走下去，我们是不是该知足，珍惜一切 就算没有拥有，还记得你说家是唯一的城堡，随着稻香河流继续奔跑，微微笑 小时候的梦我知道\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['对这个世界如果你有太多的抱怨，跌倒了就不敢继续往前走，为什么人要这么的脆弱 堕落，请你打开电视看看，多少人为生命在努力勇敢的走下去，', '我们是不是该知足，珍惜一切 就算没有拥有，还记得你说家是唯一的城堡，随着稻香河流继续奔跑，微微笑 小时候的梦我知道。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 2/2 [00:02<00:00,  1.49s/it]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['我们是不是该知足,珍惜一切就算没有拥有,还记得你说家是唯一的城堡,随着稻香河流继续奔跑,微微笑小时候的梦我知道.', '对这个世界如果你有太多的抱怨,跌倒了就不敢继续往前走,为什么人要这么的脆弱堕落,请你打开电视看看,多少人为生命在努力勇敢的走下去,']\n",
            " 25% 371/1500 [00:06<00:17, 63.50it/s]T2S Decoding EOS [139 -> 514]\n",
            " 25% 374/1500 [00:06<00:18, 60.44it/s]\n",
            "0.000\t2.985\t6.199\t0.178\n",
            "Set seed to 3976741211\n",
            "并行推理模式已开启\n",
            "分桶处理模式已开启\n",
            "Actual Input Reference Text: 北极圈，阿拉斯加的山巅，谁的脸，出现海角的天边。\n",
            "############ 切分文本 ############\n",
            "Actual Input Target Text:\n",
            "我好想被喻明宇干死啊\n",
            "Actual Input Target Text (after sentence segmentation):\n",
            "['我好想被喻明宇干死啊。']\n",
            "############ 提取文本Bert特征 ############\n",
            "100% 1/1 [00:00<00:00,  7.86it/s]\n",
            "############ 推理 ############\n",
            "Processed text from the frontend (per sentence): ['我好想被喻明宇干死啊.']\n",
            "  3% 52/1500 [00:01<00:28, 50.49it/s]T2S Decoding EOS [139 -> 193]\n",
            "  4% 53/1500 [00:01<00:31, 46.67it/s]\n",
            "0.000\t0.128\t1.138\t0.252\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 2400, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GPT-SoVITS/webui.py\", line 1048, in <module>\n",
            "    app.queue().launch(#concurrency_count=511, max_size=1022\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 2307, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/blocks.py\", line 2404, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.9/site-packages/gradio/http_server.py\", line 68, in close\n",
            "    self.thread.join()\n",
            "  File \"/usr/local/lib/python3.9/threading.py\", line 1060, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/local/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:9874 <> https://62c218b5b9d01a8b51.gradio.live\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}